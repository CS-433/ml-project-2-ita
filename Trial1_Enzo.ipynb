{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install vtk\n",
        "!pip install meshio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Invi4SwWUGt0",
        "outputId": "6786de37-7375-4a95-8292-40f877cda3ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vtk in /usr/local/lib/python3.10/dist-packages (9.3.1)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from vtk) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->vtk) (1.16.0)\n",
            "Requirement already satisfied: meshio in /usr/local/lib/python3.10/dist-packages (5.3.5)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from meshio) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from meshio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->meshio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->meshio) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->meshio) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->meshio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xcjMZnnRwpCN"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import os\n",
        "import vtkmodules.all as vtk\n",
        "from vtkmodules.util.numpy_support import vtk_to_numpy, numpy_to_vtk\n",
        "import meshio\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.model_selection import KFold\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "45x3tIx5roX0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ3-Jiio0MMr",
        "outputId": "68db39c3-1c65-4e60-927f-eb016c7c9c31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fields = {'velocity', 'pressure'}\n",
        "\n",
        "basis_space, sv_space, Nh_space, nmodes_space = dict(), dict(), dict(), dict()\n",
        "basis_time, sv_time, Nh_time, nmodes_time = dict(), dict(), dict(), dict()\n",
        "nmodes = dict()\n",
        "'/content/drive/MyDrive/dati/input_data.npy'\n",
        "for field in fields:\n",
        "    basis_space[field] = np.load(os.path.join('/content/drive/MyDrive/data_ML4Science', 'basis', field, 'space_basis.npy'))  # spatial basis\n",
        "    sv_space[field] = np.load(os.path.join('/content/drive/MyDrive/data_ML4Science', 'basis', field, 'space_sv.npy'))  # singular values in space\n",
        "    Nh_space[field], nmodes_space[field] = basis_space[field].shape  # number of FOM and ROM unknowns in space\n",
        "    basis_time[field] = np.load(os.path.join('/content/drive/MyDrive/data_ML4Science', 'basis', field, 'time_basis.npy'))  # temporal basis\n",
        "    sv_time[field] = np.load(os.path.join('/content/drive/MyDrive/data_ML4Science', 'basis', field, 'time_sv.npy'))  # singular values in time\n",
        "    Nh_time[field], nmodes_time[field] = basis_time[field].shape  # number of FOM and ROM unknowns in time\n",
        "    nmodes[field] = nmodes_space[field] * nmodes_time[field]  # total dimension of the reduced basis\n",
        "\n",
        "# UPDATE VELOCITY BASES TO ACCOUNT FOR SUPREMIZERS AND STABILIZERS\n",
        "N_supr_space = basis_space['pressure'].shape[1] + 66  # number of extra bases in space for the velocity\n",
        "N_supr_time = 5  # number of extra bases in time for the velocity\n",
        "\n",
        "# STORE ORIGINAL NUMBER OF VELOCITY MODES IN THE DICTIONARY\n",
        "nmodes_space['velocity_full'] = nmodes_space['velocity']\n",
        "nmodes_time['velocity_full'] = nmodes_time['velocity']\n",
        "nmodes['velocity_full'] = nmodes['velocity']\n",
        "\n",
        "# UPDATE THE NUMBER OF VELOCITY MODES TO ACCOUNT FOR SUPREMIZERS AND STABILIZERS\n",
        "nmodes_space['velocity'] -= N_supr_space\n",
        "nmodes_time['velocity'] -= N_supr_time\n",
        "nmodes['velocity'] = nmodes_space['velocity'] * nmodes_time['velocity']\n",
        "\n",
        "# UPDATE VELOCITY BASES TO ACCOUNT FOR SUPREMIZERS AND STABILIZERS\n",
        "basis_space['velocity'] = basis_space['velocity'][:, :nmodes_space['velocity']]\n",
        "basis_time['velocity'] = basis_time['velocity'][:, :nmodes_time['velocity']]\n",
        "\n",
        "# LOAD NORMED BASIS MATRICES IN SPACE (needed for projections)\n",
        "basis_space_normed = dict()\n",
        "#norm = dict()\n",
        "for field in fields:\n",
        "    #norm[field] = load_npz(os.path.join('dataset', 'norms', f'norm_{field}.npz'))\n",
        "    #basis_space_normed[field] = norm[field].dot(basis_space[field])\n",
        "    #np.save(os.path.join('dataset', 'basis', field, 'basis_space_normed.npy'), basis_space_normed[field])\n",
        "    basis_space_normed[field] = np.load(os.path.join('/content/drive/MyDrive/data_ML4Science', 'basis', field, 'basis_space_normed.npy'))\n",
        "\n",
        "# n_snaps = None  # change to a number if you want to load only a subset of snapshots\n",
        "# _sol = np.load(os.path.join('dataset', 'RB_data', 'solutions.npy'))[:n_snaps]\n",
        "_sol = np.load(os.path.join('/content/drive/MyDrive/data_ML4Science', 'RB_data', 'solutions.npy'))\n",
        "\n",
        "solutions = dict()\n",
        "\n",
        "# velocity reduced solutions (with and without supremizers and stabilizers)\n",
        "solutions['velocity_full'] = np.reshape(_sol[:, :nmodes['velocity_full']],\n",
        "                                        (-1, nmodes_space['velocity_full'], nmodes_time['velocity_full']))\n",
        "solutions['velocity'] = solutions['velocity_full'][:, :nmodes_space['velocity'], :nmodes_time['velocity']]\n",
        "\n",
        "# pressure reduced solutions\n",
        "solutions['pressure'] = np.reshape(_sol[:, :nmodes['pressure']],\n",
        "                                   (-1, nmodes_space['pressure'], nmodes_time['pressure']))\n",
        "\n",
        "##################################################################\n",
        "def project(sol, normed_basis_space, basis_time):\n",
        "    \"\"\" Project a full-order solution in space-time.\"\"\"\n",
        "    return (normed_basis_space.T.dot(sol)).dot(basis_time) # !! REMARK: here we need the normed basis in space !!\n",
        "\n",
        "def expand(sol, basis_space, basis_time):\n",
        "    \"\"\" Expand a reduced-order solution in space-time.\"\"\"\n",
        "    return (basis_space.dot(sol)).dot(basis_time.T)\n",
        "\n",
        "##################################################################\n",
        "\n",
        "params = np.load(os.path.join('/content/drive/MyDrive/data_ML4Science', 'RB_data', 'parameters.npy'))\n",
        "params = np.delete(params, 2, axis=1)  # one column is useless and we delete it\n",
        "\n",
        "Q = lambda t, mu, T=1: 1 - np.cos(2*np.pi*t/T) + mu[1] * np.sin(mu[0]*2*np.pi*t/T)\n",
        "\n",
        "times = np.linspace(0,1,1000)\n",
        "plt.plot(times, Q(times, [4.0, 0.1]), 'b-', label=r\"$\\mu = [4.0, 0.1]$\")\n",
        "plt.plot(times, Q(times, [4.0, 0.3]), 'b--', label=r\"$\\mu = [4.0, 0.3]$\")\n",
        "plt.plot(times, Q(times, [8.0, 0.1]), 'r-', label=r\"$\\mu = [8.0, 0.1]$\")\n",
        "plt.plot(times, Q(times, [8.0, 0.3]), 'r--', label=r\"$\\mu = [8.0, 0.3]$\")\n",
        "\n",
        "plt.grid()\n",
        "plt.xlim([0,1])\n",
        "plt.legend()\n",
        "plt.title(\"Parametric flow rate\", fontweight='bold', fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "_Bqc_7Kl3RmC",
        "outputId": "51210ba0-bf8e-43d6-b99d-7e5244865f51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solutions['velocity'].shape)\n",
        "print(solutions['pressure'].shape)\n",
        "print(params.shape)\n",
        "\n",
        "params = params.astype(np.float32)\n",
        "solutions['velocity'] = solutions['velocity'].astype(np.float32)\n",
        "solutions['pressure'] = solutions['pressure'].astype(np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qmiswGP51Cj",
        "outputId": "4adf68c3-bee0-46e7-aa38-ea0f4a5601d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1950, 39, 16)\n",
            "(1950, 9, 19)\n",
            "(1950, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "class Fluid_Dataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        input_data = self.inputs[idx]\n",
        "        target_data = self.targets[idx]\n",
        "        return input_data, target_data\n",
        "\n",
        "class Fluid_Dataset_auto(Dataset):\n",
        "    def __init__(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.targets = inputs\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        input_data = self.inputs[idx]\n",
        "        target_data = self.targets[idx]\n",
        "        return input_data, target_data\n",
        "\n",
        "N_data=params.shape[0]\n",
        "indices = torch.randperm(N_data)\n",
        "ratio_data=0.8\n",
        "train_size = int(ratio_data * N_data)\n",
        "train_indices = indices[:train_size]\n",
        "test_indices=indices[train_size:]\n",
        "\n",
        "mean_params=np.mean(params,axis=0)\n",
        "min_params=np.min(params,axis=0)\n",
        "max_params=np.max(params,axis=0)\n",
        "std_params=np.std(params,axis=0)\n",
        "params_norm=(params-mean_params)/std_params\n",
        "# params_norm=(params-min_params)/(max_params-min_params)\n",
        "#FARE IN -1,1??\n",
        "\n",
        "train_params=params_norm[train_indices]\n",
        "test_params=params_norm[test_indices]\n",
        "\n",
        "vel=solutions['velocity']\n",
        "# max_vel=np.max(vel,axis=0)\n",
        "# min_vel=np.min(vel,axis=0)\n",
        "# # print(min_vel.shape)\n",
        "# vel=(vel-min_vel)/(max_vel-min_vel)\n",
        "# mean_vel=np.mean(vel,axis=0)\n",
        "# std_vel=np.std(vel,axis=0)\n",
        "# vel=(vel-mean_vel)/std_vel\n",
        "\n",
        "train_vel=vel[train_indices]\n",
        "test_vel=vel[test_indices]\n",
        "\n",
        "train_press=solutions['pressure'][train_indices]\n",
        "test_press=solutions['pressure'][test_indices]\n",
        "# dataset = Fluid_Dataset(params_norm, solutions['velocity'])\n",
        "\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "print(test_vel.shape,train_vel.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoN6CMUf93jj",
        "outputId": "083300b0-4a3c-431d-f78b-74112c26b5eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(390, 39, 16) (1560, 39, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K=256\n",
        "class MLP_vel_model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gelu = torch.nn.GELU()\n",
        "        self.ln = torch.nn.LayerNorm(K)\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(3,K),\n",
        "            torch.nn.LayerNorm(K),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(K,K),\n",
        "            torch.nn.LayerNorm(K),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(K,K),\n",
        "            torch.nn.LayerNorm(K),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(K,K),\n",
        "            torch.nn.LayerNorm(K),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(K,K),\n",
        "            torch.nn.LayerNorm(K),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(K,39*16)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, Z):\n",
        "        Z = torch.flatten(Z, 1)\n",
        "        Z = self.mlp(Z)\n",
        "        Z = Z.view(-1, 39, 16)\n",
        "        return Z\n",
        "\n",
        "class Autoencoder(torch.nn.Module):\n",
        "    def __init__(self): #VARIARE NUMERO LAYER E NH es (Nh=32)\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(39 * 16, 512),  # Livello completamente connesso\n",
        "            torch.nn.LayerNorm(512),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.LayerNorm(512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, 39 * 16),  # Ricostruisce la dimensione piatta\n",
        "            torch.nn.Unflatten(1, (39, 16))\n",
        "        )\n",
        "                                     # Normalizza l'output [0,1])\n",
        "    def forward(self, x):\n",
        "        Z = torch.flatten(x, 1)\n",
        "        Z1=self.encoder(Z)\n",
        "        Z2=self.decoder(Z1)\n",
        "        return Z2\n",
        "\n",
        "class Autoencoder2(torch.nn.Module):\n",
        "    def __init__(self): #VARIARE NUMERO LAYER E NH es (Nh=32)\n",
        "        super(Autoencoder2, self).__init__()\n",
        "\n",
        "\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=1),  # (1, 39, 16) -> (64, 19, 7)\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # (64, 19, 7) -> (128, 10, 4)\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Flatten(),                                          # (32, 10, 4) -> (32*10*4)\n",
        "            torch.nn.Linear(128*10*4, 256)                               # Bottleneck\n",
        "            # torch.nn.ReLU(),\n",
        "            # torch.nn.Linear(128, 32)                                     # Latent space\n",
        "        )\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            # torch.nn.Linear(32, 128),\n",
        "            # torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 128*10*4),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Unflatten(1, (128, 10, 4)),                         # (32*10*4) -> (32, 10, 4)\n",
        "            torch.nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=(0, 0)), #(64,19,7)\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.ConvTranspose2d(64, 1, kernel_size=5, stride=2, padding=1, output_padding=(0, 1)),   # -> (1, 39, 16)\n",
        "            # torch.nn.Sigmoid()                                          # Normalizza l'output [0,1]\n",
        "        )\n",
        "                                     # Normalizza l'output [0,1])\n",
        "    def forward(self, x):\n",
        "        x=x.unsqueeze(1)\n",
        "        Z1=self.encoder(x)\n",
        "        Z2=self.decoder(Z1)\n",
        "        Z2=Z2.squeeze(1)\n",
        "        return Z2\n",
        "\n",
        "class MLP_vel_ConvAutoencoder(torch.nn.Module):\n",
        "    def __init__(self): #VARIARE NUMERO LAYER E NH es (Nh=32)\n",
        "        super(MLP_vel_ConvAutoencoder, self).__init__()\n",
        "\n",
        "        self.gelu = torch.nn.GELU()\n",
        "        self.ln = torch.nn.LayerNorm(256)\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(3,256),\n",
        "            torch.nn.LayerNorm(256),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(256,256),\n",
        "            torch.nn.LayerNorm(256),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(256,256),\n",
        "            torch.nn.LayerNorm(256),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(256,256),\n",
        "            torch.nn.LayerNorm(256),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(256,256),\n",
        "            torch.nn.LayerNorm(256),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(256,128)\n",
        "        )\n",
        "\n",
        "        # self.encoder = torch.nn.Sequential(\n",
        "        #     torch.nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # (1, 39, 16) -> (16, 20, 8)\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # (16, 20, 8) -> (32, 10, 4)\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Flatten(),                                          # (32, 10, 4) -> (32*10*4)\n",
        "        #     torch.nn.Linear(32*10*4, 128),                               # Bottleneck\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Linear(128, 64)                                     # Latent space\n",
        "        # )\n",
        "\n",
        "        # self.encoder = torch.nn.Sequential(\n",
        "        #     torch.nn.Flatten(),\n",
        "        #     torch.nn.Linear(39 * 16, 512),  # Livello completamente connesso\n",
        "        #     torch.nn.LayerNorm(512),\n",
        "        #     torch.nn.ReLU(),\n",
        "        #     torch.nn.Linear(512, 256),\n",
        "        #     torch.nn.LayerNorm(256),\n",
        "        #     torch.nn.ReLU(),\n",
        "        #     torch.nn.Linear(256, 256)\n",
        "        # )\n",
        "\n",
        "        # self.decoder = torch.nn.Sequential(\n",
        "        #     torch.nn.Linear(256, 256),\n",
        "        #     torch.nn.LayerNorm(256),\n",
        "        #     torch.nn.ReLU(),\n",
        "        #     torch.nn.Linear(256, 512),\n",
        "        #     torch.nn.LayerNorm(512),\n",
        "        #     torch.nn.ReLU(),\n",
        "        #     torch.nn.Linear(512, 39 * 16),  # Ricostruisce la dimensione piatta\n",
        "        #     torch.nn.Unflatten(1, (39, 16))\n",
        "        # )\n",
        "\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=1),  # (1, 39, 16) -> (64, 19, 7)\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # (64, 19, 7) -> (128, 10, 4)\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Flatten(),                                          # (32, 10, 4) -> (32*10*4)\n",
        "            torch.nn.Linear(128*10*4, 256),                               # Bottleneck\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 128)                                     # Latent space\n",
        "        )\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(128, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 128*10*4),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Unflatten(1, (128, 10, 4)),                         # (32*10*4) -> (32, 10, 4)\n",
        "            torch.nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=(0, 0)), #(64,19,7)\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.ConvTranspose2d(64, 1, kernel_size=5, stride=2, padding=1, output_padding=(0, 1)),   # -> (1, 39, 16)\n",
        "            # torch.nn.Sigmoid()                                          # Normalizza l'output [0,1]\n",
        "        )\n",
        "\n",
        "        # # Decoder\n",
        "        # self.decoder = torch.nn.Sequential(\n",
        "        #     torch.nn.Linear(64, 128),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Linear(128, 32*10*4),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Unflatten(1, (32, 10, 4)),                         # (32*10*4) -> (32, 10, 4)\n",
        "        #     torch.nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=(1, 1)), #(16,20,8)\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=(0, 1)))   # -> (1, 39, 16)\n",
        "        #    # torch.nn.Sigmoid()  )                                        # Normalizza l'output [0,1])\n",
        "    def forward(self, x, y):\n",
        "        # Z = torch.flatten(x, 1)\n",
        "        Z=x\n",
        "        Z1 = self.mlp(Z)\n",
        "\n",
        "        y = y.unsqueeze(1)\n",
        "        Z2=self.encoder(y)\n",
        "        Z3=self.decoder(Z2)\n",
        "        Z3=Z3.squeeze(1)\n",
        "        return Z1, Z2, Z3\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Z = torch.flatten(x, 1)\n",
        "        Z=x\n",
        "        Z1 = self.mlp(Z)\n",
        "\n",
        "        Z2=self.decoder(Z1)\n",
        "        Z2=Z2.squeeze(1)\n",
        "        return Z2\n",
        "\n"
      ],
      "metadata": {
        "id": "deK6tggV7Oos"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, device, train_loader, optimizer, scheduler, epoch, criterion):\n",
        "    model.train()  # Important set model to train mode (affects dropout, batch norm etc)\n",
        "\n",
        "    loss_history = []\n",
        "    accuracy_history = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        data=data.to(device)\n",
        "        target=target.to(device)\n",
        "        output = model.forward(data)  # TODO\n",
        "        loss =  criterion(output, target)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        loss.backward()        # Backpropagation\n",
        "        optimizer.step()       # Update the weights\n",
        "        scheduler.step()\n",
        "\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        if batch_idx % (len(train_loader.dataset) // len(data) // 4) == 0:\n",
        "            print(\n",
        "                f\"Train Epoch: {epoch}-{batch_idx} batch_loss={loss.item()/len(data):0.2e}\"\n",
        "            )\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "def train_epoch_conv(model, device, train_loader, optimizer, epoch, criterion,scheduler):\n",
        "  model.train()  # Important set model to train mode (affects dropout, batch norm etc)\n",
        "\n",
        "  loss_history = []\n",
        "  accuracy_history = []\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "      data=data.to(device)\n",
        "      target=target.to(device)\n",
        "      Z1,Z2,Z3 = model.forward(data,target)\n",
        "      # print(Z1.shape,Z2.shape,Z3.shape)\n",
        "      loss =  0.5* criterion(Z1, Z2)+ 0.5* criterion(target, Z3)\n",
        "\n",
        "      optimizer.zero_grad()  # Zero the gradients\n",
        "      loss.backward()        # Backpropagation\n",
        "      optimizer.step()       # Update the weights\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "      loss_history.append(loss.item())\n",
        "\n",
        "      if batch_idx % (len(train_loader.dataset) // len(data) // 4) == 0:\n",
        "          print(\n",
        "              f\"Train Epoch: {epoch}-{batch_idx} batch_loss={loss.item()/len(data):0.2e}\"\n",
        "          )\n",
        "\n",
        "  return loss_history\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, device, val_loader, criterion):\n",
        "    model.eval()  # Important set model to eval mode (affects dropout, batch norm etc)\n",
        "    test_loss = 0\n",
        "    test_rel_loss = 0\n",
        "    for data, target in val_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item() * len(data)\n",
        "        # test_rel_loss += criterion(output, target).item()/torch.norm(target).item() * len(data)\n",
        "        test_rel_loss += ((torch.norm((output-target).view(output.shape[0], -1), dim=1)/torch.norm((target).view(output.shape[0],-1), dim=1)).sum()).item()\n",
        "\n",
        "    test_loss /= len(val_loader.dataset)\n",
        "    test_rel_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"Test set: Average loss: {:.4f} Average relative error: {:.4f}\".format(\n",
        "            test_loss, test_rel_loss\n",
        "        )\n",
        "    )\n",
        "    return test_loss,test_rel_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_conv(model, device, val_loader, criterion):\n",
        "    model.eval()  # Important set model to eval mode (affects dropout, batch norm etc)\n",
        "    test_loss = 0\n",
        "    test_rel_loss = 0\n",
        "    for data, target in val_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model.predict(data)\n",
        "        Z1,Z2,Z3 = model.forward(data,target)\n",
        "        test_loss +=  (0.5* criterion(Z1, Z2).item()+ 0.5* criterion(target, Z3).item()) * len(data)\n",
        "        test_rel_loss += ((torch.norm((output-target).view(output.shape[0], -1), dim=1)/torch.norm((target).view(output.shape[0],-1), dim=1)).sum()).item()\n",
        "\n",
        "    test_loss /= len(val_loader.dataset)\n",
        "    test_rel_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"Test set: Average loss: {:.4f} Average relative error: {:.4f}\".format(\n",
        "            test_loss, test_rel_loss\n",
        "        )\n",
        "    )\n",
        "    return test_loss,test_rel_loss\n",
        "\n",
        "\n",
        "# def run_vel_training(num_epochs, lr, batch_size, device=\"cuda\"):\n",
        "\n",
        "#     train_set = Fluid_Dataset(train_params, train_vel)\n",
        "\n",
        "#     val_set = Fluid_Dataset(test_params, test_vel)\n",
        "\n",
        "#     train_loader = torch.utils.data.DataLoader(\n",
        "#         train_set,\n",
        "#         batch_size=batch_size,\n",
        "#         shuffle=True,  # Can be important for training\n",
        "#         pin_memory=torch.cuda.is_available(),\n",
        "#         drop_last=True,\n",
        "#         num_workers=2,\n",
        "#     )\n",
        "#     val_loader = torch.utils.data.DataLoader(\n",
        "#         val_set,\n",
        "#         batch_size=batch_size,\n",
        "#     )\n",
        "\n",
        "#     # ===== Model, Optimizer and Criterion =====\n",
        "#     model = MLP_vel_model()\n",
        "#     model = model.to(device=device)\n",
        "#     optimizer = torch.optim.SGD(\n",
        "#         model.parameters(),\n",
        "#         lr=lr,\n",
        "#         weight_decay=1e-4\n",
        "#     )\n",
        "#     criterion = torch.nn.functional.mse_loss\n",
        "\n",
        "#     # ===== Train Model =====\n",
        "#     train_loss_history = []\n",
        "#     val_loss_history = []\n",
        "#     val_rel_loss_history = []\n",
        "\n",
        "#     for epoch in range(1, num_epochs + 1):\n",
        "#         train_loss = train_epoch(\n",
        "#             model, device, train_loader, optimizer, epoch, criterion\n",
        "#         )\n",
        "#         train_loss_history.extend(train_loss)\n",
        "\n",
        "#         val_loss,val_rel_loss= validate(model, device, val_loader, criterion)\n",
        "#         val_loss_history.append(val_loss)\n",
        "#         val_rel_loss_history.append(val_rel_loss)\n",
        "\n",
        "#     # ===== Plot training curves =====\n",
        "#     n_train = len(train_loss_history)\n",
        "#     t_train = num_epochs * np.arange(n_train) / n_train\n",
        "#     t_val = np.arange(1, num_epochs + 1)\n",
        "\n",
        "#     plt.figure()\n",
        "#     plt.plot(t_train, train_loss_history, label=\"Train\")\n",
        "#     plt.plot(t_val, val_loss_history, label=\"Val\")\n",
        "#     plt.legend()\n",
        "#     plt.xlabel(\"Epoch\")\n",
        "#     plt.ylabel(\"Loss\")\n",
        "\n",
        "#     plt.figure()\n",
        "#     plt.plot(t_val, val_rel_loss_history, label=\"Val rel err\")\n",
        "#     plt.legend()\n",
        "#     plt.xlabel(\"Epoch\")\n",
        "#     plt.ylabel(\"rel err\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def run_training(model,train_output,test_output,num_epochs, lr, batch_size, device=\"cuda\"):\n",
        "\n",
        "  train_set = Fluid_Dataset(train_params, train_output)\n",
        "\n",
        "  val_set = Fluid_Dataset(test_params, test_output)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_set,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,  # Can be important for training\n",
        "      pin_memory=torch.cuda.is_available(),\n",
        "      drop_last=True,\n",
        "      num_workers=2,\n",
        "  )\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      val_set,\n",
        "      batch_size=batch_size,\n",
        "  )\n",
        "\n",
        "  # ===== Model, Optimizer and Criterion =====\n",
        "  model = model.to(device=device)\n",
        "  optimizer = torch.optim.Adam(\n",
        "      model.parameters(),\n",
        "      lr=lr,\n",
        "  )\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.99)\n",
        "\n",
        "  criterion = torch.nn.functional.mse_loss\n",
        "\n",
        "  # ===== Train Model =====\n",
        "  train_loss_history = []\n",
        "  val_loss_history = []\n",
        "  val_rel_loss_history = []\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "      train_loss = train_epoch(\n",
        "          model, device, train_loader, optimizer, scheduler, epoch, criterion\n",
        "      )\n",
        "      train_loss_history.extend(train_loss)\n",
        "\n",
        "      val_loss,val_rel_loss= validate(model, device, val_loader, criterion)\n",
        "      val_loss_history.append(val_loss)\n",
        "      val_rel_loss_history.append(val_rel_loss)\n",
        "\n",
        "  # ===== Plot training curves =====\n",
        "  n_train = len(train_loss_history)\n",
        "  t_train = num_epochs * np.arange(n_train) / n_train\n",
        "  t_val = np.arange(1, num_epochs + 1)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(t_train, train_loss_history, label=\"Train\")\n",
        "  plt.plot(t_val, val_loss_history, label=\"Val\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(t_val, val_rel_loss_history, label=\"Val rel err\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"rel err\")\n",
        "\n",
        "  return model\n",
        "\n",
        "def run_training_auto(model,train_output,test_output,num_epochs, lr, batch_size, device=\"cuda\"):\n",
        "\n",
        "  train_set = Fluid_Dataset_auto(train_output)\n",
        "\n",
        "  val_set = Fluid_Dataset_auto(test_output)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_set,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,  # Can be important for training\n",
        "      pin_memory=torch.cuda.is_available(),\n",
        "      drop_last=True,\n",
        "      num_workers=2,\n",
        "  )\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      val_set,\n",
        "      batch_size=batch_size,\n",
        "  )\n",
        "\n",
        "  # ===== Model, Optimizer and Criterion =====\n",
        "  model = model.to(device=device)\n",
        "  optimizer = torch.optim.Adam(\n",
        "      model.parameters(),\n",
        "      lr=lr,\n",
        "  )\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.99)\n",
        "\n",
        "  criterion = torch.nn.functional.mse_loss\n",
        "\n",
        "  # ===== Train Model =====\n",
        "  train_loss_history = []\n",
        "  val_loss_history = []\n",
        "  val_rel_loss_history = []\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "      train_loss = train_epoch(\n",
        "          model, device, train_loader, optimizer, scheduler, epoch, criterion\n",
        "      )\n",
        "      train_loss_history.extend(train_loss)\n",
        "\n",
        "      val_loss,val_rel_loss= validate(model, device, val_loader, criterion)\n",
        "      val_loss_history.append(val_loss)\n",
        "      val_rel_loss_history.append(val_rel_loss)\n",
        "\n",
        "  # ===== Plot training curves =====\n",
        "  n_train = len(train_loss_history)\n",
        "  t_train = num_epochs * np.arange(n_train) / n_train\n",
        "  t_val = np.arange(1, num_epochs + 1)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(t_train, train_loss_history, label=\"Train\")\n",
        "  plt.plot(t_val, val_loss_history, label=\"Val\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(t_val, val_rel_loss_history, label=\"Val rel err\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"rel err\")\n",
        "\n",
        "  return model\n",
        "\n",
        "def run_training_conv(model,train_output,test_output,num_epochs, lr, batch_size, wd, device=\"cuda\"):\n",
        "\n",
        "  train_set = Fluid_Dataset(train_params, train_output)\n",
        "\n",
        "  val_set = Fluid_Dataset(test_params, test_output)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_set,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,  # Can be important for training\n",
        "      pin_memory=torch.cuda.is_available(),\n",
        "      drop_last=True,\n",
        "      num_workers=2,\n",
        "  )\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      val_set,\n",
        "      batch_size=batch_size,\n",
        "  )\n",
        "\n",
        "  # ===== Model, Optimizer and Criterion =====\n",
        "  model = model.to(device=device)\n",
        "  optimizer = torch.optim.Adam(\n",
        "      model.parameters(),\n",
        "      lr=lr,\n",
        "      weight_decay=wd\n",
        "  )\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.99)\n",
        "\n",
        "  criterion = torch.nn.functional.mse_loss\n",
        "\n",
        "  # ===== Train Model =====\n",
        "  train_loss_history = []\n",
        "  val_loss_history = []\n",
        "  val_rel_loss_history = []\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "      train_loss = train_epoch_conv(\n",
        "          model, device, train_loader, optimizer, epoch, criterion,scheduler\n",
        "      )\n",
        "      train_loss_history.extend(train_loss)\n",
        "\n",
        "      val_loss, val_rel_loss= validate_conv(model, device, val_loader, criterion)\n",
        "      val_loss_history.append(val_loss)\n",
        "      val_rel_loss_history.append(val_loss)\n",
        "\n",
        "  # ===== Plot training curves =====\n",
        "  n_train = len(train_loss_history)\n",
        "  t_train = num_epochs * np.arange(n_train) / n_train\n",
        "  t_val = np.arange(1, num_epochs + 1)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(t_train, train_loss_history, label=\"Train\")\n",
        "  plt.plot(t_val, val_loss_history, label=\"Val\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(t_val, val_rel_loss_history, label=\"Val rel err\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"rel err\")\n",
        "\n",
        "\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "UTpEF7Nbyifb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lr = 0.01\n",
        "# batch_size = 32\n",
        "# num_epochs = 500\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model_vel_trained=run_training(MLP_vel_model(), train_vel, test_vel, num_epochs, lr, batch_size, device)\n",
        "\n",
        "\n",
        "lr = 0.0001\n",
        "batch_size = 32\n",
        "num_epochs = 500\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wd=1e-3\n",
        "\n",
        "model_vel_trained_autoencoder=run_training_conv(MLP_vel_ConvAutoencoder(), train_vel, test_vel, num_epochs, lr, batch_size, wd, device)\n",
        "\n",
        "# lr = 0.0005\n",
        "# batch_size = 32\n",
        "# num_epochs = 200\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# wd=1e-3\n",
        "\n",
        "# model_vel_trained_autoencoder=run_training_auto(Autoencoder(), train_vel, test_vel, num_epochs, lr, batch_size, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tR7Fp6yKFBkA",
        "outputId": "10baaebc-c931-4de5-b500-585331923c06"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1-0 batch_loss=8.36e-01\n",
            "Train Epoch: 1-12 batch_loss=8.21e-01\n",
            "Train Epoch: 1-24 batch_loss=8.28e-01\n",
            "Train Epoch: 1-36 batch_loss=8.28e-01\n",
            "Test set: Average loss: 25.6492 Average relative error: 0.9883\n",
            "Train Epoch: 2-0 batch_loss=7.86e-01\n",
            "Train Epoch: 2-12 batch_loss=7.68e-01\n",
            "Train Epoch: 2-24 batch_loss=7.26e-01\n",
            "Train Epoch: 2-36 batch_loss=6.60e-01\n",
            "Test set: Average loss: 18.5284 Average relative error: 0.8811\n",
            "Train Epoch: 3-0 batch_loss=5.78e-01\n",
            "Train Epoch: 3-12 batch_loss=5.06e-01\n",
            "Train Epoch: 3-24 batch_loss=4.35e-01\n",
            "Train Epoch: 3-36 batch_loss=3.69e-01\n",
            "Test set: Average loss: 9.0505 Average relative error: 0.6752\n",
            "Train Epoch: 4-0 batch_loss=2.75e-01\n",
            "Train Epoch: 4-12 batch_loss=1.96e-01\n",
            "Train Epoch: 4-24 batch_loss=1.24e-01\n",
            "Train Epoch: 4-36 batch_loss=1.02e-01\n",
            "Test set: Average loss: 2.6143 Average relative error: 0.3677\n",
            "Train Epoch: 5-0 batch_loss=7.69e-02\n",
            "Train Epoch: 5-12 batch_loss=7.91e-02\n",
            "Train Epoch: 5-24 batch_loss=7.61e-02\n",
            "Train Epoch: 5-36 batch_loss=5.89e-02\n",
            "Test set: Average loss: 2.2200 Average relative error: 0.2758\n",
            "Train Epoch: 6-0 batch_loss=7.00e-02\n",
            "Train Epoch: 6-12 batch_loss=7.34e-02\n",
            "Train Epoch: 6-24 batch_loss=7.09e-02\n",
            "Train Epoch: 6-36 batch_loss=6.76e-02\n",
            "Test set: Average loss: 2.2119 Average relative error: 0.2728\n",
            "Train Epoch: 7-0 batch_loss=7.95e-02\n",
            "Train Epoch: 7-12 batch_loss=6.02e-02\n",
            "Train Epoch: 7-24 batch_loss=7.70e-02\n",
            "Train Epoch: 7-36 batch_loss=6.24e-02\n",
            "Test set: Average loss: 2.2097 Average relative error: 0.2729\n",
            "Train Epoch: 8-0 batch_loss=7.56e-02\n",
            "Train Epoch: 8-12 batch_loss=7.53e-02\n",
            "Train Epoch: 8-24 batch_loss=7.27e-02\n",
            "Train Epoch: 8-36 batch_loss=7.46e-02\n",
            "Test set: Average loss: 2.2086 Average relative error: 0.2728\n",
            "Train Epoch: 9-0 batch_loss=7.28e-02\n",
            "Train Epoch: 9-12 batch_loss=7.08e-02\n",
            "Train Epoch: 9-24 batch_loss=7.33e-02\n",
            "Train Epoch: 9-36 batch_loss=6.44e-02\n",
            "Test set: Average loss: 2.1854 Average relative error: 0.2722\n",
            "Train Epoch: 10-0 batch_loss=7.23e-02\n",
            "Train Epoch: 10-12 batch_loss=7.40e-02\n",
            "Train Epoch: 10-24 batch_loss=7.21e-02\n",
            "Train Epoch: 10-36 batch_loss=7.17e-02\n",
            "Test set: Average loss: 2.1429 Average relative error: 0.2710\n",
            "Train Epoch: 11-0 batch_loss=5.41e-02\n",
            "Train Epoch: 11-12 batch_loss=6.86e-02\n",
            "Train Epoch: 11-24 batch_loss=6.00e-02\n",
            "Train Epoch: 11-36 batch_loss=6.99e-02\n",
            "Test set: Average loss: 2.0217 Average relative error: 0.2668\n",
            "Train Epoch: 12-0 batch_loss=6.41e-02\n",
            "Train Epoch: 12-12 batch_loss=5.46e-02\n",
            "Train Epoch: 12-24 batch_loss=6.40e-02\n",
            "Train Epoch: 12-36 batch_loss=6.05e-02\n",
            "Test set: Average loss: 1.7169 Average relative error: 0.2537\n",
            "Train Epoch: 13-0 batch_loss=5.82e-02\n",
            "Train Epoch: 13-12 batch_loss=4.93e-02\n",
            "Train Epoch: 13-24 batch_loss=4.04e-02\n",
            "Train Epoch: 13-36 batch_loss=4.74e-02\n",
            "Test set: Average loss: 1.3111 Average relative error: 0.2283\n",
            "Train Epoch: 14-0 batch_loss=4.32e-02\n",
            "Train Epoch: 14-12 batch_loss=3.52e-02\n",
            "Train Epoch: 14-24 batch_loss=3.22e-02\n",
            "Train Epoch: 14-36 batch_loss=3.40e-02\n",
            "Test set: Average loss: 0.9906 Average relative error: 0.2006\n",
            "Train Epoch: 15-0 batch_loss=2.56e-02\n",
            "Train Epoch: 15-12 batch_loss=2.99e-02\n",
            "Train Epoch: 15-24 batch_loss=2.93e-02\n",
            "Train Epoch: 15-36 batch_loss=3.39e-02\n",
            "Test set: Average loss: 0.8756 Average relative error: 0.1835\n",
            "Train Epoch: 16-0 batch_loss=3.32e-02\n",
            "Train Epoch: 16-12 batch_loss=2.79e-02\n",
            "Train Epoch: 16-24 batch_loss=2.45e-02\n",
            "Train Epoch: 16-36 batch_loss=2.67e-02\n",
            "Test set: Average loss: 0.8505 Average relative error: 0.1769\n",
            "Train Epoch: 17-0 batch_loss=2.99e-02\n",
            "Train Epoch: 17-12 batch_loss=2.19e-02\n",
            "Train Epoch: 17-24 batch_loss=2.52e-02\n",
            "Train Epoch: 17-36 batch_loss=2.63e-02\n",
            "Test set: Average loss: 0.8371 Average relative error: 0.1736\n",
            "Train Epoch: 18-0 batch_loss=2.33e-02\n",
            "Train Epoch: 18-12 batch_loss=2.97e-02\n",
            "Train Epoch: 18-24 batch_loss=2.79e-02\n",
            "Train Epoch: 18-36 batch_loss=2.77e-02\n",
            "Test set: Average loss: 0.8305 Average relative error: 0.1720\n",
            "Train Epoch: 19-0 batch_loss=2.90e-02\n",
            "Train Epoch: 19-12 batch_loss=2.60e-02\n",
            "Train Epoch: 19-24 batch_loss=2.79e-02\n",
            "Train Epoch: 19-36 batch_loss=2.63e-02\n",
            "Test set: Average loss: 0.8315 Average relative error: 0.1716\n",
            "Train Epoch: 20-0 batch_loss=2.93e-02\n",
            "Train Epoch: 20-12 batch_loss=2.36e-02\n",
            "Train Epoch: 20-24 batch_loss=2.66e-02\n",
            "Train Epoch: 20-36 batch_loss=2.82e-02\n",
            "Test set: Average loss: 0.8295 Average relative error: 0.1711\n",
            "Train Epoch: 21-0 batch_loss=2.56e-02\n",
            "Train Epoch: 21-12 batch_loss=2.64e-02\n",
            "Train Epoch: 21-24 batch_loss=2.91e-02\n",
            "Train Epoch: 21-36 batch_loss=2.82e-02\n",
            "Test set: Average loss: 0.8271 Average relative error: 0.1708\n",
            "Train Epoch: 22-0 batch_loss=2.98e-02\n",
            "Train Epoch: 22-12 batch_loss=2.68e-02\n",
            "Train Epoch: 22-24 batch_loss=2.22e-02\n",
            "Train Epoch: 22-36 batch_loss=2.78e-02\n",
            "Test set: Average loss: 0.8257 Average relative error: 0.1705\n",
            "Train Epoch: 23-0 batch_loss=2.68e-02\n",
            "Train Epoch: 23-12 batch_loss=2.93e-02\n",
            "Train Epoch: 23-24 batch_loss=2.73e-02\n",
            "Train Epoch: 23-36 batch_loss=2.45e-02\n",
            "Test set: Average loss: 0.8261 Average relative error: 0.1705\n",
            "Train Epoch: 24-0 batch_loss=2.66e-02\n",
            "Train Epoch: 24-12 batch_loss=2.73e-02\n",
            "Train Epoch: 24-24 batch_loss=2.66e-02\n",
            "Train Epoch: 24-36 batch_loss=2.96e-02\n",
            "Test set: Average loss: 0.8248 Average relative error: 0.1702\n",
            "Train Epoch: 25-0 batch_loss=2.40e-02\n",
            "Train Epoch: 25-12 batch_loss=2.58e-02\n",
            "Train Epoch: 25-24 batch_loss=2.75e-02\n",
            "Train Epoch: 25-36 batch_loss=2.40e-02\n",
            "Test set: Average loss: 0.8251 Average relative error: 0.1702\n",
            "Train Epoch: 26-0 batch_loss=2.70e-02\n",
            "Train Epoch: 26-12 batch_loss=2.51e-02\n",
            "Train Epoch: 26-24 batch_loss=2.68e-02\n",
            "Train Epoch: 26-36 batch_loss=2.49e-02\n",
            "Test set: Average loss: 0.8254 Average relative error: 0.1703\n",
            "Train Epoch: 27-0 batch_loss=2.46e-02\n",
            "Train Epoch: 27-12 batch_loss=2.43e-02\n",
            "Train Epoch: 27-24 batch_loss=2.70e-02\n",
            "Train Epoch: 27-36 batch_loss=2.75e-02\n",
            "Test set: Average loss: 0.8233 Average relative error: 0.1700\n",
            "Train Epoch: 28-0 batch_loss=2.37e-02\n",
            "Train Epoch: 28-12 batch_loss=2.69e-02\n",
            "Train Epoch: 28-24 batch_loss=2.30e-02\n",
            "Train Epoch: 28-36 batch_loss=2.61e-02\n",
            "Test set: Average loss: 0.8232 Average relative error: 0.1700\n",
            "Train Epoch: 29-0 batch_loss=2.48e-02\n",
            "Train Epoch: 29-12 batch_loss=2.11e-02\n",
            "Train Epoch: 29-24 batch_loss=2.60e-02\n",
            "Train Epoch: 29-36 batch_loss=2.36e-02\n",
            "Test set: Average loss: 0.8254 Average relative error: 0.1703\n",
            "Train Epoch: 30-0 batch_loss=2.58e-02\n",
            "Train Epoch: 30-12 batch_loss=2.74e-02\n",
            "Train Epoch: 30-24 batch_loss=2.69e-02\n",
            "Train Epoch: 30-36 batch_loss=2.70e-02\n",
            "Test set: Average loss: 0.8248 Average relative error: 0.1701\n",
            "Train Epoch: 31-0 batch_loss=2.85e-02\n",
            "Train Epoch: 31-12 batch_loss=2.58e-02\n",
            "Train Epoch: 31-24 batch_loss=2.57e-02\n",
            "Train Epoch: 31-36 batch_loss=2.65e-02\n",
            "Test set: Average loss: 0.8264 Average relative error: 0.1704\n",
            "Train Epoch: 32-0 batch_loss=2.25e-02\n",
            "Train Epoch: 32-12 batch_loss=2.32e-02\n",
            "Train Epoch: 32-24 batch_loss=2.60e-02\n",
            "Train Epoch: 32-36 batch_loss=2.69e-02\n",
            "Test set: Average loss: 0.8263 Average relative error: 0.1703\n",
            "Train Epoch: 33-0 batch_loss=2.33e-02\n",
            "Train Epoch: 33-12 batch_loss=2.55e-02\n",
            "Train Epoch: 33-24 batch_loss=3.01e-02\n",
            "Train Epoch: 33-36 batch_loss=2.85e-02\n",
            "Test set: Average loss: 0.8249 Average relative error: 0.1702\n",
            "Train Epoch: 34-0 batch_loss=2.83e-02\n",
            "Train Epoch: 34-12 batch_loss=2.72e-02\n",
            "Train Epoch: 34-24 batch_loss=3.32e-02\n",
            "Train Epoch: 34-36 batch_loss=2.36e-02\n",
            "Test set: Average loss: 0.8232 Average relative error: 0.1699\n",
            "Train Epoch: 35-0 batch_loss=2.78e-02\n",
            "Train Epoch: 35-12 batch_loss=2.30e-02\n",
            "Train Epoch: 35-24 batch_loss=2.75e-02\n",
            "Train Epoch: 35-36 batch_loss=3.01e-02\n",
            "Test set: Average loss: 0.8244 Average relative error: 0.1701\n",
            "Train Epoch: 36-0 batch_loss=2.75e-02\n",
            "Train Epoch: 36-12 batch_loss=2.40e-02\n",
            "Train Epoch: 36-24 batch_loss=2.90e-02\n",
            "Train Epoch: 36-36 batch_loss=2.78e-02\n",
            "Test set: Average loss: 0.8229 Average relative error: 0.1699\n",
            "Train Epoch: 37-0 batch_loss=2.69e-02\n",
            "Train Epoch: 37-12 batch_loss=2.53e-02\n",
            "Train Epoch: 37-24 batch_loss=2.31e-02\n",
            "Train Epoch: 37-36 batch_loss=2.45e-02\n",
            "Test set: Average loss: 0.8244 Average relative error: 0.1700\n",
            "Train Epoch: 38-0 batch_loss=2.75e-02\n",
            "Train Epoch: 38-12 batch_loss=2.04e-02\n",
            "Train Epoch: 38-24 batch_loss=2.64e-02\n",
            "Train Epoch: 38-36 batch_loss=2.53e-02\n",
            "Test set: Average loss: 0.8229 Average relative error: 0.1699\n",
            "Train Epoch: 39-0 batch_loss=2.78e-02\n",
            "Train Epoch: 39-12 batch_loss=2.61e-02\n",
            "Train Epoch: 39-24 batch_loss=2.93e-02\n",
            "Train Epoch: 39-36 batch_loss=2.50e-02\n",
            "Test set: Average loss: 0.8246 Average relative error: 0.1701\n",
            "Train Epoch: 40-0 batch_loss=2.28e-02\n",
            "Train Epoch: 40-12 batch_loss=2.44e-02\n",
            "Train Epoch: 40-24 batch_loss=3.19e-02\n",
            "Train Epoch: 40-36 batch_loss=2.63e-02\n",
            "Test set: Average loss: 0.8222 Average relative error: 0.1699\n",
            "Train Epoch: 41-0 batch_loss=2.47e-02\n",
            "Train Epoch: 41-12 batch_loss=2.92e-02\n",
            "Train Epoch: 41-24 batch_loss=2.61e-02\n",
            "Train Epoch: 41-36 batch_loss=2.81e-02\n",
            "Test set: Average loss: 0.8238 Average relative error: 0.1700\n",
            "Train Epoch: 42-0 batch_loss=2.29e-02\n",
            "Train Epoch: 42-12 batch_loss=2.53e-02\n",
            "Train Epoch: 42-24 batch_loss=2.66e-02\n",
            "Train Epoch: 42-36 batch_loss=2.78e-02\n",
            "Test set: Average loss: 0.8217 Average relative error: 0.1698\n",
            "Train Epoch: 43-0 batch_loss=2.47e-02\n",
            "Train Epoch: 43-12 batch_loss=2.52e-02\n",
            "Train Epoch: 43-24 batch_loss=2.31e-02\n",
            "Train Epoch: 43-36 batch_loss=2.60e-02\n",
            "Test set: Average loss: 0.8201 Average relative error: 0.1696\n",
            "Train Epoch: 44-0 batch_loss=2.67e-02\n",
            "Train Epoch: 44-12 batch_loss=2.39e-02\n",
            "Train Epoch: 44-24 batch_loss=2.49e-02\n",
            "Train Epoch: 44-36 batch_loss=2.72e-02\n",
            "Test set: Average loss: 0.8238 Average relative error: 0.1701\n",
            "Train Epoch: 45-0 batch_loss=2.76e-02\n",
            "Train Epoch: 45-12 batch_loss=2.90e-02\n",
            "Train Epoch: 45-24 batch_loss=2.25e-02\n",
            "Train Epoch: 45-36 batch_loss=2.63e-02\n",
            "Test set: Average loss: 0.8211 Average relative error: 0.1697\n",
            "Train Epoch: 46-0 batch_loss=2.10e-02\n",
            "Train Epoch: 46-12 batch_loss=2.78e-02\n",
            "Train Epoch: 46-24 batch_loss=2.94e-02\n",
            "Train Epoch: 46-36 batch_loss=2.75e-02\n",
            "Test set: Average loss: 0.8179 Average relative error: 0.1694\n",
            "Train Epoch: 47-0 batch_loss=2.60e-02\n",
            "Train Epoch: 47-12 batch_loss=2.46e-02\n",
            "Train Epoch: 47-24 batch_loss=2.71e-02\n",
            "Train Epoch: 47-36 batch_loss=2.75e-02\n",
            "Test set: Average loss: 0.8181 Average relative error: 0.1695\n",
            "Train Epoch: 48-0 batch_loss=2.37e-02\n",
            "Train Epoch: 48-12 batch_loss=2.82e-02\n",
            "Train Epoch: 48-24 batch_loss=2.44e-02\n",
            "Train Epoch: 48-36 batch_loss=2.60e-02\n",
            "Test set: Average loss: 0.8159 Average relative error: 0.1693\n",
            "Train Epoch: 49-0 batch_loss=2.85e-02\n",
            "Train Epoch: 49-12 batch_loss=2.66e-02\n",
            "Train Epoch: 49-24 batch_loss=2.38e-02\n",
            "Train Epoch: 49-36 batch_loss=2.63e-02\n",
            "Test set: Average loss: 0.8121 Average relative error: 0.1691\n",
            "Train Epoch: 50-0 batch_loss=2.80e-02\n",
            "Train Epoch: 50-12 batch_loss=2.41e-02\n",
            "Train Epoch: 50-24 batch_loss=3.04e-02\n",
            "Train Epoch: 50-36 batch_loss=2.30e-02\n",
            "Test set: Average loss: 0.8011 Average relative error: 0.1682\n",
            "Train Epoch: 51-0 batch_loss=2.94e-02\n",
            "Train Epoch: 51-12 batch_loss=2.51e-02\n",
            "Train Epoch: 51-24 batch_loss=2.57e-02\n",
            "Train Epoch: 51-36 batch_loss=2.60e-02\n",
            "Test set: Average loss: 0.7807 Average relative error: 0.1662\n",
            "Train Epoch: 52-0 batch_loss=2.27e-02\n",
            "Train Epoch: 52-12 batch_loss=2.16e-02\n",
            "Train Epoch: 52-24 batch_loss=2.62e-02\n",
            "Train Epoch: 52-36 batch_loss=2.62e-02\n",
            "Test set: Average loss: 0.7550 Average relative error: 0.1637\n",
            "Train Epoch: 53-0 batch_loss=2.38e-02\n",
            "Train Epoch: 53-12 batch_loss=2.41e-02\n",
            "Train Epoch: 53-24 batch_loss=2.59e-02\n",
            "Train Epoch: 53-36 batch_loss=2.29e-02\n",
            "Test set: Average loss: 0.7360 Average relative error: 0.1612\n",
            "Train Epoch: 54-0 batch_loss=2.26e-02\n",
            "Train Epoch: 54-12 batch_loss=2.19e-02\n",
            "Train Epoch: 54-24 batch_loss=2.33e-02\n",
            "Train Epoch: 54-36 batch_loss=2.34e-02\n",
            "Test set: Average loss: 0.7212 Average relative error: 0.1596\n",
            "Train Epoch: 55-0 batch_loss=2.03e-02\n",
            "Train Epoch: 55-12 batch_loss=2.22e-02\n",
            "Train Epoch: 55-24 batch_loss=2.50e-02\n",
            "Train Epoch: 55-36 batch_loss=2.20e-02\n",
            "Test set: Average loss: 0.7118 Average relative error: 0.1581\n",
            "Train Epoch: 56-0 batch_loss=2.07e-02\n",
            "Train Epoch: 56-12 batch_loss=1.86e-02\n",
            "Train Epoch: 56-24 batch_loss=2.30e-02\n",
            "Train Epoch: 56-36 batch_loss=2.18e-02\n",
            "Test set: Average loss: 0.7077 Average relative error: 0.1578\n",
            "Train Epoch: 57-0 batch_loss=2.37e-02\n",
            "Train Epoch: 57-12 batch_loss=2.13e-02\n",
            "Train Epoch: 57-24 batch_loss=2.24e-02\n",
            "Train Epoch: 57-36 batch_loss=2.16e-02\n",
            "Test set: Average loss: 0.7040 Average relative error: 0.1573\n",
            "Train Epoch: 58-0 batch_loss=2.26e-02\n",
            "Train Epoch: 58-12 batch_loss=2.18e-02\n",
            "Train Epoch: 58-24 batch_loss=2.10e-02\n",
            "Train Epoch: 58-36 batch_loss=1.99e-02\n",
            "Test set: Average loss: 0.6951 Average relative error: 0.1558\n",
            "Train Epoch: 59-0 batch_loss=2.56e-02\n",
            "Train Epoch: 59-12 batch_loss=2.39e-02\n",
            "Train Epoch: 59-24 batch_loss=1.88e-02\n",
            "Train Epoch: 59-36 batch_loss=2.45e-02\n",
            "Test set: Average loss: 0.6886 Average relative error: 0.1549\n",
            "Train Epoch: 60-0 batch_loss=2.41e-02\n",
            "Train Epoch: 60-12 batch_loss=2.33e-02\n",
            "Train Epoch: 60-24 batch_loss=1.90e-02\n",
            "Train Epoch: 60-36 batch_loss=2.10e-02\n",
            "Test set: Average loss: 0.6809 Average relative error: 0.1539\n",
            "Train Epoch: 61-0 batch_loss=2.43e-02\n",
            "Train Epoch: 61-12 batch_loss=2.19e-02\n",
            "Train Epoch: 61-24 batch_loss=2.06e-02\n",
            "Train Epoch: 61-36 batch_loss=1.82e-02\n",
            "Test set: Average loss: 0.6751 Average relative error: 0.1532\n",
            "Train Epoch: 62-0 batch_loss=2.15e-02\n",
            "Train Epoch: 62-12 batch_loss=1.90e-02\n",
            "Train Epoch: 62-24 batch_loss=2.12e-02\n",
            "Train Epoch: 62-36 batch_loss=1.81e-02\n",
            "Test set: Average loss: 0.6678 Average relative error: 0.1524\n",
            "Train Epoch: 63-0 batch_loss=2.24e-02\n",
            "Train Epoch: 63-12 batch_loss=2.14e-02\n",
            "Train Epoch: 63-24 batch_loss=2.05e-02\n",
            "Train Epoch: 63-36 batch_loss=2.14e-02\n",
            "Test set: Average loss: 0.6632 Average relative error: 0.1518\n",
            "Train Epoch: 64-0 batch_loss=2.15e-02\n",
            "Train Epoch: 64-12 batch_loss=2.25e-02\n",
            "Train Epoch: 64-24 batch_loss=2.11e-02\n",
            "Train Epoch: 64-36 batch_loss=1.95e-02\n",
            "Test set: Average loss: 0.6531 Average relative error: 0.1508\n",
            "Train Epoch: 65-0 batch_loss=1.99e-02\n",
            "Train Epoch: 65-12 batch_loss=2.33e-02\n",
            "Train Epoch: 65-24 batch_loss=1.95e-02\n",
            "Train Epoch: 65-36 batch_loss=1.82e-02\n",
            "Test set: Average loss: 0.6436 Average relative error: 0.1494\n",
            "Train Epoch: 66-0 batch_loss=2.17e-02\n",
            "Train Epoch: 66-12 batch_loss=1.93e-02\n",
            "Train Epoch: 66-24 batch_loss=1.98e-02\n",
            "Train Epoch: 66-36 batch_loss=2.38e-02\n",
            "Test set: Average loss: 0.6349 Average relative error: 0.1489\n",
            "Train Epoch: 67-0 batch_loss=2.08e-02\n",
            "Train Epoch: 67-12 batch_loss=1.78e-02\n",
            "Train Epoch: 67-24 batch_loss=1.81e-02\n",
            "Train Epoch: 67-36 batch_loss=1.63e-02\n",
            "Test set: Average loss: 0.6206 Average relative error: 0.1469\n",
            "Train Epoch: 68-0 batch_loss=1.59e-02\n",
            "Train Epoch: 68-12 batch_loss=2.16e-02\n",
            "Train Epoch: 68-24 batch_loss=1.99e-02\n",
            "Train Epoch: 68-36 batch_loss=1.40e-02\n",
            "Test set: Average loss: 0.6005 Average relative error: 0.1444\n",
            "Train Epoch: 69-0 batch_loss=1.96e-02\n",
            "Train Epoch: 69-12 batch_loss=1.47e-02\n",
            "Train Epoch: 69-24 batch_loss=1.89e-02\n",
            "Train Epoch: 69-36 batch_loss=1.55e-02\n",
            "Test set: Average loss: 0.5778 Average relative error: 0.1420\n",
            "Train Epoch: 70-0 batch_loss=1.81e-02\n",
            "Train Epoch: 70-12 batch_loss=1.87e-02\n",
            "Train Epoch: 70-24 batch_loss=1.91e-02\n",
            "Train Epoch: 70-36 batch_loss=1.47e-02\n",
            "Test set: Average loss: 0.5511 Average relative error: 0.1389\n",
            "Train Epoch: 71-0 batch_loss=1.60e-02\n",
            "Train Epoch: 71-12 batch_loss=1.45e-02\n",
            "Train Epoch: 71-24 batch_loss=1.70e-02\n",
            "Train Epoch: 71-36 batch_loss=1.85e-02\n",
            "Test set: Average loss: 0.5316 Average relative error: 0.1357\n",
            "Train Epoch: 72-0 batch_loss=1.38e-02\n",
            "Train Epoch: 72-12 batch_loss=1.50e-02\n",
            "Train Epoch: 72-24 batch_loss=1.53e-02\n",
            "Train Epoch: 72-36 batch_loss=1.47e-02\n",
            "Test set: Average loss: 0.5136 Average relative error: 0.1330\n",
            "Train Epoch: 73-0 batch_loss=1.60e-02\n",
            "Train Epoch: 73-12 batch_loss=1.94e-02\n",
            "Train Epoch: 73-24 batch_loss=1.99e-02\n",
            "Train Epoch: 73-36 batch_loss=1.76e-02\n",
            "Test set: Average loss: 0.4948 Average relative error: 0.1302\n",
            "Train Epoch: 74-0 batch_loss=1.76e-02\n",
            "Train Epoch: 74-12 batch_loss=1.38e-02\n",
            "Train Epoch: 74-24 batch_loss=1.60e-02\n",
            "Train Epoch: 74-36 batch_loss=1.34e-02\n",
            "Test set: Average loss: 0.4805 Average relative error: 0.1286\n",
            "Train Epoch: 75-0 batch_loss=1.78e-02\n",
            "Train Epoch: 75-12 batch_loss=1.33e-02\n",
            "Train Epoch: 75-24 batch_loss=1.68e-02\n",
            "Train Epoch: 75-36 batch_loss=1.55e-02\n",
            "Test set: Average loss: 0.4650 Average relative error: 0.1264\n",
            "Train Epoch: 76-0 batch_loss=1.10e-02\n",
            "Train Epoch: 76-12 batch_loss=1.54e-02\n",
            "Train Epoch: 76-24 batch_loss=1.19e-02\n",
            "Train Epoch: 76-36 batch_loss=1.48e-02\n",
            "Test set: Average loss: 0.4521 Average relative error: 0.1242\n",
            "Train Epoch: 77-0 batch_loss=1.57e-02\n",
            "Train Epoch: 77-12 batch_loss=1.28e-02\n",
            "Train Epoch: 77-24 batch_loss=1.51e-02\n",
            "Train Epoch: 77-36 batch_loss=1.08e-02\n",
            "Test set: Average loss: 0.4387 Average relative error: 0.1224\n",
            "Train Epoch: 78-0 batch_loss=1.51e-02\n",
            "Train Epoch: 78-12 batch_loss=1.36e-02\n",
            "Train Epoch: 78-24 batch_loss=1.05e-02\n",
            "Train Epoch: 78-36 batch_loss=1.45e-02\n",
            "Test set: Average loss: 0.4272 Average relative error: 0.1210\n",
            "Train Epoch: 79-0 batch_loss=1.30e-02\n",
            "Train Epoch: 79-12 batch_loss=1.16e-02\n",
            "Train Epoch: 79-24 batch_loss=1.31e-02\n",
            "Train Epoch: 79-36 batch_loss=1.45e-02\n",
            "Test set: Average loss: 0.4134 Average relative error: 0.1192\n",
            "Train Epoch: 80-0 batch_loss=1.20e-02\n",
            "Train Epoch: 80-12 batch_loss=1.22e-02\n",
            "Train Epoch: 80-24 batch_loss=1.20e-02\n",
            "Train Epoch: 80-36 batch_loss=1.28e-02\n",
            "Test set: Average loss: 0.4022 Average relative error: 0.1169\n",
            "Train Epoch: 81-0 batch_loss=1.45e-02\n",
            "Train Epoch: 81-12 batch_loss=1.23e-02\n",
            "Train Epoch: 81-24 batch_loss=1.14e-02\n",
            "Train Epoch: 81-36 batch_loss=1.84e-02\n",
            "Test set: Average loss: 0.3920 Average relative error: 0.1152\n",
            "Train Epoch: 82-0 batch_loss=1.18e-02\n",
            "Train Epoch: 82-12 batch_loss=8.22e-03\n",
            "Train Epoch: 82-24 batch_loss=1.14e-02\n",
            "Train Epoch: 82-36 batch_loss=1.34e-02\n",
            "Test set: Average loss: 0.3828 Average relative error: 0.1134\n",
            "Train Epoch: 83-0 batch_loss=1.51e-02\n",
            "Train Epoch: 83-12 batch_loss=1.17e-02\n",
            "Train Epoch: 83-24 batch_loss=1.18e-02\n",
            "Train Epoch: 83-36 batch_loss=1.26e-02\n",
            "Test set: Average loss: 0.3713 Average relative error: 0.1118\n",
            "Train Epoch: 84-0 batch_loss=1.08e-02\n",
            "Train Epoch: 84-12 batch_loss=1.07e-02\n",
            "Train Epoch: 84-24 batch_loss=1.52e-02\n",
            "Train Epoch: 84-36 batch_loss=1.10e-02\n",
            "Test set: Average loss: 0.3639 Average relative error: 0.1109\n",
            "Train Epoch: 85-0 batch_loss=1.20e-02\n",
            "Train Epoch: 85-12 batch_loss=1.16e-02\n",
            "Train Epoch: 85-24 batch_loss=1.14e-02\n",
            "Train Epoch: 85-36 batch_loss=9.16e-03\n",
            "Test set: Average loss: 0.3556 Average relative error: 0.1091\n",
            "Train Epoch: 86-0 batch_loss=1.29e-02\n",
            "Train Epoch: 86-12 batch_loss=9.84e-03\n",
            "Train Epoch: 86-24 batch_loss=1.12e-02\n",
            "Train Epoch: 86-36 batch_loss=8.63e-03\n",
            "Test set: Average loss: 0.3470 Average relative error: 0.1073\n",
            "Train Epoch: 87-0 batch_loss=1.12e-02\n",
            "Train Epoch: 87-12 batch_loss=9.49e-03\n",
            "Train Epoch: 87-24 batch_loss=8.96e-03\n",
            "Train Epoch: 87-36 batch_loss=1.03e-02\n",
            "Test set: Average loss: 0.3403 Average relative error: 0.1061\n",
            "Train Epoch: 88-0 batch_loss=1.02e-02\n",
            "Train Epoch: 88-12 batch_loss=9.84e-03\n",
            "Train Epoch: 88-24 batch_loss=9.97e-03\n",
            "Train Epoch: 88-36 batch_loss=9.47e-03\n",
            "Test set: Average loss: 0.3349 Average relative error: 0.1053\n",
            "Train Epoch: 89-0 batch_loss=1.02e-02\n",
            "Train Epoch: 89-12 batch_loss=9.30e-03\n",
            "Train Epoch: 89-24 batch_loss=8.41e-03\n",
            "Train Epoch: 89-36 batch_loss=9.14e-03\n",
            "Test set: Average loss: 0.3276 Average relative error: 0.1042\n",
            "Train Epoch: 90-0 batch_loss=1.13e-02\n",
            "Train Epoch: 90-12 batch_loss=1.12e-02\n",
            "Train Epoch: 90-24 batch_loss=9.95e-03\n",
            "Train Epoch: 90-36 batch_loss=1.34e-02\n",
            "Test set: Average loss: 0.3201 Average relative error: 0.1029\n",
            "Train Epoch: 91-0 batch_loss=7.61e-03\n",
            "Train Epoch: 91-12 batch_loss=9.05e-03\n",
            "Train Epoch: 91-24 batch_loss=1.01e-02\n",
            "Train Epoch: 91-36 batch_loss=1.09e-02\n",
            "Test set: Average loss: 0.3139 Average relative error: 0.1016\n",
            "Train Epoch: 92-0 batch_loss=1.02e-02\n",
            "Train Epoch: 92-12 batch_loss=1.08e-02\n",
            "Train Epoch: 92-24 batch_loss=9.75e-03\n",
            "Train Epoch: 92-36 batch_loss=9.86e-03\n",
            "Test set: Average loss: 0.3084 Average relative error: 0.1009\n",
            "Train Epoch: 93-0 batch_loss=7.66e-03\n",
            "Train Epoch: 93-12 batch_loss=1.01e-02\n",
            "Train Epoch: 93-24 batch_loss=1.16e-02\n",
            "Train Epoch: 93-36 batch_loss=1.05e-02\n",
            "Test set: Average loss: 0.3019 Average relative error: 0.0997\n",
            "Train Epoch: 94-0 batch_loss=1.01e-02\n",
            "Train Epoch: 94-12 batch_loss=1.07e-02\n",
            "Train Epoch: 94-24 batch_loss=9.03e-03\n",
            "Train Epoch: 94-36 batch_loss=9.80e-03\n",
            "Test set: Average loss: 0.2956 Average relative error: 0.0985\n",
            "Train Epoch: 95-0 batch_loss=9.93e-03\n",
            "Train Epoch: 95-12 batch_loss=9.62e-03\n",
            "Train Epoch: 95-24 batch_loss=8.88e-03\n",
            "Train Epoch: 95-36 batch_loss=7.83e-03\n",
            "Test set: Average loss: 0.2916 Average relative error: 0.0977\n",
            "Train Epoch: 96-0 batch_loss=8.82e-03\n",
            "Train Epoch: 96-12 batch_loss=6.75e-03\n",
            "Train Epoch: 96-24 batch_loss=7.60e-03\n",
            "Train Epoch: 96-36 batch_loss=7.36e-03\n",
            "Test set: Average loss: 0.2846 Average relative error: 0.0966\n",
            "Train Epoch: 97-0 batch_loss=7.55e-03\n",
            "Train Epoch: 97-12 batch_loss=9.62e-03\n",
            "Train Epoch: 97-24 batch_loss=8.01e-03\n",
            "Train Epoch: 97-36 batch_loss=6.75e-03\n",
            "Test set: Average loss: 0.2803 Average relative error: 0.0955\n",
            "Train Epoch: 98-0 batch_loss=7.89e-03\n",
            "Train Epoch: 98-12 batch_loss=8.18e-03\n",
            "Train Epoch: 98-24 batch_loss=7.35e-03\n",
            "Train Epoch: 98-36 batch_loss=7.93e-03\n",
            "Test set: Average loss: 0.2731 Average relative error: 0.0944\n",
            "Train Epoch: 99-0 batch_loss=9.10e-03\n",
            "Train Epoch: 99-12 batch_loss=6.59e-03\n",
            "Train Epoch: 99-24 batch_loss=8.29e-03\n",
            "Train Epoch: 99-36 batch_loss=7.59e-03\n",
            "Test set: Average loss: 0.2694 Average relative error: 0.0937\n",
            "Train Epoch: 100-0 batch_loss=8.52e-03\n",
            "Train Epoch: 100-12 batch_loss=7.25e-03\n",
            "Train Epoch: 100-24 batch_loss=6.46e-03\n",
            "Train Epoch: 100-36 batch_loss=6.66e-03\n",
            "Test set: Average loss: 0.2634 Average relative error: 0.0926\n",
            "Train Epoch: 101-0 batch_loss=6.12e-03\n",
            "Train Epoch: 101-12 batch_loss=5.73e-03\n",
            "Train Epoch: 101-24 batch_loss=1.01e-02\n",
            "Train Epoch: 101-36 batch_loss=7.45e-03\n",
            "Test set: Average loss: 0.2588 Average relative error: 0.0916\n",
            "Train Epoch: 102-0 batch_loss=8.01e-03\n",
            "Train Epoch: 102-12 batch_loss=7.79e-03\n",
            "Train Epoch: 102-24 batch_loss=7.90e-03\n",
            "Train Epoch: 102-36 batch_loss=5.62e-03\n",
            "Test set: Average loss: 0.2531 Average relative error: 0.0907\n",
            "Train Epoch: 103-0 batch_loss=5.76e-03\n",
            "Train Epoch: 103-12 batch_loss=6.98e-03\n",
            "Train Epoch: 103-24 batch_loss=8.67e-03\n",
            "Train Epoch: 103-36 batch_loss=7.09e-03\n",
            "Test set: Average loss: 0.2487 Average relative error: 0.0898\n",
            "Train Epoch: 104-0 batch_loss=8.11e-03\n",
            "Train Epoch: 104-12 batch_loss=5.81e-03\n",
            "Train Epoch: 104-24 batch_loss=5.79e-03\n",
            "Train Epoch: 104-36 batch_loss=7.39e-03\n",
            "Test set: Average loss: 0.2420 Average relative error: 0.0886\n",
            "Train Epoch: 105-0 batch_loss=7.75e-03\n",
            "Train Epoch: 105-12 batch_loss=6.92e-03\n",
            "Train Epoch: 105-24 batch_loss=7.40e-03\n",
            "Train Epoch: 105-36 batch_loss=6.43e-03\n",
            "Test set: Average loss: 0.2367 Average relative error: 0.0875\n",
            "Train Epoch: 106-0 batch_loss=7.75e-03\n",
            "Train Epoch: 106-12 batch_loss=5.73e-03\n",
            "Train Epoch: 106-24 batch_loss=7.54e-03\n",
            "Train Epoch: 106-36 batch_loss=5.97e-03\n",
            "Test set: Average loss: 0.2318 Average relative error: 0.0865\n",
            "Train Epoch: 107-0 batch_loss=6.75e-03\n",
            "Train Epoch: 107-12 batch_loss=7.41e-03\n",
            "Train Epoch: 107-24 batch_loss=6.75e-03\n",
            "Train Epoch: 107-36 batch_loss=7.35e-03\n",
            "Test set: Average loss: 0.2276 Average relative error: 0.0860\n",
            "Train Epoch: 108-0 batch_loss=6.28e-03\n",
            "Train Epoch: 108-12 batch_loss=4.93e-03\n",
            "Train Epoch: 108-24 batch_loss=5.89e-03\n",
            "Train Epoch: 108-36 batch_loss=7.40e-03\n",
            "Test set: Average loss: 0.2228 Average relative error: 0.0851\n",
            "Train Epoch: 109-0 batch_loss=8.26e-03\n",
            "Train Epoch: 109-12 batch_loss=6.26e-03\n",
            "Train Epoch: 109-24 batch_loss=6.84e-03\n",
            "Train Epoch: 109-36 batch_loss=5.93e-03\n",
            "Test set: Average loss: 0.2182 Average relative error: 0.0842\n",
            "Train Epoch: 110-0 batch_loss=4.96e-03\n",
            "Train Epoch: 110-12 batch_loss=6.17e-03\n",
            "Train Epoch: 110-24 batch_loss=6.57e-03\n",
            "Train Epoch: 110-36 batch_loss=7.45e-03\n",
            "Test set: Average loss: 0.2134 Average relative error: 0.0831\n",
            "Train Epoch: 111-0 batch_loss=5.85e-03\n",
            "Train Epoch: 111-12 batch_loss=6.67e-03\n",
            "Train Epoch: 111-24 batch_loss=5.69e-03\n",
            "Train Epoch: 111-36 batch_loss=5.48e-03\n",
            "Test set: Average loss: 0.2090 Average relative error: 0.0824\n",
            "Train Epoch: 112-0 batch_loss=6.78e-03\n",
            "Train Epoch: 112-12 batch_loss=5.19e-03\n",
            "Train Epoch: 112-24 batch_loss=7.10e-03\n",
            "Train Epoch: 112-36 batch_loss=6.69e-03\n",
            "Test set: Average loss: 0.2052 Average relative error: 0.0815\n",
            "Train Epoch: 113-0 batch_loss=6.41e-03\n",
            "Train Epoch: 113-12 batch_loss=5.33e-03\n",
            "Train Epoch: 113-24 batch_loss=5.99e-03\n",
            "Train Epoch: 113-36 batch_loss=8.55e-03\n",
            "Test set: Average loss: 0.2010 Average relative error: 0.0810\n",
            "Train Epoch: 114-0 batch_loss=6.35e-03\n",
            "Train Epoch: 114-12 batch_loss=5.15e-03\n",
            "Train Epoch: 114-24 batch_loss=6.54e-03\n",
            "Train Epoch: 114-36 batch_loss=5.11e-03\n",
            "Test set: Average loss: 0.1957 Average relative error: 0.0797\n",
            "Train Epoch: 115-0 batch_loss=5.29e-03\n",
            "Train Epoch: 115-12 batch_loss=5.47e-03\n",
            "Train Epoch: 115-24 batch_loss=6.44e-03\n",
            "Train Epoch: 115-36 batch_loss=7.44e-03\n",
            "Test set: Average loss: 0.1918 Average relative error: 0.0790\n",
            "Train Epoch: 116-0 batch_loss=6.06e-03\n",
            "Train Epoch: 116-12 batch_loss=5.88e-03\n",
            "Train Epoch: 116-24 batch_loss=5.24e-03\n",
            "Train Epoch: 116-36 batch_loss=4.84e-03\n",
            "Test set: Average loss: 0.1884 Average relative error: 0.0781\n",
            "Train Epoch: 117-0 batch_loss=6.12e-03\n",
            "Train Epoch: 117-12 batch_loss=5.08e-03\n",
            "Train Epoch: 117-24 batch_loss=4.01e-03\n",
            "Train Epoch: 117-36 batch_loss=6.41e-03\n",
            "Test set: Average loss: 0.1836 Average relative error: 0.0772\n",
            "Train Epoch: 118-0 batch_loss=6.15e-03\n",
            "Train Epoch: 118-12 batch_loss=5.11e-03\n",
            "Train Epoch: 118-24 batch_loss=5.12e-03\n",
            "Train Epoch: 118-36 batch_loss=6.24e-03\n",
            "Test set: Average loss: 0.1799 Average relative error: 0.0764\n",
            "Train Epoch: 119-0 batch_loss=3.83e-03\n",
            "Train Epoch: 119-12 batch_loss=4.63e-03\n",
            "Train Epoch: 119-24 batch_loss=6.87e-03\n",
            "Train Epoch: 119-36 batch_loss=4.06e-03\n",
            "Test set: Average loss: 0.1772 Average relative error: 0.0764\n",
            "Train Epoch: 120-0 batch_loss=6.33e-03\n",
            "Train Epoch: 120-12 batch_loss=5.37e-03\n",
            "Train Epoch: 120-24 batch_loss=5.50e-03\n",
            "Train Epoch: 120-36 batch_loss=4.18e-03\n",
            "Test set: Average loss: 0.1732 Average relative error: 0.0753\n",
            "Train Epoch: 121-0 batch_loss=5.93e-03\n",
            "Train Epoch: 121-12 batch_loss=6.64e-03\n",
            "Train Epoch: 121-24 batch_loss=5.31e-03\n",
            "Train Epoch: 121-36 batch_loss=4.17e-03\n",
            "Test set: Average loss: 0.1699 Average relative error: 0.0747\n",
            "Train Epoch: 122-0 batch_loss=4.64e-03\n",
            "Train Epoch: 122-12 batch_loss=4.68e-03\n",
            "Train Epoch: 122-24 batch_loss=4.30e-03\n",
            "Train Epoch: 122-36 batch_loss=3.68e-03\n",
            "Test set: Average loss: 0.1671 Average relative error: 0.0740\n",
            "Train Epoch: 123-0 batch_loss=4.09e-03\n",
            "Train Epoch: 123-12 batch_loss=5.15e-03\n",
            "Train Epoch: 123-24 batch_loss=3.90e-03\n",
            "Train Epoch: 123-36 batch_loss=4.91e-03\n",
            "Test set: Average loss: 0.1631 Average relative error: 0.0731\n",
            "Train Epoch: 124-0 batch_loss=4.87e-03\n",
            "Train Epoch: 124-12 batch_loss=4.15e-03\n",
            "Train Epoch: 124-24 batch_loss=3.07e-03\n",
            "Train Epoch: 124-36 batch_loss=4.14e-03\n",
            "Test set: Average loss: 0.1600 Average relative error: 0.0726\n",
            "Train Epoch: 125-0 batch_loss=4.04e-03\n",
            "Train Epoch: 125-12 batch_loss=4.85e-03\n",
            "Train Epoch: 125-24 batch_loss=5.13e-03\n",
            "Train Epoch: 125-36 batch_loss=5.03e-03\n",
            "Test set: Average loss: 0.1573 Average relative error: 0.0718\n",
            "Train Epoch: 126-0 batch_loss=3.88e-03\n",
            "Train Epoch: 126-12 batch_loss=4.20e-03\n",
            "Train Epoch: 126-24 batch_loss=3.58e-03\n",
            "Train Epoch: 126-36 batch_loss=4.46e-03\n",
            "Test set: Average loss: 0.1538 Average relative error: 0.0714\n",
            "Train Epoch: 127-0 batch_loss=3.84e-03\n",
            "Train Epoch: 127-12 batch_loss=5.08e-03\n",
            "Train Epoch: 127-24 batch_loss=4.43e-03\n",
            "Train Epoch: 127-36 batch_loss=5.26e-03\n",
            "Test set: Average loss: 0.1501 Average relative error: 0.0702\n",
            "Train Epoch: 128-0 batch_loss=3.68e-03\n",
            "Train Epoch: 128-12 batch_loss=3.53e-03\n",
            "Train Epoch: 128-24 batch_loss=3.91e-03\n",
            "Train Epoch: 128-36 batch_loss=4.16e-03\n",
            "Test set: Average loss: 0.1481 Average relative error: 0.0696\n",
            "Train Epoch: 129-0 batch_loss=5.08e-03\n",
            "Train Epoch: 129-12 batch_loss=3.84e-03\n",
            "Train Epoch: 129-24 batch_loss=3.50e-03\n",
            "Train Epoch: 129-36 batch_loss=3.85e-03\n",
            "Test set: Average loss: 0.1439 Average relative error: 0.0687\n",
            "Train Epoch: 130-0 batch_loss=4.36e-03\n",
            "Train Epoch: 130-12 batch_loss=3.90e-03\n",
            "Train Epoch: 130-24 batch_loss=4.18e-03\n",
            "Train Epoch: 130-36 batch_loss=4.52e-03\n",
            "Test set: Average loss: 0.1419 Average relative error: 0.0683\n",
            "Train Epoch: 131-0 batch_loss=3.54e-03\n",
            "Train Epoch: 131-12 batch_loss=4.26e-03\n",
            "Train Epoch: 131-24 batch_loss=4.31e-03\n",
            "Train Epoch: 131-36 batch_loss=4.55e-03\n",
            "Test set: Average loss: 0.1403 Average relative error: 0.0684\n",
            "Train Epoch: 132-0 batch_loss=4.64e-03\n",
            "Train Epoch: 132-12 batch_loss=4.54e-03\n",
            "Train Epoch: 132-24 batch_loss=3.60e-03\n",
            "Train Epoch: 132-36 batch_loss=3.46e-03\n",
            "Test set: Average loss: 0.1366 Average relative error: 0.0671\n",
            "Train Epoch: 133-0 batch_loss=4.58e-03\n",
            "Train Epoch: 133-12 batch_loss=3.15e-03\n",
            "Train Epoch: 133-24 batch_loss=3.52e-03\n",
            "Train Epoch: 133-36 batch_loss=2.72e-03\n",
            "Test set: Average loss: 0.1343 Average relative error: 0.0666\n",
            "Train Epoch: 134-0 batch_loss=3.28e-03\n",
            "Train Epoch: 134-12 batch_loss=3.32e-03\n",
            "Train Epoch: 134-24 batch_loss=3.80e-03\n",
            "Train Epoch: 134-36 batch_loss=5.04e-03\n",
            "Test set: Average loss: 0.1305 Average relative error: 0.0656\n",
            "Train Epoch: 135-0 batch_loss=5.01e-03\n",
            "Train Epoch: 135-12 batch_loss=3.42e-03\n",
            "Train Epoch: 135-24 batch_loss=5.33e-03\n",
            "Train Epoch: 135-36 batch_loss=4.22e-03\n",
            "Test set: Average loss: 0.1289 Average relative error: 0.0651\n",
            "Train Epoch: 136-0 batch_loss=3.60e-03\n",
            "Train Epoch: 136-12 batch_loss=3.79e-03\n",
            "Train Epoch: 136-24 batch_loss=3.24e-03\n",
            "Train Epoch: 136-36 batch_loss=4.44e-03\n",
            "Test set: Average loss: 0.1263 Average relative error: 0.0645\n",
            "Train Epoch: 137-0 batch_loss=4.54e-03\n",
            "Train Epoch: 137-12 batch_loss=4.16e-03\n",
            "Train Epoch: 137-24 batch_loss=4.11e-03\n",
            "Train Epoch: 137-36 batch_loss=4.18e-03\n",
            "Test set: Average loss: 0.1246 Average relative error: 0.0641\n",
            "Train Epoch: 138-0 batch_loss=3.44e-03\n",
            "Train Epoch: 138-12 batch_loss=3.70e-03\n",
            "Train Epoch: 138-24 batch_loss=3.38e-03\n",
            "Train Epoch: 138-36 batch_loss=4.06e-03\n",
            "Test set: Average loss: 0.1217 Average relative error: 0.0634\n",
            "Train Epoch: 139-0 batch_loss=3.52e-03\n",
            "Train Epoch: 139-12 batch_loss=2.77e-03\n",
            "Train Epoch: 139-24 batch_loss=3.79e-03\n",
            "Train Epoch: 139-36 batch_loss=2.34e-03\n",
            "Test set: Average loss: 0.1197 Average relative error: 0.0627\n",
            "Train Epoch: 140-0 batch_loss=3.01e-03\n",
            "Train Epoch: 140-12 batch_loss=2.65e-03\n",
            "Train Epoch: 140-24 batch_loss=3.57e-03\n",
            "Train Epoch: 140-36 batch_loss=2.89e-03\n",
            "Test set: Average loss: 0.1191 Average relative error: 0.0629\n",
            "Train Epoch: 141-0 batch_loss=4.07e-03\n",
            "Train Epoch: 141-12 batch_loss=3.40e-03\n",
            "Train Epoch: 141-24 batch_loss=3.63e-03\n",
            "Train Epoch: 141-36 batch_loss=3.24e-03\n",
            "Test set: Average loss: 0.1171 Average relative error: 0.0621\n",
            "Train Epoch: 142-0 batch_loss=4.22e-03\n",
            "Train Epoch: 142-12 batch_loss=2.59e-03\n",
            "Train Epoch: 142-24 batch_loss=3.01e-03\n",
            "Train Epoch: 142-36 batch_loss=2.87e-03\n",
            "Test set: Average loss: 0.1145 Average relative error: 0.0612\n",
            "Train Epoch: 143-0 batch_loss=2.52e-03\n",
            "Train Epoch: 143-12 batch_loss=3.41e-03\n",
            "Train Epoch: 143-24 batch_loss=3.79e-03\n",
            "Train Epoch: 143-36 batch_loss=4.47e-03\n",
            "Test set: Average loss: 0.1135 Average relative error: 0.0613\n",
            "Train Epoch: 144-0 batch_loss=2.64e-03\n",
            "Train Epoch: 144-12 batch_loss=4.01e-03\n",
            "Train Epoch: 144-24 batch_loss=2.72e-03\n",
            "Train Epoch: 144-36 batch_loss=3.20e-03\n",
            "Test set: Average loss: 0.1119 Average relative error: 0.0608\n",
            "Train Epoch: 145-0 batch_loss=2.57e-03\n",
            "Train Epoch: 145-12 batch_loss=3.17e-03\n",
            "Train Epoch: 145-24 batch_loss=2.69e-03\n",
            "Train Epoch: 145-36 batch_loss=2.94e-03\n",
            "Test set: Average loss: 0.1098 Average relative error: 0.0602\n",
            "Train Epoch: 146-0 batch_loss=2.82e-03\n",
            "Train Epoch: 146-12 batch_loss=3.33e-03\n",
            "Train Epoch: 146-24 batch_loss=2.96e-03\n",
            "Train Epoch: 146-36 batch_loss=4.74e-03\n",
            "Test set: Average loss: 0.1084 Average relative error: 0.0599\n",
            "Train Epoch: 147-0 batch_loss=3.23e-03\n",
            "Train Epoch: 147-12 batch_loss=3.37e-03\n",
            "Train Epoch: 147-24 batch_loss=3.14e-03\n",
            "Train Epoch: 147-36 batch_loss=2.96e-03\n",
            "Test set: Average loss: 0.1070 Average relative error: 0.0594\n",
            "Train Epoch: 148-0 batch_loss=3.02e-03\n",
            "Train Epoch: 148-12 batch_loss=2.32e-03\n",
            "Train Epoch: 148-24 batch_loss=3.26e-03\n",
            "Train Epoch: 148-36 batch_loss=3.28e-03\n",
            "Test set: Average loss: 0.1060 Average relative error: 0.0591\n",
            "Train Epoch: 149-0 batch_loss=2.34e-03\n",
            "Train Epoch: 149-12 batch_loss=2.59e-03\n",
            "Train Epoch: 149-24 batch_loss=3.25e-03\n",
            "Train Epoch: 149-36 batch_loss=2.00e-03\n",
            "Test set: Average loss: 0.1044 Average relative error: 0.0587\n",
            "Train Epoch: 150-0 batch_loss=2.68e-03\n",
            "Train Epoch: 150-12 batch_loss=2.66e-03\n",
            "Train Epoch: 150-24 batch_loss=2.45e-03\n",
            "Train Epoch: 150-36 batch_loss=2.55e-03\n",
            "Test set: Average loss: 0.1030 Average relative error: 0.0583\n",
            "Train Epoch: 151-0 batch_loss=2.68e-03\n",
            "Train Epoch: 151-12 batch_loss=2.66e-03\n",
            "Train Epoch: 151-24 batch_loss=3.16e-03\n",
            "Train Epoch: 151-36 batch_loss=3.60e-03\n",
            "Test set: Average loss: 0.1018 Average relative error: 0.0581\n",
            "Train Epoch: 152-0 batch_loss=2.78e-03\n",
            "Train Epoch: 152-12 batch_loss=2.59e-03\n",
            "Train Epoch: 152-24 batch_loss=2.56e-03\n",
            "Train Epoch: 152-36 batch_loss=3.47e-03\n",
            "Test set: Average loss: 0.1003 Average relative error: 0.0575\n",
            "Train Epoch: 153-0 batch_loss=3.24e-03\n",
            "Train Epoch: 153-12 batch_loss=2.86e-03\n",
            "Train Epoch: 153-24 batch_loss=2.09e-03\n",
            "Train Epoch: 153-36 batch_loss=2.10e-03\n",
            "Test set: Average loss: 0.0998 Average relative error: 0.0574\n",
            "Train Epoch: 154-0 batch_loss=2.74e-03\n",
            "Train Epoch: 154-12 batch_loss=3.72e-03\n",
            "Train Epoch: 154-24 batch_loss=3.00e-03\n",
            "Train Epoch: 154-36 batch_loss=2.94e-03\n",
            "Test set: Average loss: 0.0986 Average relative error: 0.0569\n",
            "Train Epoch: 155-0 batch_loss=2.83e-03\n",
            "Train Epoch: 155-12 batch_loss=2.88e-03\n",
            "Train Epoch: 155-24 batch_loss=2.49e-03\n",
            "Train Epoch: 155-36 batch_loss=2.07e-03\n",
            "Test set: Average loss: 0.0983 Average relative error: 0.0571\n",
            "Train Epoch: 156-0 batch_loss=3.79e-03\n",
            "Train Epoch: 156-12 batch_loss=2.85e-03\n",
            "Train Epoch: 156-24 batch_loss=3.50e-03\n",
            "Train Epoch: 156-36 batch_loss=3.49e-03\n",
            "Test set: Average loss: 0.0965 Average relative error: 0.0563\n",
            "Train Epoch: 157-0 batch_loss=2.73e-03\n",
            "Train Epoch: 157-12 batch_loss=2.72e-03\n",
            "Train Epoch: 157-24 batch_loss=3.28e-03\n",
            "Train Epoch: 157-36 batch_loss=2.56e-03\n",
            "Test set: Average loss: 0.0963 Average relative error: 0.0564\n",
            "Train Epoch: 158-0 batch_loss=3.36e-03\n",
            "Train Epoch: 158-12 batch_loss=3.07e-03\n",
            "Train Epoch: 158-24 batch_loss=2.86e-03\n",
            "Train Epoch: 158-36 batch_loss=2.44e-03\n",
            "Test set: Average loss: 0.0944 Average relative error: 0.0556\n",
            "Train Epoch: 159-0 batch_loss=2.96e-03\n",
            "Train Epoch: 159-12 batch_loss=2.51e-03\n",
            "Train Epoch: 159-24 batch_loss=3.18e-03\n",
            "Train Epoch: 159-36 batch_loss=2.20e-03\n",
            "Test set: Average loss: 0.0938 Average relative error: 0.0556\n",
            "Train Epoch: 160-0 batch_loss=2.74e-03\n",
            "Train Epoch: 160-12 batch_loss=2.83e-03\n",
            "Train Epoch: 160-24 batch_loss=2.15e-03\n",
            "Train Epoch: 160-36 batch_loss=2.47e-03\n",
            "Test set: Average loss: 0.0925 Average relative error: 0.0551\n",
            "Train Epoch: 161-0 batch_loss=2.29e-03\n",
            "Train Epoch: 161-12 batch_loss=2.53e-03\n",
            "Train Epoch: 161-24 batch_loss=2.19e-03\n",
            "Train Epoch: 161-36 batch_loss=2.40e-03\n",
            "Test set: Average loss: 0.0916 Average relative error: 0.0550\n",
            "Train Epoch: 162-0 batch_loss=2.86e-03\n",
            "Train Epoch: 162-12 batch_loss=3.21e-03\n",
            "Train Epoch: 162-24 batch_loss=2.88e-03\n",
            "Train Epoch: 162-36 batch_loss=2.66e-03\n",
            "Test set: Average loss: 0.0918 Average relative error: 0.0549\n",
            "Train Epoch: 163-0 batch_loss=2.28e-03\n",
            "Train Epoch: 163-12 batch_loss=2.81e-03\n",
            "Train Epoch: 163-24 batch_loss=2.52e-03\n",
            "Train Epoch: 163-36 batch_loss=2.76e-03\n",
            "Test set: Average loss: 0.0906 Average relative error: 0.0547\n",
            "Train Epoch: 164-0 batch_loss=2.74e-03\n",
            "Train Epoch: 164-12 batch_loss=2.29e-03\n",
            "Train Epoch: 164-24 batch_loss=2.42e-03\n",
            "Train Epoch: 164-36 batch_loss=2.00e-03\n",
            "Test set: Average loss: 0.0902 Average relative error: 0.0546\n",
            "Train Epoch: 165-0 batch_loss=3.22e-03\n",
            "Train Epoch: 165-12 batch_loss=2.05e-03\n",
            "Train Epoch: 165-24 batch_loss=2.90e-03\n",
            "Train Epoch: 165-36 batch_loss=2.28e-03\n",
            "Test set: Average loss: 0.0890 Average relative error: 0.0543\n",
            "Train Epoch: 166-0 batch_loss=3.77e-03\n",
            "Train Epoch: 166-12 batch_loss=2.26e-03\n",
            "Train Epoch: 166-24 batch_loss=2.58e-03\n",
            "Train Epoch: 166-36 batch_loss=2.85e-03\n",
            "Test set: Average loss: 0.0882 Average relative error: 0.0539\n",
            "Train Epoch: 167-0 batch_loss=2.24e-03\n",
            "Train Epoch: 167-12 batch_loss=2.71e-03\n",
            "Train Epoch: 167-24 batch_loss=3.05e-03\n",
            "Train Epoch: 167-36 batch_loss=3.41e-03\n",
            "Test set: Average loss: 0.0874 Average relative error: 0.0536\n",
            "Train Epoch: 168-0 batch_loss=2.69e-03\n",
            "Train Epoch: 168-12 batch_loss=2.89e-03\n",
            "Train Epoch: 168-24 batch_loss=2.69e-03\n",
            "Train Epoch: 168-36 batch_loss=2.89e-03\n",
            "Test set: Average loss: 0.0868 Average relative error: 0.0533\n",
            "Train Epoch: 169-0 batch_loss=2.15e-03\n",
            "Train Epoch: 169-12 batch_loss=2.62e-03\n",
            "Train Epoch: 169-24 batch_loss=2.74e-03\n",
            "Train Epoch: 169-36 batch_loss=2.17e-03\n",
            "Test set: Average loss: 0.0863 Average relative error: 0.0534\n",
            "Train Epoch: 170-0 batch_loss=3.64e-03\n",
            "Train Epoch: 170-12 batch_loss=2.76e-03\n",
            "Train Epoch: 170-24 batch_loss=2.31e-03\n",
            "Train Epoch: 170-36 batch_loss=2.42e-03\n",
            "Test set: Average loss: 0.0860 Average relative error: 0.0531\n",
            "Train Epoch: 171-0 batch_loss=2.49e-03\n",
            "Train Epoch: 171-12 batch_loss=3.02e-03\n",
            "Train Epoch: 171-24 batch_loss=2.63e-03\n",
            "Train Epoch: 171-36 batch_loss=2.27e-03\n",
            "Test set: Average loss: 0.0848 Average relative error: 0.0529\n",
            "Train Epoch: 172-0 batch_loss=3.40e-03\n",
            "Train Epoch: 172-12 batch_loss=2.27e-03\n",
            "Train Epoch: 172-24 batch_loss=2.40e-03\n",
            "Train Epoch: 172-36 batch_loss=2.65e-03\n",
            "Test set: Average loss: 0.0843 Average relative error: 0.0526\n",
            "Train Epoch: 173-0 batch_loss=1.98e-03\n",
            "Train Epoch: 173-12 batch_loss=2.74e-03\n",
            "Train Epoch: 173-24 batch_loss=2.36e-03\n",
            "Train Epoch: 173-36 batch_loss=2.55e-03\n",
            "Test set: Average loss: 0.0843 Average relative error: 0.0527\n",
            "Train Epoch: 174-0 batch_loss=2.07e-03\n",
            "Train Epoch: 174-12 batch_loss=2.98e-03\n",
            "Train Epoch: 174-24 batch_loss=2.02e-03\n",
            "Train Epoch: 174-36 batch_loss=2.70e-03\n",
            "Test set: Average loss: 0.0833 Average relative error: 0.0525\n",
            "Train Epoch: 175-0 batch_loss=2.03e-03\n",
            "Train Epoch: 175-12 batch_loss=1.91e-03\n",
            "Train Epoch: 175-24 batch_loss=2.82e-03\n",
            "Train Epoch: 175-36 batch_loss=2.07e-03\n",
            "Test set: Average loss: 0.0827 Average relative error: 0.0522\n",
            "Train Epoch: 176-0 batch_loss=2.55e-03\n",
            "Train Epoch: 176-12 batch_loss=2.40e-03\n",
            "Train Epoch: 176-24 batch_loss=2.03e-03\n",
            "Train Epoch: 176-36 batch_loss=2.29e-03\n",
            "Test set: Average loss: 0.0824 Average relative error: 0.0520\n",
            "Train Epoch: 177-0 batch_loss=2.84e-03\n",
            "Train Epoch: 177-12 batch_loss=2.60e-03\n",
            "Train Epoch: 177-24 batch_loss=1.89e-03\n",
            "Train Epoch: 177-36 batch_loss=2.34e-03\n",
            "Test set: Average loss: 0.0819 Average relative error: 0.0521\n",
            "Train Epoch: 178-0 batch_loss=1.71e-03\n",
            "Train Epoch: 178-12 batch_loss=2.07e-03\n",
            "Train Epoch: 178-24 batch_loss=2.43e-03\n",
            "Train Epoch: 178-36 batch_loss=1.84e-03\n",
            "Test set: Average loss: 0.0814 Average relative error: 0.0517\n",
            "Train Epoch: 179-0 batch_loss=2.69e-03\n",
            "Train Epoch: 179-12 batch_loss=1.83e-03\n",
            "Train Epoch: 179-24 batch_loss=2.05e-03\n",
            "Train Epoch: 179-36 batch_loss=2.26e-03\n",
            "Test set: Average loss: 0.0808 Average relative error: 0.0515\n",
            "Train Epoch: 180-0 batch_loss=2.43e-03\n",
            "Train Epoch: 180-12 batch_loss=1.74e-03\n",
            "Train Epoch: 180-24 batch_loss=2.27e-03\n",
            "Train Epoch: 180-36 batch_loss=2.35e-03\n",
            "Test set: Average loss: 0.0802 Average relative error: 0.0516\n",
            "Train Epoch: 181-0 batch_loss=2.14e-03\n",
            "Train Epoch: 181-12 batch_loss=2.25e-03\n",
            "Train Epoch: 181-24 batch_loss=2.11e-03\n",
            "Train Epoch: 181-36 batch_loss=2.20e-03\n",
            "Test set: Average loss: 0.0800 Average relative error: 0.0514\n",
            "Train Epoch: 182-0 batch_loss=2.07e-03\n",
            "Train Epoch: 182-12 batch_loss=3.02e-03\n",
            "Train Epoch: 182-24 batch_loss=1.96e-03\n",
            "Train Epoch: 182-36 batch_loss=1.87e-03\n",
            "Test set: Average loss: 0.0794 Average relative error: 0.0510\n",
            "Train Epoch: 183-0 batch_loss=2.01e-03\n",
            "Train Epoch: 183-12 batch_loss=2.45e-03\n",
            "Train Epoch: 183-24 batch_loss=2.25e-03\n",
            "Train Epoch: 183-36 batch_loss=2.03e-03\n",
            "Test set: Average loss: 0.0791 Average relative error: 0.0510\n",
            "Train Epoch: 184-0 batch_loss=2.48e-03\n",
            "Train Epoch: 184-12 batch_loss=1.94e-03\n",
            "Train Epoch: 184-24 batch_loss=2.12e-03\n",
            "Train Epoch: 184-36 batch_loss=2.40e-03\n",
            "Test set: Average loss: 0.0786 Average relative error: 0.0508\n",
            "Train Epoch: 185-0 batch_loss=1.93e-03\n",
            "Train Epoch: 185-12 batch_loss=2.32e-03\n",
            "Train Epoch: 185-24 batch_loss=2.16e-03\n",
            "Train Epoch: 185-36 batch_loss=1.99e-03\n",
            "Test set: Average loss: 0.0782 Average relative error: 0.0507\n",
            "Train Epoch: 186-0 batch_loss=2.46e-03\n",
            "Train Epoch: 186-12 batch_loss=1.73e-03\n",
            "Train Epoch: 186-24 batch_loss=2.22e-03\n",
            "Train Epoch: 186-36 batch_loss=2.28e-03\n",
            "Test set: Average loss: 0.0777 Average relative error: 0.0504\n",
            "Train Epoch: 187-0 batch_loss=2.29e-03\n",
            "Train Epoch: 187-12 batch_loss=2.43e-03\n",
            "Train Epoch: 187-24 batch_loss=1.92e-03\n",
            "Train Epoch: 187-36 batch_loss=2.02e-03\n",
            "Test set: Average loss: 0.0776 Average relative error: 0.0505\n",
            "Train Epoch: 188-0 batch_loss=2.18e-03\n",
            "Train Epoch: 188-12 batch_loss=2.01e-03\n",
            "Train Epoch: 188-24 batch_loss=1.96e-03\n",
            "Train Epoch: 188-36 batch_loss=1.65e-03\n",
            "Test set: Average loss: 0.0767 Average relative error: 0.0503\n",
            "Train Epoch: 189-0 batch_loss=2.08e-03\n",
            "Train Epoch: 189-12 batch_loss=2.60e-03\n",
            "Train Epoch: 189-24 batch_loss=1.80e-03\n",
            "Train Epoch: 189-36 batch_loss=3.16e-03\n",
            "Test set: Average loss: 0.0763 Average relative error: 0.0501\n",
            "Train Epoch: 190-0 batch_loss=2.16e-03\n",
            "Train Epoch: 190-12 batch_loss=2.51e-03\n",
            "Train Epoch: 190-24 batch_loss=1.76e-03\n",
            "Train Epoch: 190-36 batch_loss=2.22e-03\n",
            "Test set: Average loss: 0.0759 Average relative error: 0.0500\n",
            "Train Epoch: 191-0 batch_loss=2.06e-03\n",
            "Train Epoch: 191-12 batch_loss=1.87e-03\n",
            "Train Epoch: 191-24 batch_loss=1.71e-03\n",
            "Train Epoch: 191-36 batch_loss=2.42e-03\n",
            "Test set: Average loss: 0.0761 Average relative error: 0.0501\n",
            "Train Epoch: 192-0 batch_loss=1.80e-03\n",
            "Train Epoch: 192-12 batch_loss=2.15e-03\n",
            "Train Epoch: 192-24 batch_loss=1.41e-03\n",
            "Train Epoch: 192-36 batch_loss=1.96e-03\n",
            "Test set: Average loss: 0.0754 Average relative error: 0.0498\n",
            "Train Epoch: 193-0 batch_loss=2.01e-03\n",
            "Train Epoch: 193-12 batch_loss=2.69e-03\n",
            "Train Epoch: 193-24 batch_loss=2.13e-03\n",
            "Train Epoch: 193-36 batch_loss=1.72e-03\n",
            "Test set: Average loss: 0.0750 Average relative error: 0.0496\n",
            "Train Epoch: 194-0 batch_loss=1.99e-03\n",
            "Train Epoch: 194-12 batch_loss=2.11e-03\n",
            "Train Epoch: 194-24 batch_loss=1.77e-03\n",
            "Train Epoch: 194-36 batch_loss=2.30e-03\n",
            "Test set: Average loss: 0.0749 Average relative error: 0.0496\n",
            "Train Epoch: 195-0 batch_loss=2.09e-03\n",
            "Train Epoch: 195-12 batch_loss=1.90e-03\n",
            "Train Epoch: 195-24 batch_loss=2.54e-03\n",
            "Train Epoch: 195-36 batch_loss=2.42e-03\n",
            "Test set: Average loss: 0.0746 Average relative error: 0.0496\n",
            "Train Epoch: 196-0 batch_loss=2.02e-03\n",
            "Train Epoch: 196-12 batch_loss=2.31e-03\n",
            "Train Epoch: 196-24 batch_loss=1.97e-03\n",
            "Train Epoch: 196-36 batch_loss=1.91e-03\n",
            "Test set: Average loss: 0.0745 Average relative error: 0.0494\n",
            "Train Epoch: 197-0 batch_loss=2.52e-03\n",
            "Train Epoch: 197-12 batch_loss=2.09e-03\n",
            "Train Epoch: 197-24 batch_loss=2.20e-03\n",
            "Train Epoch: 197-36 batch_loss=2.04e-03\n",
            "Test set: Average loss: 0.0738 Average relative error: 0.0494\n",
            "Train Epoch: 198-0 batch_loss=1.62e-03\n",
            "Train Epoch: 198-12 batch_loss=2.00e-03\n",
            "Train Epoch: 198-24 batch_loss=1.92e-03\n",
            "Train Epoch: 198-36 batch_loss=1.88e-03\n",
            "Test set: Average loss: 0.0733 Average relative error: 0.0490\n",
            "Train Epoch: 199-0 batch_loss=2.16e-03\n",
            "Train Epoch: 199-12 batch_loss=1.56e-03\n",
            "Train Epoch: 199-24 batch_loss=2.31e-03\n",
            "Train Epoch: 199-36 batch_loss=2.04e-03\n",
            "Test set: Average loss: 0.0730 Average relative error: 0.0490\n",
            "Train Epoch: 200-0 batch_loss=2.00e-03\n",
            "Train Epoch: 200-12 batch_loss=2.29e-03\n",
            "Train Epoch: 200-24 batch_loss=2.55e-03\n",
            "Train Epoch: 200-36 batch_loss=2.06e-03\n",
            "Test set: Average loss: 0.0729 Average relative error: 0.0489\n",
            "Train Epoch: 201-0 batch_loss=1.69e-03\n",
            "Train Epoch: 201-12 batch_loss=1.82e-03\n",
            "Train Epoch: 201-24 batch_loss=2.80e-03\n",
            "Train Epoch: 201-36 batch_loss=1.92e-03\n",
            "Test set: Average loss: 0.0722 Average relative error: 0.0487\n",
            "Train Epoch: 202-0 batch_loss=1.92e-03\n",
            "Train Epoch: 202-12 batch_loss=2.26e-03\n",
            "Train Epoch: 202-24 batch_loss=1.65e-03\n",
            "Train Epoch: 202-36 batch_loss=1.48e-03\n",
            "Test set: Average loss: 0.0721 Average relative error: 0.0486\n",
            "Train Epoch: 203-0 batch_loss=1.81e-03\n",
            "Train Epoch: 203-12 batch_loss=1.66e-03\n",
            "Train Epoch: 203-24 batch_loss=1.80e-03\n",
            "Train Epoch: 203-36 batch_loss=1.75e-03\n",
            "Test set: Average loss: 0.0720 Average relative error: 0.0486\n",
            "Train Epoch: 204-0 batch_loss=1.65e-03\n",
            "Train Epoch: 204-12 batch_loss=2.30e-03\n",
            "Train Epoch: 204-24 batch_loss=2.50e-03\n",
            "Train Epoch: 204-36 batch_loss=2.33e-03\n",
            "Test set: Average loss: 0.0722 Average relative error: 0.0488\n",
            "Train Epoch: 205-0 batch_loss=1.81e-03\n",
            "Train Epoch: 205-12 batch_loss=1.73e-03\n",
            "Train Epoch: 205-24 batch_loss=1.85e-03\n",
            "Train Epoch: 205-36 batch_loss=2.42e-03\n",
            "Test set: Average loss: 0.0715 Average relative error: 0.0484\n",
            "Train Epoch: 206-0 batch_loss=1.95e-03\n",
            "Train Epoch: 206-12 batch_loss=2.12e-03\n",
            "Train Epoch: 206-24 batch_loss=2.08e-03\n",
            "Train Epoch: 206-36 batch_loss=2.18e-03\n",
            "Test set: Average loss: 0.0711 Average relative error: 0.0484\n",
            "Train Epoch: 207-0 batch_loss=2.18e-03\n",
            "Train Epoch: 207-12 batch_loss=1.73e-03\n",
            "Train Epoch: 207-24 batch_loss=2.60e-03\n",
            "Train Epoch: 207-36 batch_loss=2.13e-03\n",
            "Test set: Average loss: 0.0711 Average relative error: 0.0482\n",
            "Train Epoch: 208-0 batch_loss=2.46e-03\n",
            "Train Epoch: 208-12 batch_loss=2.06e-03\n",
            "Train Epoch: 208-24 batch_loss=1.90e-03\n",
            "Train Epoch: 208-36 batch_loss=2.11e-03\n",
            "Test set: Average loss: 0.0707 Average relative error: 0.0481\n",
            "Train Epoch: 209-0 batch_loss=1.73e-03\n",
            "Train Epoch: 209-12 batch_loss=1.90e-03\n",
            "Train Epoch: 209-24 batch_loss=1.94e-03\n",
            "Train Epoch: 209-36 batch_loss=1.70e-03\n",
            "Test set: Average loss: 0.0703 Average relative error: 0.0480\n",
            "Train Epoch: 210-0 batch_loss=1.66e-03\n",
            "Train Epoch: 210-12 batch_loss=2.26e-03\n",
            "Train Epoch: 210-24 batch_loss=1.90e-03\n",
            "Train Epoch: 210-36 batch_loss=1.99e-03\n",
            "Test set: Average loss: 0.0702 Average relative error: 0.0481\n",
            "Train Epoch: 211-0 batch_loss=2.17e-03\n",
            "Train Epoch: 211-12 batch_loss=1.83e-03\n",
            "Train Epoch: 211-24 batch_loss=2.21e-03\n",
            "Train Epoch: 211-36 batch_loss=2.08e-03\n",
            "Test set: Average loss: 0.0699 Average relative error: 0.0478\n",
            "Train Epoch: 212-0 batch_loss=2.39e-03\n",
            "Train Epoch: 212-12 batch_loss=2.84e-03\n",
            "Train Epoch: 212-24 batch_loss=2.03e-03\n",
            "Train Epoch: 212-36 batch_loss=1.58e-03\n",
            "Test set: Average loss: 0.0697 Average relative error: 0.0478\n",
            "Train Epoch: 213-0 batch_loss=2.28e-03\n",
            "Train Epoch: 213-12 batch_loss=1.57e-03\n",
            "Train Epoch: 213-24 batch_loss=1.80e-03\n",
            "Train Epoch: 213-36 batch_loss=2.27e-03\n",
            "Test set: Average loss: 0.0691 Average relative error: 0.0476\n",
            "Train Epoch: 214-0 batch_loss=1.50e-03\n",
            "Train Epoch: 214-12 batch_loss=2.65e-03\n",
            "Train Epoch: 214-24 batch_loss=1.88e-03\n",
            "Train Epoch: 214-36 batch_loss=1.90e-03\n",
            "Test set: Average loss: 0.0694 Average relative error: 0.0476\n",
            "Train Epoch: 215-0 batch_loss=2.09e-03\n",
            "Train Epoch: 215-12 batch_loss=2.03e-03\n",
            "Train Epoch: 215-24 batch_loss=2.07e-03\n",
            "Train Epoch: 215-36 batch_loss=1.96e-03\n",
            "Test set: Average loss: 0.0689 Average relative error: 0.0473\n",
            "Train Epoch: 216-0 batch_loss=1.77e-03\n",
            "Train Epoch: 216-12 batch_loss=1.93e-03\n",
            "Train Epoch: 216-24 batch_loss=1.64e-03\n",
            "Train Epoch: 216-36 batch_loss=2.17e-03\n",
            "Test set: Average loss: 0.0686 Average relative error: 0.0474\n",
            "Train Epoch: 217-0 batch_loss=1.76e-03\n",
            "Train Epoch: 217-12 batch_loss=1.59e-03\n",
            "Train Epoch: 217-24 batch_loss=1.88e-03\n",
            "Train Epoch: 217-36 batch_loss=1.54e-03\n",
            "Test set: Average loss: 0.0685 Average relative error: 0.0474\n",
            "Train Epoch: 218-0 batch_loss=1.91e-03\n",
            "Train Epoch: 218-12 batch_loss=2.19e-03\n",
            "Train Epoch: 218-24 batch_loss=2.43e-03\n",
            "Train Epoch: 218-36 batch_loss=2.04e-03\n",
            "Test set: Average loss: 0.0684 Average relative error: 0.0472\n",
            "Train Epoch: 219-0 batch_loss=1.96e-03\n",
            "Train Epoch: 219-12 batch_loss=1.42e-03\n",
            "Train Epoch: 219-24 batch_loss=2.18e-03\n",
            "Train Epoch: 219-36 batch_loss=2.17e-03\n",
            "Test set: Average loss: 0.0679 Average relative error: 0.0471\n",
            "Train Epoch: 220-0 batch_loss=1.81e-03\n",
            "Train Epoch: 220-12 batch_loss=1.77e-03\n",
            "Train Epoch: 220-24 batch_loss=1.98e-03\n",
            "Train Epoch: 220-36 batch_loss=2.17e-03\n",
            "Test set: Average loss: 0.0679 Average relative error: 0.0471\n",
            "Train Epoch: 221-0 batch_loss=1.75e-03\n",
            "Train Epoch: 221-12 batch_loss=1.83e-03\n",
            "Train Epoch: 221-24 batch_loss=1.93e-03\n",
            "Train Epoch: 221-36 batch_loss=1.72e-03\n",
            "Test set: Average loss: 0.0681 Average relative error: 0.0472\n",
            "Train Epoch: 222-0 batch_loss=1.75e-03\n",
            "Train Epoch: 222-12 batch_loss=2.17e-03\n",
            "Train Epoch: 222-24 batch_loss=1.86e-03\n",
            "Train Epoch: 222-36 batch_loss=2.04e-03\n",
            "Test set: Average loss: 0.0678 Average relative error: 0.0471\n",
            "Train Epoch: 223-0 batch_loss=2.03e-03\n",
            "Train Epoch: 223-12 batch_loss=2.23e-03\n",
            "Train Epoch: 223-24 batch_loss=2.16e-03\n",
            "Train Epoch: 223-36 batch_loss=1.84e-03\n",
            "Test set: Average loss: 0.0672 Average relative error: 0.0468\n",
            "Train Epoch: 224-0 batch_loss=2.48e-03\n",
            "Train Epoch: 224-12 batch_loss=1.78e-03\n",
            "Train Epoch: 224-24 batch_loss=2.26e-03\n",
            "Train Epoch: 224-36 batch_loss=1.78e-03\n",
            "Test set: Average loss: 0.0671 Average relative error: 0.0470\n",
            "Train Epoch: 225-0 batch_loss=1.65e-03\n",
            "Train Epoch: 225-12 batch_loss=1.70e-03\n",
            "Train Epoch: 225-24 batch_loss=1.82e-03\n",
            "Train Epoch: 225-36 batch_loss=2.06e-03\n",
            "Test set: Average loss: 0.0667 Average relative error: 0.0467\n",
            "Train Epoch: 226-0 batch_loss=1.87e-03\n",
            "Train Epoch: 226-12 batch_loss=2.25e-03\n",
            "Train Epoch: 226-24 batch_loss=2.29e-03\n",
            "Train Epoch: 226-36 batch_loss=2.30e-03\n",
            "Test set: Average loss: 0.0668 Average relative error: 0.0468\n",
            "Train Epoch: 227-0 batch_loss=1.56e-03\n",
            "Train Epoch: 227-12 batch_loss=1.70e-03\n",
            "Train Epoch: 227-24 batch_loss=2.28e-03\n",
            "Train Epoch: 227-36 batch_loss=1.66e-03\n",
            "Test set: Average loss: 0.0666 Average relative error: 0.0467\n",
            "Train Epoch: 228-0 batch_loss=1.79e-03\n",
            "Train Epoch: 228-12 batch_loss=2.17e-03\n",
            "Train Epoch: 228-24 batch_loss=1.57e-03\n",
            "Train Epoch: 228-36 batch_loss=1.84e-03\n",
            "Test set: Average loss: 0.0661 Average relative error: 0.0466\n",
            "Train Epoch: 229-0 batch_loss=1.76e-03\n",
            "Train Epoch: 229-12 batch_loss=2.05e-03\n",
            "Train Epoch: 229-24 batch_loss=1.90e-03\n",
            "Train Epoch: 229-36 batch_loss=1.60e-03\n",
            "Test set: Average loss: 0.0660 Average relative error: 0.0464\n",
            "Train Epoch: 230-0 batch_loss=1.64e-03\n",
            "Train Epoch: 230-12 batch_loss=1.81e-03\n",
            "Train Epoch: 230-24 batch_loss=1.76e-03\n",
            "Train Epoch: 230-36 batch_loss=1.46e-03\n",
            "Test set: Average loss: 0.0660 Average relative error: 0.0465\n",
            "Train Epoch: 231-0 batch_loss=1.95e-03\n",
            "Train Epoch: 231-12 batch_loss=1.43e-03\n",
            "Train Epoch: 231-24 batch_loss=1.51e-03\n",
            "Train Epoch: 231-36 batch_loss=2.05e-03\n",
            "Test set: Average loss: 0.0657 Average relative error: 0.0464\n",
            "Train Epoch: 232-0 batch_loss=1.70e-03\n",
            "Train Epoch: 232-12 batch_loss=1.64e-03\n",
            "Train Epoch: 232-24 batch_loss=1.64e-03\n",
            "Train Epoch: 232-36 batch_loss=1.71e-03\n",
            "Test set: Average loss: 0.0654 Average relative error: 0.0463\n",
            "Train Epoch: 233-0 batch_loss=2.38e-03\n",
            "Train Epoch: 233-12 batch_loss=1.81e-03\n",
            "Train Epoch: 233-24 batch_loss=2.04e-03\n",
            "Train Epoch: 233-36 batch_loss=1.48e-03\n",
            "Test set: Average loss: 0.0654 Average relative error: 0.0463\n",
            "Train Epoch: 234-0 batch_loss=1.56e-03\n",
            "Train Epoch: 234-12 batch_loss=2.16e-03\n",
            "Train Epoch: 234-24 batch_loss=1.69e-03\n",
            "Train Epoch: 234-36 batch_loss=2.36e-03\n",
            "Test set: Average loss: 0.0652 Average relative error: 0.0462\n",
            "Train Epoch: 235-0 batch_loss=2.08e-03\n",
            "Train Epoch: 235-12 batch_loss=2.31e-03\n",
            "Train Epoch: 235-24 batch_loss=1.82e-03\n",
            "Train Epoch: 235-36 batch_loss=2.40e-03\n",
            "Test set: Average loss: 0.0650 Average relative error: 0.0461\n",
            "Train Epoch: 236-0 batch_loss=1.39e-03\n",
            "Train Epoch: 236-12 batch_loss=1.89e-03\n",
            "Train Epoch: 236-24 batch_loss=2.20e-03\n",
            "Train Epoch: 236-36 batch_loss=2.22e-03\n",
            "Test set: Average loss: 0.0650 Average relative error: 0.0461\n",
            "Train Epoch: 237-0 batch_loss=1.56e-03\n",
            "Train Epoch: 237-12 batch_loss=2.03e-03\n",
            "Train Epoch: 237-24 batch_loss=1.71e-03\n",
            "Train Epoch: 237-36 batch_loss=1.55e-03\n",
            "Test set: Average loss: 0.0648 Average relative error: 0.0460\n",
            "Train Epoch: 238-0 batch_loss=1.65e-03\n",
            "Train Epoch: 238-12 batch_loss=2.02e-03\n",
            "Train Epoch: 238-24 batch_loss=1.69e-03\n",
            "Train Epoch: 238-36 batch_loss=2.04e-03\n",
            "Test set: Average loss: 0.0646 Average relative error: 0.0459\n",
            "Train Epoch: 239-0 batch_loss=2.04e-03\n",
            "Train Epoch: 239-12 batch_loss=2.08e-03\n",
            "Train Epoch: 239-24 batch_loss=1.67e-03\n",
            "Train Epoch: 239-36 batch_loss=2.03e-03\n",
            "Test set: Average loss: 0.0645 Average relative error: 0.0459\n",
            "Train Epoch: 240-0 batch_loss=1.57e-03\n",
            "Train Epoch: 240-12 batch_loss=2.06e-03\n",
            "Train Epoch: 240-24 batch_loss=1.72e-03\n",
            "Train Epoch: 240-36 batch_loss=1.78e-03\n",
            "Test set: Average loss: 0.0641 Average relative error: 0.0458\n",
            "Train Epoch: 241-0 batch_loss=1.41e-03\n",
            "Train Epoch: 241-12 batch_loss=1.83e-03\n",
            "Train Epoch: 241-24 batch_loss=1.66e-03\n",
            "Train Epoch: 241-36 batch_loss=2.00e-03\n",
            "Test set: Average loss: 0.0645 Average relative error: 0.0459\n",
            "Train Epoch: 242-0 batch_loss=1.77e-03\n",
            "Train Epoch: 242-12 batch_loss=1.42e-03\n",
            "Train Epoch: 242-24 batch_loss=1.50e-03\n",
            "Train Epoch: 242-36 batch_loss=2.27e-03\n",
            "Test set: Average loss: 0.0642 Average relative error: 0.0460\n",
            "Train Epoch: 243-0 batch_loss=1.70e-03\n",
            "Train Epoch: 243-12 batch_loss=1.45e-03\n",
            "Train Epoch: 243-24 batch_loss=2.00e-03\n",
            "Train Epoch: 243-36 batch_loss=2.00e-03\n",
            "Test set: Average loss: 0.0639 Average relative error: 0.0457\n",
            "Train Epoch: 244-0 batch_loss=1.43e-03\n",
            "Train Epoch: 244-12 batch_loss=1.96e-03\n",
            "Train Epoch: 244-24 batch_loss=2.15e-03\n",
            "Train Epoch: 244-36 batch_loss=1.53e-03\n",
            "Test set: Average loss: 0.0638 Average relative error: 0.0456\n",
            "Train Epoch: 245-0 batch_loss=2.05e-03\n",
            "Train Epoch: 245-12 batch_loss=1.51e-03\n",
            "Train Epoch: 245-24 batch_loss=1.54e-03\n",
            "Train Epoch: 245-36 batch_loss=1.55e-03\n",
            "Test set: Average loss: 0.0634 Average relative error: 0.0455\n",
            "Train Epoch: 246-0 batch_loss=1.74e-03\n",
            "Train Epoch: 246-12 batch_loss=1.51e-03\n",
            "Train Epoch: 246-24 batch_loss=1.79e-03\n",
            "Train Epoch: 246-36 batch_loss=2.22e-03\n",
            "Test set: Average loss: 0.0633 Average relative error: 0.0455\n",
            "Train Epoch: 247-0 batch_loss=1.56e-03\n",
            "Train Epoch: 247-12 batch_loss=1.95e-03\n",
            "Train Epoch: 247-24 batch_loss=1.81e-03\n",
            "Train Epoch: 247-36 batch_loss=2.09e-03\n",
            "Test set: Average loss: 0.0633 Average relative error: 0.0456\n",
            "Train Epoch: 248-0 batch_loss=1.92e-03\n",
            "Train Epoch: 248-12 batch_loss=1.95e-03\n",
            "Train Epoch: 248-24 batch_loss=1.35e-03\n",
            "Train Epoch: 248-36 batch_loss=1.81e-03\n",
            "Test set: Average loss: 0.0631 Average relative error: 0.0458\n",
            "Train Epoch: 249-0 batch_loss=1.57e-03\n",
            "Train Epoch: 249-12 batch_loss=1.61e-03\n",
            "Train Epoch: 249-24 batch_loss=1.83e-03\n",
            "Train Epoch: 249-36 batch_loss=2.33e-03\n",
            "Test set: Average loss: 0.0628 Average relative error: 0.0454\n",
            "Train Epoch: 250-0 batch_loss=2.02e-03\n",
            "Train Epoch: 250-12 batch_loss=1.72e-03\n",
            "Train Epoch: 250-24 batch_loss=1.81e-03\n",
            "Train Epoch: 250-36 batch_loss=1.61e-03\n",
            "Test set: Average loss: 0.0629 Average relative error: 0.0454\n",
            "Train Epoch: 251-0 batch_loss=1.83e-03\n",
            "Train Epoch: 251-12 batch_loss=2.56e-03\n",
            "Train Epoch: 251-24 batch_loss=1.40e-03\n",
            "Train Epoch: 251-36 batch_loss=1.38e-03\n",
            "Test set: Average loss: 0.0628 Average relative error: 0.0453\n",
            "Train Epoch: 252-0 batch_loss=1.86e-03\n",
            "Train Epoch: 252-12 batch_loss=1.87e-03\n",
            "Train Epoch: 252-24 batch_loss=1.74e-03\n",
            "Train Epoch: 252-36 batch_loss=2.05e-03\n",
            "Test set: Average loss: 0.0626 Average relative error: 0.0453\n",
            "Train Epoch: 253-0 batch_loss=1.37e-03\n",
            "Train Epoch: 253-12 batch_loss=2.32e-03\n",
            "Train Epoch: 253-24 batch_loss=1.44e-03\n",
            "Train Epoch: 253-36 batch_loss=1.55e-03\n",
            "Test set: Average loss: 0.0624 Average relative error: 0.0452\n",
            "Train Epoch: 254-0 batch_loss=1.83e-03\n",
            "Train Epoch: 254-12 batch_loss=1.86e-03\n",
            "Train Epoch: 254-24 batch_loss=1.51e-03\n",
            "Train Epoch: 254-36 batch_loss=2.06e-03\n",
            "Test set: Average loss: 0.0622 Average relative error: 0.0450\n",
            "Train Epoch: 255-0 batch_loss=1.79e-03\n",
            "Train Epoch: 255-12 batch_loss=1.83e-03\n",
            "Train Epoch: 255-24 batch_loss=1.68e-03\n",
            "Train Epoch: 255-36 batch_loss=1.51e-03\n",
            "Test set: Average loss: 0.0622 Average relative error: 0.0450\n",
            "Train Epoch: 256-0 batch_loss=1.34e-03\n",
            "Train Epoch: 256-12 batch_loss=1.67e-03\n",
            "Train Epoch: 256-24 batch_loss=1.82e-03\n",
            "Train Epoch: 256-36 batch_loss=1.69e-03\n",
            "Test set: Average loss: 0.0620 Average relative error: 0.0450\n",
            "Train Epoch: 257-0 batch_loss=1.92e-03\n",
            "Train Epoch: 257-12 batch_loss=1.32e-03\n",
            "Train Epoch: 257-24 batch_loss=1.91e-03\n",
            "Train Epoch: 257-36 batch_loss=1.71e-03\n",
            "Test set: Average loss: 0.0619 Average relative error: 0.0449\n",
            "Train Epoch: 258-0 batch_loss=1.87e-03\n",
            "Train Epoch: 258-12 batch_loss=1.82e-03\n",
            "Train Epoch: 258-24 batch_loss=1.58e-03\n",
            "Train Epoch: 258-36 batch_loss=1.47e-03\n",
            "Test set: Average loss: 0.0619 Average relative error: 0.0449\n",
            "Train Epoch: 259-0 batch_loss=1.39e-03\n",
            "Train Epoch: 259-12 batch_loss=2.08e-03\n",
            "Train Epoch: 259-24 batch_loss=2.18e-03\n",
            "Train Epoch: 259-36 batch_loss=2.02e-03\n",
            "Test set: Average loss: 0.0616 Average relative error: 0.0449\n",
            "Train Epoch: 260-0 batch_loss=1.73e-03\n",
            "Train Epoch: 260-12 batch_loss=1.47e-03\n",
            "Train Epoch: 260-24 batch_loss=1.38e-03\n",
            "Train Epoch: 260-36 batch_loss=2.08e-03\n",
            "Test set: Average loss: 0.0616 Average relative error: 0.0449\n",
            "Train Epoch: 261-0 batch_loss=1.81e-03\n",
            "Train Epoch: 261-12 batch_loss=1.89e-03\n",
            "Train Epoch: 261-24 batch_loss=1.95e-03\n",
            "Train Epoch: 261-36 batch_loss=1.72e-03\n",
            "Test set: Average loss: 0.0617 Average relative error: 0.0448\n",
            "Train Epoch: 262-0 batch_loss=1.95e-03\n",
            "Train Epoch: 262-12 batch_loss=1.37e-03\n",
            "Train Epoch: 262-24 batch_loss=1.91e-03\n",
            "Train Epoch: 262-36 batch_loss=1.95e-03\n",
            "Test set: Average loss: 0.0616 Average relative error: 0.0448\n",
            "Train Epoch: 263-0 batch_loss=1.89e-03\n",
            "Train Epoch: 263-12 batch_loss=1.76e-03\n",
            "Train Epoch: 263-24 batch_loss=1.62e-03\n",
            "Train Epoch: 263-36 batch_loss=1.60e-03\n",
            "Test set: Average loss: 0.0613 Average relative error: 0.0447\n",
            "Train Epoch: 264-0 batch_loss=1.29e-03\n",
            "Train Epoch: 264-12 batch_loss=1.71e-03\n",
            "Train Epoch: 264-24 batch_loss=1.88e-03\n",
            "Train Epoch: 264-36 batch_loss=1.86e-03\n",
            "Test set: Average loss: 0.0612 Average relative error: 0.0447\n",
            "Train Epoch: 265-0 batch_loss=1.43e-03\n",
            "Train Epoch: 265-12 batch_loss=1.65e-03\n",
            "Train Epoch: 265-24 batch_loss=1.44e-03\n",
            "Train Epoch: 265-36 batch_loss=1.81e-03\n",
            "Test set: Average loss: 0.0611 Average relative error: 0.0447\n",
            "Train Epoch: 266-0 batch_loss=1.48e-03\n",
            "Train Epoch: 266-12 batch_loss=1.48e-03\n",
            "Train Epoch: 266-24 batch_loss=1.53e-03\n",
            "Train Epoch: 266-36 batch_loss=1.52e-03\n",
            "Test set: Average loss: 0.0612 Average relative error: 0.0448\n",
            "Train Epoch: 267-0 batch_loss=1.56e-03\n",
            "Train Epoch: 267-12 batch_loss=1.50e-03\n",
            "Train Epoch: 267-24 batch_loss=2.05e-03\n",
            "Train Epoch: 267-36 batch_loss=1.79e-03\n",
            "Test set: Average loss: 0.0609 Average relative error: 0.0446\n",
            "Train Epoch: 268-0 batch_loss=1.57e-03\n",
            "Train Epoch: 268-12 batch_loss=1.76e-03\n",
            "Train Epoch: 268-24 batch_loss=1.69e-03\n",
            "Train Epoch: 268-36 batch_loss=1.51e-03\n",
            "Test set: Average loss: 0.0606 Average relative error: 0.0445\n",
            "Train Epoch: 269-0 batch_loss=1.38e-03\n",
            "Train Epoch: 269-12 batch_loss=1.64e-03\n",
            "Train Epoch: 269-24 batch_loss=1.63e-03\n",
            "Train Epoch: 269-36 batch_loss=1.67e-03\n",
            "Test set: Average loss: 0.0606 Average relative error: 0.0446\n",
            "Train Epoch: 270-0 batch_loss=1.51e-03\n",
            "Train Epoch: 270-12 batch_loss=1.72e-03\n",
            "Train Epoch: 270-24 batch_loss=1.95e-03\n",
            "Train Epoch: 270-36 batch_loss=1.59e-03\n",
            "Test set: Average loss: 0.0606 Average relative error: 0.0446\n",
            "Train Epoch: 271-0 batch_loss=1.57e-03\n",
            "Train Epoch: 271-12 batch_loss=1.78e-03\n",
            "Train Epoch: 271-24 batch_loss=1.55e-03\n",
            "Train Epoch: 271-36 batch_loss=1.42e-03\n",
            "Test set: Average loss: 0.0602 Average relative error: 0.0443\n",
            "Train Epoch: 272-0 batch_loss=1.92e-03\n",
            "Train Epoch: 272-12 batch_loss=1.71e-03\n",
            "Train Epoch: 272-24 batch_loss=1.49e-03\n",
            "Train Epoch: 272-36 batch_loss=2.00e-03\n",
            "Test set: Average loss: 0.0602 Average relative error: 0.0444\n",
            "Train Epoch: 273-0 batch_loss=1.69e-03\n",
            "Train Epoch: 273-12 batch_loss=1.80e-03\n",
            "Train Epoch: 273-24 batch_loss=1.51e-03\n",
            "Train Epoch: 273-36 batch_loss=1.80e-03\n",
            "Test set: Average loss: 0.0602 Average relative error: 0.0443\n",
            "Train Epoch: 274-0 batch_loss=1.64e-03\n",
            "Train Epoch: 274-12 batch_loss=1.50e-03\n",
            "Train Epoch: 274-24 batch_loss=1.66e-03\n",
            "Train Epoch: 274-36 batch_loss=1.49e-03\n",
            "Test set: Average loss: 0.0600 Average relative error: 0.0443\n",
            "Train Epoch: 275-0 batch_loss=1.48e-03\n",
            "Train Epoch: 275-12 batch_loss=2.00e-03\n",
            "Train Epoch: 275-24 batch_loss=1.70e-03\n",
            "Train Epoch: 275-36 batch_loss=1.49e-03\n",
            "Test set: Average loss: 0.0601 Average relative error: 0.0444\n",
            "Train Epoch: 276-0 batch_loss=1.91e-03\n",
            "Train Epoch: 276-12 batch_loss=1.93e-03\n",
            "Train Epoch: 276-24 batch_loss=1.60e-03\n",
            "Train Epoch: 276-36 batch_loss=1.64e-03\n",
            "Test set: Average loss: 0.0600 Average relative error: 0.0443\n",
            "Train Epoch: 277-0 batch_loss=1.53e-03\n",
            "Train Epoch: 277-12 batch_loss=1.73e-03\n",
            "Train Epoch: 277-24 batch_loss=1.52e-03\n",
            "Train Epoch: 277-36 batch_loss=1.89e-03\n",
            "Test set: Average loss: 0.0599 Average relative error: 0.0443\n",
            "Train Epoch: 278-0 batch_loss=1.49e-03\n",
            "Train Epoch: 278-12 batch_loss=2.08e-03\n",
            "Train Epoch: 278-24 batch_loss=1.45e-03\n",
            "Train Epoch: 278-36 batch_loss=1.63e-03\n",
            "Test set: Average loss: 0.0596 Average relative error: 0.0441\n",
            "Train Epoch: 279-0 batch_loss=1.57e-03\n",
            "Train Epoch: 279-12 batch_loss=1.52e-03\n",
            "Train Epoch: 279-24 batch_loss=1.67e-03\n",
            "Train Epoch: 279-36 batch_loss=2.18e-03\n",
            "Test set: Average loss: 0.0597 Average relative error: 0.0442\n",
            "Train Epoch: 280-0 batch_loss=2.00e-03\n",
            "Train Epoch: 280-12 batch_loss=1.58e-03\n",
            "Train Epoch: 280-24 batch_loss=1.36e-03\n",
            "Train Epoch: 280-36 batch_loss=2.02e-03\n",
            "Test set: Average loss: 0.0595 Average relative error: 0.0442\n",
            "Train Epoch: 281-0 batch_loss=1.28e-03\n",
            "Train Epoch: 281-12 batch_loss=1.33e-03\n",
            "Train Epoch: 281-24 batch_loss=1.60e-03\n",
            "Train Epoch: 281-36 batch_loss=1.96e-03\n",
            "Test set: Average loss: 0.0594 Average relative error: 0.0440\n",
            "Train Epoch: 282-0 batch_loss=1.53e-03\n",
            "Train Epoch: 282-12 batch_loss=1.44e-03\n",
            "Train Epoch: 282-24 batch_loss=1.53e-03\n",
            "Train Epoch: 282-36 batch_loss=1.23e-03\n",
            "Test set: Average loss: 0.0593 Average relative error: 0.0440\n",
            "Train Epoch: 283-0 batch_loss=1.79e-03\n",
            "Train Epoch: 283-12 batch_loss=2.29e-03\n",
            "Train Epoch: 283-24 batch_loss=1.69e-03\n",
            "Train Epoch: 283-36 batch_loss=1.62e-03\n",
            "Test set: Average loss: 0.0592 Average relative error: 0.0439\n",
            "Train Epoch: 284-0 batch_loss=1.63e-03\n",
            "Train Epoch: 284-12 batch_loss=1.42e-03\n",
            "Train Epoch: 284-24 batch_loss=1.56e-03\n",
            "Train Epoch: 284-36 batch_loss=1.35e-03\n",
            "Test set: Average loss: 0.0590 Average relative error: 0.0439\n",
            "Train Epoch: 285-0 batch_loss=1.74e-03\n",
            "Train Epoch: 285-12 batch_loss=1.54e-03\n",
            "Train Epoch: 285-24 batch_loss=1.69e-03\n",
            "Train Epoch: 285-36 batch_loss=1.94e-03\n",
            "Test set: Average loss: 0.0591 Average relative error: 0.0439\n",
            "Train Epoch: 286-0 batch_loss=1.64e-03\n",
            "Train Epoch: 286-12 batch_loss=1.75e-03\n",
            "Train Epoch: 286-24 batch_loss=1.88e-03\n",
            "Train Epoch: 286-36 batch_loss=1.83e-03\n",
            "Test set: Average loss: 0.0589 Average relative error: 0.0440\n",
            "Train Epoch: 287-0 batch_loss=1.30e-03\n",
            "Train Epoch: 287-12 batch_loss=1.58e-03\n",
            "Train Epoch: 287-24 batch_loss=1.70e-03\n",
            "Train Epoch: 287-36 batch_loss=1.37e-03\n",
            "Test set: Average loss: 0.0591 Average relative error: 0.0441\n",
            "Train Epoch: 288-0 batch_loss=1.54e-03\n",
            "Train Epoch: 288-12 batch_loss=1.73e-03\n",
            "Train Epoch: 288-24 batch_loss=1.50e-03\n",
            "Train Epoch: 288-36 batch_loss=1.72e-03\n",
            "Test set: Average loss: 0.0589 Average relative error: 0.0438\n",
            "Train Epoch: 289-0 batch_loss=1.85e-03\n",
            "Train Epoch: 289-12 batch_loss=1.37e-03\n",
            "Train Epoch: 289-24 batch_loss=1.37e-03\n",
            "Train Epoch: 289-36 batch_loss=2.03e-03\n",
            "Test set: Average loss: 0.0588 Average relative error: 0.0438\n",
            "Train Epoch: 290-0 batch_loss=1.54e-03\n",
            "Train Epoch: 290-12 batch_loss=1.94e-03\n",
            "Train Epoch: 290-24 batch_loss=1.39e-03\n",
            "Train Epoch: 290-36 batch_loss=1.59e-03\n",
            "Test set: Average loss: 0.0586 Average relative error: 0.0437\n",
            "Train Epoch: 291-0 batch_loss=1.88e-03\n",
            "Train Epoch: 291-12 batch_loss=1.77e-03\n",
            "Train Epoch: 291-24 batch_loss=1.85e-03\n",
            "Train Epoch: 291-36 batch_loss=1.35e-03\n",
            "Test set: Average loss: 0.0586 Average relative error: 0.0437\n",
            "Train Epoch: 292-0 batch_loss=1.82e-03\n",
            "Train Epoch: 292-12 batch_loss=1.63e-03\n",
            "Train Epoch: 292-24 batch_loss=2.27e-03\n",
            "Train Epoch: 292-36 batch_loss=1.74e-03\n",
            "Test set: Average loss: 0.0586 Average relative error: 0.0437\n",
            "Train Epoch: 293-0 batch_loss=1.67e-03\n",
            "Train Epoch: 293-12 batch_loss=2.47e-03\n",
            "Train Epoch: 293-24 batch_loss=1.49e-03\n",
            "Train Epoch: 293-36 batch_loss=1.80e-03\n",
            "Test set: Average loss: 0.0585 Average relative error: 0.0437\n",
            "Train Epoch: 294-0 batch_loss=1.93e-03\n",
            "Train Epoch: 294-12 batch_loss=1.52e-03\n",
            "Train Epoch: 294-24 batch_loss=1.42e-03\n",
            "Train Epoch: 294-36 batch_loss=1.15e-03\n",
            "Test set: Average loss: 0.0586 Average relative error: 0.0438\n",
            "Train Epoch: 295-0 batch_loss=1.62e-03\n",
            "Train Epoch: 295-12 batch_loss=1.92e-03\n",
            "Train Epoch: 295-24 batch_loss=1.83e-03\n",
            "Train Epoch: 295-36 batch_loss=1.43e-03\n",
            "Test set: Average loss: 0.0585 Average relative error: 0.0437\n",
            "Train Epoch: 296-0 batch_loss=1.94e-03\n",
            "Train Epoch: 296-12 batch_loss=1.52e-03\n",
            "Train Epoch: 296-24 batch_loss=1.75e-03\n",
            "Train Epoch: 296-36 batch_loss=1.70e-03\n",
            "Test set: Average loss: 0.0583 Average relative error: 0.0436\n",
            "Train Epoch: 297-0 batch_loss=1.32e-03\n",
            "Train Epoch: 297-12 batch_loss=1.42e-03\n",
            "Train Epoch: 297-24 batch_loss=1.65e-03\n",
            "Train Epoch: 297-36 batch_loss=2.06e-03\n",
            "Test set: Average loss: 0.0583 Average relative error: 0.0436\n",
            "Train Epoch: 298-0 batch_loss=1.75e-03\n",
            "Train Epoch: 298-12 batch_loss=1.49e-03\n",
            "Train Epoch: 298-24 batch_loss=1.18e-03\n",
            "Train Epoch: 298-36 batch_loss=1.74e-03\n",
            "Test set: Average loss: 0.0582 Average relative error: 0.0436\n",
            "Train Epoch: 299-0 batch_loss=1.55e-03\n",
            "Train Epoch: 299-12 batch_loss=1.52e-03\n",
            "Train Epoch: 299-24 batch_loss=1.81e-03\n",
            "Train Epoch: 299-36 batch_loss=1.86e-03\n",
            "Test set: Average loss: 0.0584 Average relative error: 0.0437\n",
            "Train Epoch: 300-0 batch_loss=1.83e-03\n",
            "Train Epoch: 300-12 batch_loss=1.87e-03\n",
            "Train Epoch: 300-24 batch_loss=1.62e-03\n",
            "Train Epoch: 300-36 batch_loss=1.33e-03\n",
            "Test set: Average loss: 0.0580 Average relative error: 0.0434\n",
            "Train Epoch: 301-0 batch_loss=2.48e-03\n",
            "Train Epoch: 301-12 batch_loss=1.45e-03\n",
            "Train Epoch: 301-24 batch_loss=1.67e-03\n",
            "Train Epoch: 301-36 batch_loss=1.74e-03\n",
            "Test set: Average loss: 0.0581 Average relative error: 0.0435\n",
            "Train Epoch: 302-0 batch_loss=1.58e-03\n",
            "Train Epoch: 302-12 batch_loss=1.60e-03\n",
            "Train Epoch: 302-24 batch_loss=1.68e-03\n",
            "Train Epoch: 302-36 batch_loss=1.98e-03\n",
            "Test set: Average loss: 0.0578 Average relative error: 0.0434\n",
            "Train Epoch: 303-0 batch_loss=1.21e-03\n",
            "Train Epoch: 303-12 batch_loss=1.83e-03\n",
            "Train Epoch: 303-24 batch_loss=1.66e-03\n",
            "Train Epoch: 303-36 batch_loss=1.68e-03\n",
            "Test set: Average loss: 0.0579 Average relative error: 0.0434\n",
            "Train Epoch: 304-0 batch_loss=1.74e-03\n",
            "Train Epoch: 304-12 batch_loss=1.92e-03\n",
            "Train Epoch: 304-24 batch_loss=1.67e-03\n",
            "Train Epoch: 304-36 batch_loss=1.58e-03\n",
            "Test set: Average loss: 0.0579 Average relative error: 0.0435\n",
            "Train Epoch: 305-0 batch_loss=1.37e-03\n",
            "Train Epoch: 305-12 batch_loss=1.43e-03\n",
            "Train Epoch: 305-24 batch_loss=1.88e-03\n",
            "Train Epoch: 305-36 batch_loss=1.47e-03\n",
            "Test set: Average loss: 0.0577 Average relative error: 0.0434\n",
            "Train Epoch: 306-0 batch_loss=1.58e-03\n",
            "Train Epoch: 306-12 batch_loss=1.56e-03\n",
            "Train Epoch: 306-24 batch_loss=1.49e-03\n",
            "Train Epoch: 306-36 batch_loss=1.62e-03\n",
            "Test set: Average loss: 0.0575 Average relative error: 0.0434\n",
            "Train Epoch: 307-0 batch_loss=1.75e-03\n",
            "Train Epoch: 307-12 batch_loss=1.67e-03\n",
            "Train Epoch: 307-24 batch_loss=1.54e-03\n",
            "Train Epoch: 307-36 batch_loss=2.00e-03\n",
            "Test set: Average loss: 0.0577 Average relative error: 0.0433\n",
            "Train Epoch: 308-0 batch_loss=1.37e-03\n",
            "Train Epoch: 308-12 batch_loss=1.72e-03\n",
            "Train Epoch: 308-24 batch_loss=2.00e-03\n",
            "Train Epoch: 308-36 batch_loss=1.56e-03\n",
            "Test set: Average loss: 0.0576 Average relative error: 0.0433\n",
            "Train Epoch: 309-0 batch_loss=1.34e-03\n",
            "Train Epoch: 309-12 batch_loss=1.52e-03\n",
            "Train Epoch: 309-24 batch_loss=1.53e-03\n",
            "Train Epoch: 309-36 batch_loss=1.28e-03\n",
            "Test set: Average loss: 0.0575 Average relative error: 0.0433\n",
            "Train Epoch: 310-0 batch_loss=1.28e-03\n",
            "Train Epoch: 310-12 batch_loss=1.70e-03\n",
            "Train Epoch: 310-24 batch_loss=1.79e-03\n",
            "Train Epoch: 310-36 batch_loss=1.48e-03\n",
            "Test set: Average loss: 0.0575 Average relative error: 0.0434\n",
            "Train Epoch: 311-0 batch_loss=1.74e-03\n",
            "Train Epoch: 311-12 batch_loss=2.30e-03\n",
            "Train Epoch: 311-24 batch_loss=1.80e-03\n",
            "Train Epoch: 311-36 batch_loss=1.32e-03\n",
            "Test set: Average loss: 0.0572 Average relative error: 0.0433\n",
            "Train Epoch: 312-0 batch_loss=1.50e-03\n",
            "Train Epoch: 312-12 batch_loss=1.76e-03\n",
            "Train Epoch: 312-24 batch_loss=1.54e-03\n",
            "Train Epoch: 312-36 batch_loss=1.45e-03\n",
            "Test set: Average loss: 0.0573 Average relative error: 0.0432\n",
            "Train Epoch: 313-0 batch_loss=1.23e-03\n",
            "Train Epoch: 313-12 batch_loss=1.91e-03\n",
            "Train Epoch: 313-24 batch_loss=1.86e-03\n",
            "Train Epoch: 313-36 batch_loss=1.94e-03\n",
            "Test set: Average loss: 0.0571 Average relative error: 0.0432\n",
            "Train Epoch: 314-0 batch_loss=1.50e-03\n",
            "Train Epoch: 314-12 batch_loss=1.94e-03\n",
            "Train Epoch: 314-24 batch_loss=2.04e-03\n",
            "Train Epoch: 314-36 batch_loss=1.45e-03\n",
            "Test set: Average loss: 0.0571 Average relative error: 0.0431\n",
            "Train Epoch: 315-0 batch_loss=1.80e-03\n",
            "Train Epoch: 315-12 batch_loss=1.70e-03\n",
            "Train Epoch: 315-24 batch_loss=1.58e-03\n",
            "Train Epoch: 315-36 batch_loss=1.98e-03\n",
            "Test set: Average loss: 0.0571 Average relative error: 0.0432\n",
            "Train Epoch: 316-0 batch_loss=2.15e-03\n",
            "Train Epoch: 316-12 batch_loss=1.60e-03\n",
            "Train Epoch: 316-24 batch_loss=1.86e-03\n",
            "Train Epoch: 316-36 batch_loss=1.85e-03\n",
            "Test set: Average loss: 0.0571 Average relative error: 0.0432\n",
            "Train Epoch: 317-0 batch_loss=1.23e-03\n",
            "Train Epoch: 317-12 batch_loss=1.47e-03\n",
            "Train Epoch: 317-24 batch_loss=1.59e-03\n",
            "Train Epoch: 317-36 batch_loss=2.38e-03\n",
            "Test set: Average loss: 0.0571 Average relative error: 0.0432\n",
            "Train Epoch: 318-0 batch_loss=1.38e-03\n",
            "Train Epoch: 318-12 batch_loss=1.69e-03\n",
            "Train Epoch: 318-24 batch_loss=1.63e-03\n",
            "Train Epoch: 318-36 batch_loss=1.44e-03\n",
            "Test set: Average loss: 0.0569 Average relative error: 0.0431\n",
            "Train Epoch: 319-0 batch_loss=1.73e-03\n",
            "Train Epoch: 319-12 batch_loss=1.49e-03\n",
            "Train Epoch: 319-24 batch_loss=1.29e-03\n",
            "Train Epoch: 319-36 batch_loss=1.31e-03\n",
            "Test set: Average loss: 0.0568 Average relative error: 0.0431\n",
            "Train Epoch: 320-0 batch_loss=1.47e-03\n",
            "Train Epoch: 320-12 batch_loss=1.67e-03\n",
            "Train Epoch: 320-24 batch_loss=1.52e-03\n",
            "Train Epoch: 320-36 batch_loss=1.68e-03\n",
            "Test set: Average loss: 0.0569 Average relative error: 0.0431\n",
            "Train Epoch: 321-0 batch_loss=1.20e-03\n",
            "Train Epoch: 321-12 batch_loss=1.23e-03\n",
            "Train Epoch: 321-24 batch_loss=2.14e-03\n",
            "Train Epoch: 321-36 batch_loss=1.98e-03\n",
            "Test set: Average loss: 0.0569 Average relative error: 0.0431\n",
            "Train Epoch: 322-0 batch_loss=1.82e-03\n",
            "Train Epoch: 322-12 batch_loss=1.14e-03\n",
            "Train Epoch: 322-24 batch_loss=1.75e-03\n",
            "Train Epoch: 322-36 batch_loss=2.05e-03\n",
            "Test set: Average loss: 0.0566 Average relative error: 0.0429\n",
            "Train Epoch: 323-0 batch_loss=1.33e-03\n",
            "Train Epoch: 323-12 batch_loss=1.71e-03\n",
            "Train Epoch: 323-24 batch_loss=1.47e-03\n",
            "Train Epoch: 323-36 batch_loss=1.42e-03\n",
            "Test set: Average loss: 0.0567 Average relative error: 0.0430\n",
            "Train Epoch: 324-0 batch_loss=1.50e-03\n",
            "Train Epoch: 324-12 batch_loss=1.31e-03\n",
            "Train Epoch: 324-24 batch_loss=1.70e-03\n",
            "Train Epoch: 324-36 batch_loss=1.47e-03\n",
            "Test set: Average loss: 0.0567 Average relative error: 0.0430\n",
            "Train Epoch: 325-0 batch_loss=1.72e-03\n",
            "Train Epoch: 325-12 batch_loss=1.51e-03\n",
            "Train Epoch: 325-24 batch_loss=1.99e-03\n",
            "Train Epoch: 325-36 batch_loss=1.33e-03\n",
            "Test set: Average loss: 0.0566 Average relative error: 0.0429\n",
            "Train Epoch: 326-0 batch_loss=2.05e-03\n",
            "Train Epoch: 326-12 batch_loss=1.51e-03\n",
            "Train Epoch: 326-24 batch_loss=1.23e-03\n",
            "Train Epoch: 326-36 batch_loss=1.47e-03\n",
            "Test set: Average loss: 0.0565 Average relative error: 0.0429\n",
            "Train Epoch: 327-0 batch_loss=1.44e-03\n",
            "Train Epoch: 327-12 batch_loss=1.80e-03\n",
            "Train Epoch: 327-24 batch_loss=1.59e-03\n",
            "Train Epoch: 327-36 batch_loss=1.22e-03\n",
            "Test set: Average loss: 0.0564 Average relative error: 0.0429\n",
            "Train Epoch: 328-0 batch_loss=1.48e-03\n",
            "Train Epoch: 328-12 batch_loss=1.12e-03\n",
            "Train Epoch: 328-24 batch_loss=1.42e-03\n",
            "Train Epoch: 328-36 batch_loss=1.95e-03\n",
            "Test set: Average loss: 0.0564 Average relative error: 0.0429\n",
            "Train Epoch: 329-0 batch_loss=1.85e-03\n",
            "Train Epoch: 329-12 batch_loss=1.44e-03\n",
            "Train Epoch: 329-24 batch_loss=1.26e-03\n",
            "Train Epoch: 329-36 batch_loss=1.21e-03\n",
            "Test set: Average loss: 0.0563 Average relative error: 0.0429\n",
            "Train Epoch: 330-0 batch_loss=1.16e-03\n",
            "Train Epoch: 330-12 batch_loss=1.51e-03\n",
            "Train Epoch: 330-24 batch_loss=1.64e-03\n",
            "Train Epoch: 330-36 batch_loss=1.82e-03\n",
            "Test set: Average loss: 0.0562 Average relative error: 0.0429\n",
            "Train Epoch: 331-0 batch_loss=1.53e-03\n",
            "Train Epoch: 331-12 batch_loss=1.60e-03\n",
            "Train Epoch: 331-24 batch_loss=1.72e-03\n",
            "Train Epoch: 331-36 batch_loss=1.69e-03\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-df0c3625c77a>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel_vel_trained_autoencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_training_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLP_vel_ConvAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_vel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_vel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# lr = 0.0005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-2e57f7ad1d30>\u001b[0m in \u001b[0;36mrun_training_conv\u001b[0;34m(model, train_output, test_output, num_epochs, lr, batch_size, wd, device)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m       train_loss = train_epoch_conv(\n\u001b[0m\u001b[1;32m    334\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m       )\n",
            "\u001b[0;32m<ipython-input-8-2e57f7ad1d30>\u001b[0m in \u001b[0;36mtrain_epoch_conv\u001b[0;34m(model, device, train_loader, optimizer, epoch, criterion, scheduler)\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mZ1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;31m# print(Z1.shape,Z2.shape,Z3.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     return torch._C._nn.mse_loss(\n\u001b[0m\u001b[1;32m   3793\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lr)"
      ],
      "metadata": {
        "id": "CzHDzBZZnkzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_vtk(filename):\n",
        "    \"\"\"Read .vtk file and return the polydata\"\"\"\n",
        "\n",
        "    fn_dir, fn_ext = os.path.splitext(filename)\n",
        "\n",
        "    if fn_ext == '.vtk':\n",
        "        reader = vtk.vtkPolyDataReader()\n",
        "    elif fn_ext == '.vtp':\n",
        "        reader = vtk.vtkXMLPolyDataReader()\n",
        "    elif fn_ext == '.stl':\n",
        "        reader = vtk.vtkSTLReader()\n",
        "    elif fn_ext == '.obj':\n",
        "        reader = vtk.vtkOBJReader()\n",
        "    elif fn_ext == '.vtu':\n",
        "        reader = vtk.vtkXMLUnstructuredGridReader()\n",
        "    elif fn_ext == '.pvtu':\n",
        "        reader = vtk.vtkXMLPUnstructuredGridReader()\n",
        "    else:\n",
        "        raise ValueError(F\"File extension {fn_ext} not supported\")\n",
        "\n",
        "    reader.SetFileName(filename)\n",
        "    reader.Update(0)\n",
        "    mesh = reader.GetOutput()\n",
        "\n",
        "    return mesh\n",
        "\n",
        "def write_vtk(mesh, fn):\n",
        "    \"\"\" Write a mesh (vtk polydata or unstructured grid) to disk \"\"\"\n",
        "\n",
        "    _, extension = os.path.splitext(fn)\n",
        "\n",
        "    if extension == '.vtk':\n",
        "        writer = vtk.vtkPolyDataWriter()\n",
        "    elif extension == '.stl':\n",
        "        writer = vtk.vtkSTLWriter()\n",
        "    elif extension == '.vtp':\n",
        "        writer = vtk.vtkXMLPolyDataWriter()\n",
        "    elif extension == '.vtu':\n",
        "        writer = vtk.vtkXMLUnstructuredGridWriter()\n",
        "    elif extension == '.obj':\n",
        "        writer = vtk.vtkOBJWriter()\n",
        "    else:\n",
        "        raise ValueError(f\"Unrecognized extension {extension}\")\n",
        "\n",
        "    writer.SetInputData(mesh)\n",
        "    writer.SetFileName(fn)\n",
        "    writer.Update(0)\n",
        "    writer.Write()\n",
        "\n",
        "    return\n",
        "\n",
        "def add_array(mesh, array, name):\n",
        "    \"\"\"Add numpy array as new field to a vtk file\"\"\"\n",
        "\n",
        "    new_array = numpy_to_vtk(array)\n",
        "    new_array.SetName(name)\n",
        "    mesh.GetPointData().AddArray(new_array)\n",
        "\n",
        "    return mesh\n",
        "\n",
        "def compute_matching_idxs():\n",
        "    \"\"\"Compute correspondences bewteen indices on the .vtu and on the .mesh file for plotting\"\"\"\n",
        "\n",
        "    mesh = read_vtk(os.path.join('/content/drive/MyDrive/data_ML4Science', 'geometries', 'bif_sym_alpha50_h0.10_ref.vtu'))\n",
        "    points = vtk_to_numpy(mesh.GetPoints().GetData())\n",
        "\n",
        "    mesh_2 = meshio.read(os.path.join('/content/drive/MyDrive/data_ML4Science', 'geometries','bif_sym_alpha50_h0.10.mesh'))\n",
        "    points_2 = mesh_2.points\n",
        "\n",
        "    dist = cdist(mesh_2.points, points, metric='euclidean')\n",
        "\n",
        "    idxs = np.argmin(dist, axis=0)\n",
        "\n",
        "    return idxs\n",
        "\n",
        "\n",
        "def visualize_solution(fields_array, fields=None, step_t=10):\n",
        "    \"\"\" Export the solution corresponding to the n-th snapshot (every step_t steps) to a .vtu file.\"\"\"\n",
        "\n",
        "    if fields is None:\n",
        "        fields = {'velocity': 3, 'pressure': 1}  # fields and corresponding dimensions\n",
        "\n",
        "    os.makedirs('solutions', exist_ok=True)\n",
        "\n",
        "    idxs = compute_matching_idxs()\n",
        "\n",
        "    mesh = read_vtk(os.path.join('/content/drive/MyDrive/data_ML4Science', 'geometries', 'bif_sym_alpha50_h0.10.vtu'))\n",
        "\n",
        "    fom_solution = dict()\n",
        "    for field in fields:\n",
        "        # print(f\"Processing field {field} - Dimension: {fields[field]}\")\n",
        "        cur_idxs = np.hstack([idxs + k * (Nh_space[field]//fields[field]) for k in range(fields[field])])\n",
        "        fom_solution[field] = expand(fields_array[field], basis_space[field], basis_time[field])[cur_idxs]\n",
        "        print(fom_solution[field].shape)\n",
        "\n",
        "    for cnt_t in range(0, Nh_time['velocity'], step_t):\n",
        "        # print(f\"\\nProcessing timestep {cnt_t} of {Nh_time['velocity']}\")\n",
        "        for field in fields:\n",
        "            cur_fom_solution = np.reshape(fom_solution[field][:, cnt_t], (fields[field], -1)).T\n",
        "            mesh = add_array(mesh, cur_fom_solution, field)\n",
        "\n",
        "        # write_vtk(mesh, os.path.join('solutions', f\"solution_{n}_{cnt_t}\" + '.vtu'))\n",
        "        write_vtk(mesh, os.path.join('solutions', f\"solution_{cnt_t}\" + '.vtu'))\n",
        "\n",
        "    return\n"
      ],
      "metadata": {
        "id": "WxKh2cvIOjf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_vel_trained.eval()\n",
        "# model_press_trained.eval()\n",
        "model_vel_trained_autoencoder.eval()\n",
        "model_press_trained_autoencoder.eval()\n",
        "\n",
        "input_tensor = torch.tensor(test_params[0,:], dtype=torch.float32)  # Trasforma in tensore\n",
        "input_tensor = input_tensor.unsqueeze(dim=0)\n",
        "input_tensor = input_tensor.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # output_vel_visual = model_vel_trained(input_tensor)\n",
        "    # output_vel_visual=output_vel_visual[0]\n",
        "    # print(output_vel_visual.shape)\n",
        "    # output_press_visual = model_press_trained(input_tensor)\n",
        "    # output_press_visual=output_press_visual[0]\n",
        "    # print(output_press_visual.shape)\n",
        "    output_vel_visual = model_vel_trained_autoencoder.predict(input_tensor)\n",
        "    output_vel_visual=output_vel_visual[0]\n",
        "    print(output_vel_visual.shape)\n",
        "    output_press_visual = model_press_trained_autoencoder.predict(input_tensor)\n",
        "    output_press_visual=output_press_visual[0]\n",
        "    print(output_press_visual.shape)\n",
        "\n",
        "output_visual=dict()\n",
        "output_visual['velocity']=output_vel_visual.cpu().numpy()\n",
        "output_visual['pressure']=output_press_visual.cpu().numpy()\n",
        "visualize_solution(output_visual,step_t=5)"
      ],
      "metadata": {
        "id": "Ezn9VDSNOwKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f290dd-3b10-44e3-9df7-28661a907324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([39, 16])\n",
            "torch.Size([9, 19])\n",
            "(10656, 1000)\n",
            "(3552, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r solutions.zip solutions\n",
        "from google.colab import files\n",
        "files.download('solutions.zip')"
      ],
      "metadata": {
        "id": "rWGvn5V8cY_I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5b2ada2-0c8e-418c-f2c3-d8a27973bef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: solutions/ (stored 0%)\n",
            "  adding: solutions/solution_285.vtu (deflated 29%)\n",
            "  adding: solutions/solution_565.vtu (deflated 29%)\n",
            "  adding: solutions/solution_535.vtu (deflated 29%)\n",
            "  adding: solutions/solution_295.vtu (deflated 29%)\n",
            "  adding: solutions/solution_410.vtu (deflated 29%)\n",
            "  adding: solutions/solution_855.vtu (deflated 29%)\n",
            "  adding: solutions/solution_945.vtu (deflated 29%)\n",
            "  adding: solutions/solution_590.vtu (deflated 29%)\n",
            "  adding: solutions/solution_805.vtu (deflated 29%)\n",
            "  adding: solutions/solution_820.vtu (deflated 29%)\n",
            "  adding: solutions/solution_650.vtu (deflated 29%)\n",
            "  adding: solutions/solution_940.vtu (deflated 29%)\n",
            "  adding: solutions/solution_110.vtu (deflated 29%)\n",
            "  adding: solutions/solution_595.vtu (deflated 29%)\n",
            "  adding: solutions/solution_635.vtu (deflated 29%)\n",
            "  adding: solutions/solution_340.vtu (deflated 29%)\n",
            "  adding: solutions/solution_985.vtu (deflated 29%)\n",
            "  adding: solutions/solution_55.vtu (deflated 29%)\n",
            "  adding: solutions/solution_365.vtu (deflated 29%)\n",
            "  adding: solutions/solution_800.vtu (deflated 29%)\n",
            "  adding: solutions/solution_355.vtu (deflated 29%)\n",
            "  adding: solutions/solution_160.vtu (deflated 29%)\n",
            "  adding: solutions/solution_860.vtu (deflated 29%)\n",
            "  adding: solutions/solution_670.vtu (deflated 29%)\n",
            "  adding: solutions/solution_735.vtu (deflated 29%)\n",
            "  adding: solutions/solution_790.vtu (deflated 29%)\n",
            "  adding: solutions/solution_930.vtu (deflated 29%)\n",
            "  adding: solutions/solution_995.vtu (deflated 29%)\n",
            "  adding: solutions/solution_965.vtu (deflated 29%)\n",
            "  adding: solutions/solution_350.vtu (deflated 29%)\n",
            "  adding: solutions/solution_170.vtu (deflated 29%)\n",
            "  adding: solutions/solution_400.vtu (deflated 29%)\n",
            "  adding: solutions/solution_30.vtu (deflated 29%)\n",
            "  adding: solutions/solution_835.vtu (deflated 29%)\n",
            "  adding: solutions/solution_515.vtu (deflated 29%)\n",
            "  adding: solutions/solution_510.vtu (deflated 29%)\n",
            "  adding: solutions/solution_85.vtu (deflated 29%)\n",
            "  adding: solutions/solution_230.vtu (deflated 29%)\n",
            "  adding: solutions/solution_330.vtu (deflated 29%)\n",
            "  adding: solutions/solution_580.vtu (deflated 29%)\n",
            "  adding: solutions/solution_395.vtu (deflated 29%)\n",
            "  adding: solutions/solution_830.vtu (deflated 29%)\n",
            "  adding: solutions/solution_40.vtu (deflated 29%)\n",
            "  adding: solutions/solution_165.vtu (deflated 29%)\n",
            "  adding: solutions/solution_815.vtu (deflated 29%)\n",
            "  adding: solutions/solution_960.vtu (deflated 29%)\n",
            "  adding: solutions/solution_200.vtu (deflated 29%)\n",
            "  adding: solutions/solution_505.vtu (deflated 29%)\n",
            "  adding: solutions/solution_75.vtu (deflated 29%)\n",
            "  adding: solutions/solution_955.vtu (deflated 29%)\n",
            "  adding: solutions/solution_120.vtu (deflated 29%)\n",
            "  adding: solutions/solution_600.vtu (deflated 29%)\n",
            "  adding: solutions/solution_500.vtu (deflated 29%)\n",
            "  adding: solutions/solution_390.vtu (deflated 29%)\n",
            "  adding: solutions/solution_905.vtu (deflated 29%)\n",
            "  adding: solutions/solution_730.vtu (deflated 29%)\n",
            "  adding: solutions/solution_865.vtu (deflated 29%)\n",
            "  adding: solutions/solution_20.vtu (deflated 29%)\n",
            "  adding: solutions/solution_460.vtu (deflated 29%)\n",
            "  adding: solutions/solution_475.vtu (deflated 29%)\n",
            "  adding: solutions/solution_260.vtu (deflated 29%)\n",
            "  adding: solutions/solution_890.vtu (deflated 29%)\n",
            "  adding: solutions/solution_425.vtu (deflated 29%)\n",
            "  adding: solutions/solution_255.vtu (deflated 29%)\n",
            "  adding: solutions/solution_615.vtu (deflated 29%)\n",
            "  adding: solutions/solution_95.vtu (deflated 29%)\n",
            "  adding: solutions/solution_15.vtu (deflated 29%)\n",
            "  adding: solutions/solution_480.vtu (deflated 29%)\n",
            "  adding: solutions/solution_585.vtu (deflated 29%)\n",
            "  adding: solutions/solution_10.vtu (deflated 29%)\n",
            "  adding: solutions/solution_525.vtu (deflated 29%)\n",
            "  adding: solutions/solution_555.vtu (deflated 29%)\n",
            "  adding: solutions/solution_315.vtu (deflated 29%)\n",
            "  adding: solutions/solution_5.vtu (deflated 29%)\n",
            "  adding: solutions/solution_240.vtu (deflated 29%)\n",
            "  adding: solutions/solution_420.vtu (deflated 29%)\n",
            "  adding: solutions/solution_495.vtu (deflated 29%)\n",
            "  adding: solutions/solution_620.vtu (deflated 29%)\n",
            "  adding: solutions/solution_810.vtu (deflated 29%)\n",
            "  adding: solutions/solution_125.vtu (deflated 29%)\n",
            "  adding: solutions/solution_90.vtu (deflated 29%)\n",
            "  adding: solutions/solution_575.vtu (deflated 29%)\n",
            "  adding: solutions/solution_455.vtu (deflated 29%)\n",
            "  adding: solutions/solution_740.vtu (deflated 29%)\n",
            "  adding: solutions/solution_190.vtu (deflated 29%)\n",
            "  adding: solutions/solution_570.vtu (deflated 29%)\n",
            "  adding: solutions/solution_375.vtu (deflated 29%)\n",
            "  adding: solutions/solution_545.vtu (deflated 29%)\n",
            "  adding: solutions/solution_990.vtu (deflated 29%)\n",
            "  adding: solutions/solution_270.vtu (deflated 29%)\n",
            "  adding: solutions/solution_900.vtu (deflated 29%)\n",
            "  adding: solutions/solution_470.vtu (deflated 29%)\n",
            "  adding: solutions/solution_135.vtu (deflated 29%)\n",
            "  adding: solutions/solution_35.vtu (deflated 29%)\n",
            "  adding: solutions/solution_300.vtu (deflated 29%)\n",
            "  adding: solutions/solution_80.vtu (deflated 29%)\n",
            "  adding: solutions/solution_435.vtu (deflated 29%)\n",
            "  adding: solutions/solution_210.vtu (deflated 29%)\n",
            "  adding: solutions/solution_345.vtu (deflated 29%)\n",
            "  adding: solutions/solution_725.vtu (deflated 29%)\n",
            "  adding: solutions/solution_310.vtu (deflated 29%)\n",
            "  adding: solutions/solution_775.vtu (deflated 29%)\n",
            "  adding: solutions/solution_605.vtu (deflated 29%)\n",
            "  adding: solutions/solution_760.vtu (deflated 29%)\n",
            "  adding: solutions/solution_540.vtu (deflated 29%)\n",
            "  adding: solutions/solution_60.vtu (deflated 29%)\n",
            "  adding: solutions/solution_625.vtu (deflated 29%)\n",
            "  adding: solutions/solution_265.vtu (deflated 29%)\n",
            "  adding: solutions/solution_450.vtu (deflated 29%)\n",
            "  adding: solutions/solution_825.vtu (deflated 29%)\n",
            "  adding: solutions/solution_710.vtu (deflated 29%)\n",
            "  adding: solutions/solution_290.vtu (deflated 29%)\n",
            "  adding: solutions/solution_65.vtu (deflated 29%)\n",
            "  adding: solutions/solution_755.vtu (deflated 29%)\n",
            "  adding: solutions/solution_215.vtu (deflated 29%)\n",
            "  adding: solutions/solution_640.vtu (deflated 29%)\n",
            "  adding: solutions/solution_220.vtu (deflated 29%)\n",
            "  adding: solutions/solution_660.vtu (deflated 29%)\n",
            "  adding: solutions/solution_25.vtu (deflated 29%)\n",
            "  adding: solutions/solution_690.vtu (deflated 29%)\n",
            "  adding: solutions/solution_245.vtu (deflated 29%)\n",
            "  adding: solutions/solution_185.vtu (deflated 29%)\n",
            "  adding: solutions/solution_205.vtu (deflated 29%)\n",
            "  adding: solutions/solution_720.vtu (deflated 29%)\n",
            "  adding: solutions/solution_50.vtu (deflated 29%)\n",
            "  adding: solutions/solution_885.vtu (deflated 29%)\n",
            "  adding: solutions/solution_465.vtu (deflated 29%)\n",
            "  adding: solutions/solution_275.vtu (deflated 29%)\n",
            "  adding: solutions/solution_925.vtu (deflated 29%)\n",
            "  adding: solutions/solution_550.vtu (deflated 29%)\n",
            "  adding: solutions/solution_105.vtu (deflated 29%)\n",
            "  adding: solutions/solution_665.vtu (deflated 29%)\n",
            "  adding: solutions/solution_920.vtu (deflated 29%)\n",
            "  adding: solutions/solution_655.vtu (deflated 29%)\n",
            "  adding: solutions/solution_850.vtu (deflated 29%)\n",
            "  adding: solutions/solution_195.vtu (deflated 29%)\n",
            "  adding: solutions/solution_415.vtu (deflated 29%)\n",
            "  adding: solutions/solution_305.vtu (deflated 29%)\n",
            "  adding: solutions/solution_840.vtu (deflated 29%)\n",
            "  adding: solutions/solution_145.vtu (deflated 29%)\n",
            "  adding: solutions/solution_45.vtu (deflated 29%)\n",
            "  adding: solutions/solution_130.vtu (deflated 29%)\n",
            "  adding: solutions/solution_385.vtu (deflated 29%)\n",
            "  adding: solutions/solution_705.vtu (deflated 29%)\n",
            "  adding: solutions/solution_530.vtu (deflated 29%)\n",
            "  adding: solutions/solution_320.vtu (deflated 29%)\n",
            "  adding: solutions/solution_695.vtu (deflated 29%)\n",
            "  adding: solutions/solution_0.vtu (deflated 29%)\n",
            "  adding: solutions/solution_645.vtu (deflated 29%)\n",
            "  adding: solutions/solution_360.vtu (deflated 29%)\n",
            "  adding: solutions/solution_745.vtu (deflated 29%)\n",
            "  adding: solutions/solution_715.vtu (deflated 29%)\n",
            "  adding: solutions/solution_325.vtu (deflated 29%)\n",
            "  adding: solutions/solution_140.vtu (deflated 29%)\n",
            "  adding: solutions/solution_780.vtu (deflated 29%)\n",
            "  adding: solutions/solution_785.vtu (deflated 29%)\n",
            "  adding: solutions/solution_100.vtu (deflated 29%)\n",
            "  adding: solutions/solution_880.vtu (deflated 29%)\n",
            "  adding: solutions/solution_485.vtu (deflated 29%)\n",
            "  adding: solutions/solution_380.vtu (deflated 29%)\n",
            "  adding: solutions/solution_895.vtu (deflated 29%)\n",
            "  adding: solutions/solution_370.vtu (deflated 29%)\n",
            "  adding: solutions/solution_430.vtu (deflated 29%)\n",
            "  adding: solutions/solution_910.vtu (deflated 29%)\n",
            "  adding: solutions/solution_685.vtu (deflated 29%)\n",
            "  adding: solutions/solution_445.vtu (deflated 29%)\n",
            "  adding: solutions/solution_680.vtu (deflated 29%)\n",
            "  adding: solutions/solution_950.vtu (deflated 29%)\n",
            "  adding: solutions/solution_975.vtu (deflated 29%)\n",
            "  adding: solutions/solution_490.vtu (deflated 29%)\n",
            "  adding: solutions/solution_980.vtu (deflated 29%)\n",
            "  adding: solutions/solution_765.vtu (deflated 29%)\n",
            "  adding: solutions/solution_70.vtu (deflated 29%)\n",
            "  adding: solutions/solution_335.vtu (deflated 29%)\n",
            "  adding: solutions/solution_970.vtu (deflated 29%)\n",
            "  adding: solutions/solution_175.vtu (deflated 29%)\n",
            "  adding: solutions/solution_610.vtu (deflated 29%)\n",
            "  adding: solutions/solution_875.vtu (deflated 29%)\n",
            "  adding: solutions/solution_845.vtu (deflated 29%)\n",
            "  adding: solutions/solution_630.vtu (deflated 29%)\n",
            "  adding: solutions/solution_675.vtu (deflated 29%)\n",
            "  adding: solutions/solution_870.vtu (deflated 29%)\n",
            "  adding: solutions/solution_115.vtu (deflated 29%)\n",
            "  adding: solutions/solution_560.vtu (deflated 29%)\n",
            "  adding: solutions/solution_520.vtu (deflated 29%)\n",
            "  adding: solutions/solution_155.vtu (deflated 29%)\n",
            "  adding: solutions/solution_150.vtu (deflated 29%)\n",
            "  adding: solutions/solution_700.vtu (deflated 29%)\n",
            "  adding: solutions/solution_750.vtu (deflated 29%)\n",
            "  adding: solutions/solution_915.vtu (deflated 29%)\n",
            "  adding: solutions/solution_250.vtu (deflated 29%)\n",
            "  adding: solutions/solution_440.vtu (deflated 29%)\n",
            "  adding: solutions/solution_225.vtu (deflated 29%)\n",
            "  adding: solutions/solution_770.vtu (deflated 29%)\n",
            "  adding: solutions/solution_280.vtu (deflated 29%)\n",
            "  adding: solutions/solution_405.vtu (deflated 29%)\n",
            "  adding: solutions/solution_795.vtu (deflated 29%)\n",
            "  adding: solutions/solution_180.vtu (deflated 29%)\n",
            "  adding: solutions/solution_235.vtu (deflated 29%)\n",
            "  adding: solutions/solution_935.vtu (deflated 29%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_beb0ab12-b178-43d0-8d1d-45a8aba7bd9c\", \"solutions.zip\", 57611833)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}