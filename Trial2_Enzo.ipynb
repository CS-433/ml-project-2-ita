{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22332,"status":"ok","timestamp":1732692357621,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"},"user_tz":-60},"id":"Invi4SwWUGt0","outputId":"73d38209-24be-4ee6-9e13-bb1e2fbd611d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vtk\n","  Downloading vtk-9.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from vtk) (3.8.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (4.55.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (1.4.7)\n","Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->vtk) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->vtk) (1.16.0)\n","Downloading vtk-9.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (105.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: vtk\n","Successfully installed vtk-9.4.0\n","Collecting meshio\n","  Downloading meshio-5.3.5-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from meshio) (1.26.4)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from meshio) (13.9.4)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->meshio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->meshio) (2.18.0)\n","Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->meshio) (4.12.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->meshio) (0.1.2)\n","Downloading meshio-5.3.5-py3-none-any.whl (166 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.2/166.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: meshio\n","Successfully installed meshio-5.3.5\n"]}],"source":["!pip install vtk\n","!pip install meshio"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"xcjMZnnRwpCN","executionInfo":{"status":"ok","timestamp":1732692377893,"user_tz":-60,"elapsed":20280,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}}},"outputs":[],"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import vtkmodules.all as vtk\n","from vtkmodules.util.numpy_support import vtk_to_numpy, numpy_to_vtk\n","import meshio\n","from scipy.spatial.distance import cdist\n","from torchvision import datasets, transforms\n","import random"]},{"cell_type":"code","source":["def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)"],"metadata":{"id":"R9olb7sJ2u0T","executionInfo":{"status":"ok","timestamp":1732692377893,"user_tz":-60,"elapsed":5,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23738,"status":"ok","timestamp":1732692401627,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"},"user_tz":-60},"id":"UQ3-Jiio0MMr","outputId":"7d43bf8b-ebb9-48e8-e232-29a44c16c95f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#device=torch.device('cpu')\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"GdOhmH83cwz2","executionInfo":{"status":"ok","timestamp":1732692401627,"user_tz":-60,"elapsed":5,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}}},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"z6Cg8T5wmrPq","executionInfo":{"status":"ok","timestamp":1732692401627,"user_tz":-60,"elapsed":4,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}}},"outputs":[],"source":["DATASET_PATH = '/content/drive/MyDrive/data_ML4Science'"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_Bqc_7Kl3RmC","executionInfo":{"status":"ok","timestamp":1732692408800,"user_tz":-60,"elapsed":7177,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}}},"outputs":[],"source":["fields = {'velocity', 'pressure'}\n","\n","basis_space, sv_space, Nh_space, nmodes_space = dict(), dict(), dict(), dict()\n","basis_time, sv_time, Nh_time, nmodes_time = dict(), dict(), dict(), dict()\n","nmodes = dict()\n","for field in fields:\n","    basis_space[field] = np.load(os.path.join(DATASET_PATH, 'basis', field, 'space_basis.npy'))  # spatial basis\n","    sv_space[field] = np.load(os.path.join(DATASET_PATH, 'basis', field, 'space_sv.npy'))  # singular values in space\n","    Nh_space[field], nmodes_space[field] = basis_space[field].shape  # number of FOM and ROM unknowns in space\n","    basis_time[field] = np.load(os.path.join(DATASET_PATH, 'basis', field, 'time_basis.npy'))  # temporal basis\n","    sv_time[field] = np.load(os.path.join(DATASET_PATH, 'basis', field, 'time_sv.npy'))  # singular values in time\n","    Nh_time[field], nmodes_time[field] = basis_time[field].shape  # number of FOM and ROM unknowns in time\n","    nmodes[field] = nmodes_space[field] * nmodes_time[field]  # total dimension of the reduced basis\n","\n","# UPDATE VELOCITY BASES TO ACCOUNT FOR SUPREMIZERS AND STABILIZERS\n","N_supr_space = basis_space['pressure'].shape[1] + 66  # number of extra bases in space for the velocity\n","N_supr_time = 5  # number of extra bases in time for the velocity\n","\n","# STORE ORIGINAL NUMBER OF VELOCITY MODES IN THE DICTIONARY\n","nmodes_space['velocity_full'] = nmodes_space['velocity']\n","nmodes_time['velocity_full'] = nmodes_time['velocity']\n","nmodes['velocity_full'] = nmodes['velocity']\n","\n","# UPDATE THE NUMBER OF VELOCITY MODES TO ACCOUNT FOR SUPREMIZERS AND STABILIZERS\n","nmodes_space['velocity'] -= N_supr_space\n","nmodes_time['velocity'] -= N_supr_time\n","nmodes['velocity'] = nmodes_space['velocity'] * nmodes_time['velocity']\n","\n","# UPDATE VELOCITY BASES TO ACCOUNT FOR SUPREMIZERS AND STABILIZERS\n","basis_space['velocity'] = basis_space['velocity'][:, :nmodes_space['velocity']]\n","basis_time['velocity'] = basis_time['velocity'][:, :nmodes_time['velocity']]\n","\n","# LOAD NORMED BASIS MATRICES IN SPACE (needed for projections)\n","basis_space_normed = dict()\n","#norm = dict()\n","for field in fields:\n","    #norm[field] = load_npz(os.path.join('dataset', 'norms', f'norm_{field}.npz'))\n","    #basis_space_normed[field] = norm[field].dot(basis_space[field])\n","    #np.save(os.path.join('dataset', 'basis', field, 'basis_space_normed.npy'), basis_space_normed[field])\n","    basis_space_normed[field] = np.load(os.path.join(DATASET_PATH, 'basis', field, 'basis_space_normed.npy'))\n","\n","##################################################################\n","def project(sol, normed_basis_space, basis_time):\n","    \"\"\" Project a full-order solution in space-time.\"\"\"\n","    return (normed_basis_space.T.dot(sol)).dot(basis_time) # !! REMARK: here we need the normed basis in space !!\n","\n","def expand(sol, basis_space, basis_time):\n","    \"\"\" Expand a reduced-order solution in space-time.\"\"\"\n","    return (basis_space.dot(sol)).dot(basis_time.T)\n","\n","##################################################################"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"mA3wRZEOoVLB","executionInfo":{"status":"ok","timestamp":1732692409586,"user_tz":-60,"elapsed":788,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}}},"outputs":[],"source":["params = np.load(os.path.join(DATASET_PATH, 'RB_data', 'parameters.npy'))\n","params = np.delete(params, 2, axis=1)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"K0EhSv14og6T","executionInfo":{"status":"ok","timestamp":1732692412800,"user_tz":-60,"elapsed":3218,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}}},"outputs":[],"source":["n_snaps = None  # change to a number if you want to load only a subset of snapshots\n","_sol = np.load(os.path.join(DATASET_PATH, 'RB_data', 'solutions.npy'))[:n_snaps]\n","\n","solutions = dict()\n","\n","# velocity reduced solutions (with and without supremizers and stabilizers)\n","solutions['velocity_full'] = np.reshape(_sol[:, :nmodes['velocity_full']],\n","                                        (-1, nmodes_space['velocity_full'], nmodes_time['velocity_full']))\n","solutions['velocity'] = solutions['velocity_full'][:, :nmodes_space['velocity'], :nmodes_time['velocity']]\n","\n","# pressure reduced solutions\n","solutions['pressure'] = np.reshape(_sol[:, :nmodes['pressure']],\n","                                   (-1, nmodes_space['pressure'], nmodes_time['pressure']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhZYXNL9n_ju","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732658105669,"user_tz":-60,"elapsed":14,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}},"outputId":"b9c84ea0-9bca-4b6f-a17f-acc4513d445d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(76974, 39)"]},"metadata":{},"execution_count":10}],"source":["basis_space['velocity'].shape #Vs = Ns *ns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_IJ_VUYoCEo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732658105669,"user_tz":-60,"elapsed":12,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}},"outputId":"9b5c6836-441e-49c2-8fd3-7e69f6081048"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 16)"]},"metadata":{},"execution_count":11}],"source":["basis_time['velocity'].shape"]},{"cell_type":"markdown","metadata":{"id":"WdrwegyJc-wT"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqQ58u-UoEeT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732631125780,"user_tz":-60,"elapsed":259,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}},"outputId":"6dc469d7-a106-41dd-d53f-e9509bf49361"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3552, 9)"]},"metadata":{},"execution_count":15}],"source":["basis_space['pressure'].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMO1E5SjoGvU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732631128042,"user_tz":-60,"elapsed":217,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}},"outputId":"d0acd185-388f-4a4b-f446-5a79f400d998"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 19)"]},"metadata":{},"execution_count":16}],"source":["basis_time['pressure'].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sstQrU6Ds11j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732631129500,"user_tz":-60,"elapsed":226,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}},"outputId":"d641d779-dc7f-4d60-8a4e-e006af059975"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1950, 9, 19)"]},"metadata":{},"execution_count":17}],"source":["solutions['pressure'].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8O_IrlxOxbRy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732631132117,"user_tz":-60,"elapsed":247,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}},"outputId":"e9632f25-d3a6-4e1b-90f7-a34cf4b90c5d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1950, 39, 16)"]},"metadata":{},"execution_count":18}],"source":["solutions['velocity'].shape"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"57AtIrbHogWJ","executionInfo":{"status":"ok","timestamp":1732692412800,"user_tz":-60,"elapsed":5,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}}},"outputs":[],"source":["solutions['pressure']= torch.tensor(solutions['pressure'], dtype=torch.float32)\n","solutions['velocity']= torch.tensor(solutions['velocity'], dtype=torch.float32)\n","params = torch.tensor(params, dtype=torch.float32)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"CMIFcsExttZc","executionInfo":{"status":"ok","timestamp":1732692412800,"user_tz":-60,"elapsed":5,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}}},"outputs":[],"source":["class Fluid_Dataset(Dataset):\n","    def __init__(self, inputs, targets):\n","        self.inputs = inputs\n","        self.targets =  targets\n","\n","    def __len__(self):\n","        return self.inputs.shape[0]\n","\n","    def __getitem__(self, idx):\n","\n","        input = self.inputs[idx]\n","        output = self.targets[idx]\n","        return input, output\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1732692412800,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"},"user_tz":-60},"id":"zePSfOcbvLOk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4181c5d3-6945-41eb-ef68-6c7fef753370"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([151.1840,  47.9287,  -4.8919, -13.6345,  15.4792,  -4.2125,  -0.3348,\n","          5.1385,  -0.6269,  -3.8354,  -0.5111,   4.1919,  -3.5017,  -0.8359,\n","         -0.5537,  -0.7395])\n"]}],"source":["N_data=params.shape[0]\n","indices = torch.randperm(N_data)\n","ratio_data=0.8\n","train_size = int(ratio_data * N_data)\n","train_indices = indices[:train_size]\n","test_indices=indices[train_size:]\n","\n","train_params=params[train_indices]\n","test_params=params[test_indices]\n","\n","mean_params=torch.mean(train_params,dim=0)\n","std_params=torch.std(train_params,dim=0)\n","train_params=(train_params-mean_params)/std_params\n","test_params=(test_params-mean_params)/std_params\n","\n","\n","train_vel=solutions['velocity'][train_indices]\n","\n","test_vel=solutions['velocity'][test_indices]\n","\n","train_press=solutions['pressure'][train_indices]\n","test_press=solutions['pressure'][test_indices]\n","print(train_vel[0,0,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZLHSePTbcFq_","executionInfo":{"status":"ok","timestamp":1732633894542,"user_tz":-60,"elapsed":220,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"90a088b3-bb44-4708-8099-0b73a74e7f7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 1.7319,  0.2168, -0.5090, -0.7062,  0.7336,  2.1690,  0.4455, -2.7250,\n","        -1.2003, -0.4249,  1.8618, -1.7595, -0.5973, -0.1114,  0.1245,  0.8888])\n"]}],"source":["#STANDARDITZAION ALTERNATIVE ZERO\n","vel_space_mean = torch.mean(train_vel, dim= 0)\n","vel_space_std = torch.std(train_vel, dim=0)\n","train_vel = (train_vel - vel_space_mean) / vel_space_std\n","test_vel =(test_vel - vel_space_mean) / vel_space_std\n","\n","\n","print(train_vel[0,0,:])\n"]},{"cell_type":"code","source":["print(train_vel.mean())\n","print(test_vel.mean())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8TI9AnOoEkj","executionInfo":{"status":"ok","timestamp":1732631389712,"user_tz":-60,"elapsed":220,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}},"outputId":"2529f791-eff8-4002-b703-52b36ab24f0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(6.3876e-10)\n","tensor(0.0003)\n"]}]},{"cell_type":"code","execution_count":13,"metadata":{"id":"foxgkv9l0ikm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732692413027,"user_tz":-60,"elapsed":231,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}},"outputId":"db8a3c9b-f06b-4920-ca68-1478a34d40ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.5125, 0.4972, 0.3822, 0.1550, 0.7809, 0.2477, 0.4258, 0.5963, 0.3093,\n","        0.2861, 0.3506, 0.6018, 0.7280, 0.6898, 0.3659, 0.5932])\n"]}],"source":["\n","#STANDARDITZAION ALTERNATIVE ONE\n","vel_space_max = torch.max(train_vel, dim= 0).values\n","vel_space_min = torch.min(train_vel, dim=0).values\n","train_vel = (train_vel - vel_space_min*0.95) / (1.05*vel_space_max - 0.95*vel_space_min)\n","test_vel =(test_vel - vel_space_min*0.95) / (1.05*vel_space_max - 0.95*vel_space_min)\n","\n","\n","\n","print(train_vel[0,0,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vnyE0gXK6WLS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jyi6yYWK2MEF"},"outputs":[],"source":["train_params.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJyxsOfu2PL1"},"outputs":[],"source":["test_params.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2ePjgZznYjm"},"outputs":[],"source":["train_vel.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzd6zZONneMB"},"outputs":[],"source":["test_vel.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xl0I7JiFuMXM"},"outputs":[],"source":["#IGNORE THIS FOR THE MOMENT\n","class MLP_press_ConvAutoencoder(torch.nn.Module):\n","    def __init__(self, parameters=3, dim_1=39, dim_2=16, dim_1_reduced=18, dim_2_reduced=8, L=4, K=256):\n","        super(MLP_press_ConvAutoencoder, self).__init__()\n","\n","        self.mlp = torch.nn.Sequential(\n","            *[layer for i in range(L) for layer in [\n","                torch.nn.Linear(parameters if i == 0 else K, K),\n","                torch.nn.LayerNorm(K),\n","                torch.nn.GELU(),\n","            ]],\n","            torch.nn.Linear(K, dim_1_reduced * dim_2_reduced),\n","            torch.nn.Unflatten(1, (dim_1_reduced, dim_2_reduced))  # Reshaping to (-1, dim_1_reduced, dim_2_reduced)\n","        )\n","\n","        # Encoder\n","        self.encoder = torch.nn.Sequential(\n","            torch.nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # (batch_size, 1, dim_1, dim_2) -> (batch_size, 16, 20, 8)\n","            torch.nn.BatchNorm2d(num_features=16),\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # (batch_size, 16, 20, 8) -> (batch_size, 32, 10, 4)\n","            torch.nn.BatchNorm2d(num_features=32),\n","            torch.nn.ReLU(),\n","            torch.nn.Flatten(),  # (batch_size, 32, 10, 4) -> (batch_size, 1280)\n","            torch.nn.Linear(32*10*4, dim_1_reduced * dim_2_reduced),  # (batch_size, 1280) -> (batch_size, dim_1_reduced * dim_2_reduced)\n","            torch.nn.ReLU(),\n","            torch.nn.Unflatten(1, (dim_1_reduced , dim_2_reduced))  # Reshape to (batch_size, dim_1_reduced, dim_2_reduced)\n","        )\n","\n","        # Decoder\n","        self.decoder = torch.nn.Sequential(\n","      torch.nn.Flatten(1),\n","      torch.nn.Linear(18 * 8, 32 * 10 * 4),  # (batch_size, dim_1_reduced * dim_2_reduced) -> (batch_size, 32*10*4)\n","      torch.nn.BatchNorm1d(32 * 10 * 4),  # Batch normalization before ReLU\n","      torch.nn.ReLU(),\n","      torch.nn.Unflatten(1, (32, 10, 4)),  # (batch_size, 32*10*4) -> (batch_size, 32, 10, 4)\n","      torch.nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=(1, 1)),  # (batch_size, 32, 10, 4) -> (batch_size, 16, 20, 8)\n","      torch.nn.BatchNorm2d(16),  # Batch normalization before ReLU\n","      torch.nn.ReLU(),\n","      torch.nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=(0, 1)),  # (batch_size, 16, 20, 8) -> (batch_size, 1, 39, 16)\n","\n",")\n","\n","\n","    def forward(self, par, out):\n","        Z1 = self.mlp(par)  # (dim_1_reduced * dim_2_reduced)\n","\n","        out_squeezed = out.unsqueeze(1)\n","          # Adding channel dimension to input\n","        Z2 = self.encoder(out_squeezed)  # (dim_1_reduced * dim_2_reduced)\n","\n","        Z3 = self.decoder(Z2) # (batch_siz, 1, dim_1, dim_2)\n","        Z3=Z3.squeeze(1)  # (batch_siz, dim_1, dim_2)\n","\n","        return Z1, Z2, Z3\n","\n","    def predict(self, par):\n","        Z = self.mlp(par)  # (dim_1_reduced * dim_2_reduced)\n","        Z = self.decoder(Z)  # (batch_size, 1, dim_1, dim_2)\n","        Z=Z.squeeze(1)\n","        return Z\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"GVL4yfwkYva1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","model = MLP_press_ConvAutoencoder()\n","  # Valori casuali tra 0 e 1\n","z1, z2, z3= model.forward(train_params,train_vel)"],"metadata":{"id":"VZUhqoGWT8y2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["z1.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_lXkgjfT-Ua","executionInfo":{"status":"ok","timestamp":1732521227682,"user_tz":-60,"elapsed":3,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"bb982e8b-4ead-4860-a2af-0fd7a60e6bb6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1560, 18, 8])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["z2.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJ2FXxmGT_cB","executionInfo":{"status":"ok","timestamp":1732521245108,"user_tz":-60,"elapsed":237,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"aa7ae255-258c-4431-d2ee-b5a49f9e8fde"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1560, 18, 8])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["z3.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UA9_cwCcUD7S","executionInfo":{"status":"ok","timestamp":1732521250484,"user_tz":-60,"elapsed":335,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"544103b0-4554-46a4-d164-7bc23bad75e5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1560, 39, 16])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["#I'M USING THIS AT THE MOMENT\n","\n","class MLP_press_ConvAutoencoder_2(torch.nn.Module):\n","    def __init__(self, parameters=3, dim_1=39, dim_2=16, dim_reduced=128, L=4, K=256):\n","        super(MLP_press_ConvAutoencoder_2, self).__init__()\n","\n","        self.mlp = torch.nn.Sequential(\n","            *[layer for i in range(L) for layer in [\n","                torch.nn.Linear(parameters if i == 0 else K, K),\n","                torch.nn.LayerNorm(K),\n","                torch.nn.ReLU(),\n","            ]],\n","            torch.nn.Linear(K, dim_reduced), #(batch_size, K) -> (batch_size, dim_reduced)\n","\n","        )\n","\n","        # Encoder\n","\n","        self.encoder = torch.nn.Sequential(\n","            torch.nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=1),  # (batch_size, 1, dim_1, dim_2) -> (batch_size, 64, 19, 7)\n","            #torch.nn.BatchNorm2d(64),\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # (batch_size, 64, 19, 7) -> (batch_size, 128, 10, 4)\n","            #torch.nn.BatchNorm2d(128),\n","            torch.nn.ReLU(),\n","            torch.nn.Flatten(),      #(batch_size, 128, 10, 4) -> (batch_size, 128 x 10 x 4)\n","            torch.nn.Linear(128*10*4, 256),    #(batch_size, 128 x 10 x 4) ->  batch_size, 256)\n","            #torch.nn.Dropout(p=0.3)\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(256, dim_reduced)  #(batch_size, 256) ->  (batch_size, dim_reduced)\n","\n","        )\n","\n","\n","        # Decoder\n","        self.decoder = torch.nn.Sequential(\n","            torch.nn.Linear(dim_reduced, 256),    # (batch_size, dim_reduced)-> #(batch_size, 256)\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(256, 128*10*4), # (batch_size, 256)-> #(batch_size, 128*10*4)\n","            torch.nn.ReLU(),\n","            torch.nn.Unflatten(1, (128, 10, 4)),      #(batch_size, 128*10*4) -> (batch_size, 128, 10, 4)\n","            torch.nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=(0, 0)), # (batch_size, 128, 10, 4) -> # (batch_size, 64, 19, 7)\n","            #torch.nn.BatchNorm2d(64),\n","            torch.nn.ReLU(),\n","            torch.nn.ConvTranspose2d(64, 1, kernel_size=5, stride=2, padding=1, output_padding=(0, 1)),   # -> (batch_size, 64, 19, 7) --> (batch_size, 1, dim_1, dim_2)\n","            torch.nn.Sigmoid() #LEVAREEEEEEEE SE NON MIN MAX\n","            #-------->LEVAREEEEEEEE SE NON MIN MAX\n","            #-------->LEVAREEEEEEEE SE NON MIN MAX\n","            #-------->LEVAREEEEEEEE SE NON MIN MAX\n","            #-------->LEVAREEEEEEEE SE NON MIN MAX\n","            #-------->LEVAREEEEEEEE SE NON MIN MAX\n","            #-------->LEVAREEEEEEEE SE NON MIN MAX\n","        )\n","\n","\n","    def forward(self, par, out):\n","        Z1 = self.mlp(par)  # (batch_size, dim_reduced)\n","\n","        out_squeezed = out.unsqueeze(1) #(batch_size x 39 x 16) -> #(batch_size x 1 x 39 x 16)\n","        Z2 = self.encoder(out_squeezed)  # (batch_size, dim_reduced)\n","\n","        Z3 = self.decoder(Z2) # (batch_siz, 1, dim_1, dim_2)\n","        Z3=Z3.squeeze(1)  # (batch_siz, dim_1, dim_2)\n","\n","        return Z1, Z2, Z3\n","\n","    def predict(self, par):\n","        Z = self.mlp(par)  # (dim_1_reduced * dim_2_reduced)\n","        Z = self.decoder(Z)  # (batch_size, 1, dim_1, dim_2)\n","        Z=Z.squeeze(1)\n","        return Z\n","\n","  ######################################################################################\n","\n","class MLP_press_ConvAutoencoder_3(torch.nn.Module):\n","    def __init__(self, activation, pooling, K_comb, L_c, L_l, dim_1=39, dim_2=16, dim_reduced=128, K=256):\n","        super(MLP_press_ConvAutoencoder_3, self).__init__()\n","\n","        if L_c == 1:\n","            dim_post_encoder = 4854\n","            tuple_dec=[(32,19,8),(39,16)]\n","        elif L_c == 2:\n","            dim_post_encoder = 2304\n","            tuple_dec=[(64,9,4),(19,8),(39,16)]\n","        elif L_c == 3:\n","            dim_post_encoder = 1024\n","            tuple_dec=[(128,4,2),(9,4),(19,8),(39,16)]\n","\n","        self.mlp = torch.nn.Sequential(\n","            *[layer for i in range(4) for layer in [\n","                torch.nn.Linear(3 if i == 0 else K, K),\n","                torch.nn.LayerNorm(K),\n","                torch.nn.ReLU(),\n","            ]],\n","            torch.nn.Linear(K, dim_reduced), #(batch_size, K) -> (batch_size, dim_reduced)\n","\n","        )\n","\n","        self.activation=torch.nn.ReLU if activation=='relu' else torch.nn.GELU\n","        self.pooling=torch.nn.MaxPool2d if pooling=='max' else torch.nn.AvgPool2d\n","\n","        # Encoder\n","        self.encoder = torch.nn.Sequential(\n","            *[layer for i in range(L_c) for layer in [\n","                torch.nn.Conv2d(1 if i == 0 else 2**(i-1)*32, 2**(i)*32, kernel_size=K_comb[0], stride=K_comb[1], padding=K_comb[2]),\n","                self.activation(),\n","                self.pooling(2,2)\n","            ]],\n","            torch.nn.Flatten(),\n","            *[layer for i in range(L_l) for layer in [\n","                torch.nn.Linear(dim_post_encoder if i==0 else dim_reduced*(L_l-i+1), dim_reduced*(L_l-i)),\n","                torch.nn.LayerNorm(dim_reduced*(L_l-i)),\n","                self.activation()\n","            ]]\n","        )\n","\n","        self.decoder = torch.nn.Sequential(\n","            *[layer for i in range(L_l-1,-1,-1) for layer in [\n","                torch.nn.Linear(dim_reduced*(L_l-i), dim_post_encoder if i==0 else dim_reduced*(L_l-i+1)),\n","                torch.nn.LayerNorm(dim_post_encoder if i==0 else dim_reduced*(L_l-i+1)),\n","                self.activation()\n","            ]],\n","            torch.nn.Unflatten(1, tuple_dec[0]),\n","            *[layer for i in range(L_c-1) for layer in [\n","                torch.nn.Upsample(size=tuple_dec[i+1]),\n","                torch.nn.ConvTranspose2d(2**(L_c-i-1)*32, 2**(L_c-i-2)*32, kernel_size=K_comb[0], stride=K_comb[1], padding=K_comb[2]),\n","                self.activation(),\n","            ]],\n","            torch.nn.Upsample(size=tuple_dec[-1]),\n","            torch.nn.ConvTranspose2d(32, 1, kernel_size=K_comb[0], stride=K_comb[1], padding=K_comb[2]),\n","            torch.nn.Sigmoid()\n","        )\n","\n","\n","\n","    def forward(self, par, out):\n","        Z1 = self.mlp(par)  # (batch_size, dim_reduced)\n","\n","        out_squeezed = out.unsqueeze(1) #(batch_size x 39 x 16) -> #(batch_size x 1 x 39 x 16)\n","\n","        # print(out_squeezed.shape)\n","        Z2 = self.encoder(out_squeezed)  # (batch_size, dim_reduced)\n","\n","        Z3 = self.decoder(Z2) # (batch_siz, 1, dim_1, dim_2)\n","        Z3=Z3.squeeze(1)  # (batch_siz, dim_1, dim_2)\n","\n","        return Z1, Z2, Z3\n","\n","    def predict(self, par):\n","        Z = self.mlp(par)  # (dim_1_reduced * dim_2_reduced)\n","        Z = self.decoder(Z)  # (batch_size, 1, dim_1, dim_2)\n","        Z=Z.squeeze(1)\n","        return Z"],"metadata":{"id":"HAX-EZRXUBll","executionInfo":{"status":"ok","timestamp":1732692413027,"user_tz":-60,"elapsed":5,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yIMUR21Cfoez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","model = MLP_press_ConvAutoencoder_2()\n","  # Valori casuali tra 0 e 1\n","z1, z2, z3= model.forward(train_params,train_vel)"],"metadata":{"id":"TMi1vXFzY8xi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["z1.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2NhSZdWwZFKA","executionInfo":{"status":"ok","timestamp":1732631472455,"user_tz":-60,"elapsed":2,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}},"outputId":"88af2bcd-0897-4f26-b40d-00dcdf25c174"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1560, 128])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["z2.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggE3gpJhZGsL","executionInfo":{"status":"ok","timestamp":1732525777641,"user_tz":-60,"elapsed":415,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"4466d6fa-64fd-4fd1-df90-fde17acd45a7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1560, 128])"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["z3.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_YzJP87ZJbM","executionInfo":{"status":"ok","timestamp":1732525778355,"user_tz":-60,"elapsed":2,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"8b8471f0-d791-48e5-ef65-ce042177307a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1560, 39, 16])"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"I141B3VFW7DW","executionInfo":{"status":"ok","timestamp":1732692433695,"user_tz":-60,"elapsed":389,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}}},"outputs":[],"source":["# vel_space_mean=vel_space_mean.to('cuda')\n","# vel_space_std=vel_space_std.to('cuda')\n","vel_space_min=vel_space_min.to('cpu')\n","vel_space_max=vel_space_max.to('cpu')\n","\n","#TODO: normalize input of encoder\n","\n","def train_encoder_decoder_epoch(model, device, train_loader, optimizer, epoch, criterion):\n","    model.train()  # Important set model to train mode (affects dropout, batch norm etc)\n","\n","    loss_history = []\n","    accuracy_history = []\n","    loss_history_enc_dec = []\n","    loss_history_mlp_encoder = []\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","\n","\n","        data=data.to(device)\n","        target=target.to(device)\n","\n","        Z1, Z2, Z3 = model.forward(data, target)\n","\n","        assert Z3.shape == target.shape\n","        assert Z1.shape == Z2.shape\n","\n","        # ###########################\n","        # target=target*vel_space_std+vel_space_mean\n","        # Z3=Z3*vel_space_std+vel_space_mean\n","        # ###########################\n","        target=target*(1.05*vel_space_max - 0.95*vel_space_min)+0.95*vel_space_min\n","        Z3=Z3*(1.05*vel_space_max - 0.95*vel_space_min)+0.95*vel_space_min\n","        ##############################\n","\n","        Z3_flattened = Z3.flatten(1)\n","        target_fattened = target.flatten(1)\n","\n","        loss_enc_dec =  criterion(target_fattened, Z3_flattened)\n","        loss_mlp_encoder = criterion(Z1, Z2)\n","\n","\n","        loss =  loss_enc_dec + loss_mlp_encoder\n","\n","        optimizer.zero_grad()  # Zero the gradients\n","\n","        loss.backward()        # Backpropagation\n","        optimizer.step()       # Update the weights\n","\n","        loss_history_enc_dec.append(loss_enc_dec.item())\n","        loss_history_mlp_encoder.append(loss_mlp_encoder.item())\n","        loss_history.append(loss.item())\n","\n","    return loss_history, loss_history_enc_dec, loss_history_mlp_encoder\n","\n","@torch.no_grad()\n","def validate(model, device, val_loader, criterion):\n","    model.eval()  # Important set model to eval mode (affects dropout, batch norm etc)\n","    test_loss = 0\n","    test_rel_loss = 0\n","    for data, target in val_loader:\n","\n","        data, target = data.to(device), target.to(device)\n","        output = model.predict(data)\n","        assert output.shape == target.shape\n","\n","        # #####################\n","        # output=output*vel_space_std+vel_space_mean\n","        # target=target*vel_space_std+vel_space_mean\n","        # #####################\n","        output=output*(1.05*vel_space_max - 0.95*vel_space_min)+0.95*vel_space_min\n","        target=target*(1.05*vel_space_max - 0.95*vel_space_min)+0.95*vel_space_min\n","        ##############################\n","\n","        output_flattened = output.flatten(1)\n","        target_flattened = target.flatten(1)\n","        test_loss += criterion(output_flattened, target_flattened).item() * len(data)\n","\n","        test_rel_loss += ((torch.norm((output-target).view(output.shape[0], -1), dim=1, p=2)/torch.norm((target).view(output.shape[0], -1), dim=1, p=2)).sum()).item()\n","    test_loss /= len(val_loader.dataset)\n","    test_rel_loss /= len(val_loader.dataset)\n","\n","    print(\n","         \"Test set: Average loss: {:.4f} Average relative error: {:.4f}\".format(\n","           test_loss, test_rel_loss\n","        )\n","    )\n","    return test_loss,test_rel_loss\n","\n","\n","def run_vel_training_encoder_decoder(model_encoder_decoder,num_epochs, lr, batch_size, device=\"cuda\"):\n","\n","    train_vel_DataSet = Fluid_Dataset(train_params, train_vel)\n","    test_vel_DataSet = Fluid_Dataset(test_params, test_vel)\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        train_vel_DataSet,\n","        batch_size=batch_size,\n","        shuffle=True,  # Can be important for training\n","        pin_memory=torch.cuda.is_available(),\n","        drop_last=True,\n","        num_workers=2,\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        test_vel_DataSet,\n","        batch_size=batch_size,\n","    )\n","\n","    # ===== Model, Optimizer and Criterion =====\n","    model_encoder_decoder = model_encoder_decoder.to(device=device)\n","    optimizer_encoder_decoder  = torch.optim.Adam(\n","        model_encoder_decoder.parameters(),\n","        lr=lr,\n","        weight_decay=1e-3\n","    )\n","    criterion_mlp = torch.nn.functional.mse_loss\n","\n","    # ===== Train Model =====\n","    train_loss_history = []\n","    train_loss_enc_dec_history = []\n","    train_loss_mlp_enc_history = []\n","    val_loss_history = []\n","    val_rel_loss_history = []\n","\n","    for epoch in range(1, num_epochs + 1):\n","        train_loss, trai_loss_enc_dec, train_loss_mlp_enc = train_encoder_decoder_epoch(\n","            model_encoder_decoder, device, train_loader, optimizer_encoder_decoder, epoch, criterion_mlp\n","        )\n","        print(\"LOSS_ENC: \", sum(trai_loss_enc_dec)/len(trai_loss_enc_dec), \" LOSS_MLP: \", sum(train_loss_mlp_enc)/len(train_loss_mlp_enc))\n","        train_loss_history.extend(train_loss)\n","        train_loss_enc_dec_history.extend(trai_loss_enc_dec)\n","        train_loss_mlp_enc_history.extend(train_loss_mlp_enc)\n","\n","        val_loss,val_rel_loss= validate(model_encoder_decoder, device, val_loader, criterion_mlp)\n","        val_loss_history.append(val_loss)\n","        val_rel_loss_history.append(val_rel_loss)\n","\n","\n","    # ===== Plot training curves =====\n","    n_train = len(train_loss_history)\n","    t_train = num_epochs * np.arange(n_train) / n_train\n","    t_val = np.arange(1, num_epochs + 1)\n","\n","    plt.figure()\n","    plt.plot(t_train, train_loss_history, label=\"Train\")\n","    plt.plot(t_val, val_loss_history, label=\"Val\")\n","    plt.legend()\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","\n","    plt.figure()\n","    plt.plot(t_train, train_loss_enc_dec_history, label=\"Train enc dec\")\n","    plt.plot(t_train, train_loss_mlp_enc_history, label=\"Train mlp enc\")\n","    plt.legend()\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","\n","    plt.figure()\n","    plt.plot(t_val, val_rel_loss_history, label=\"Val rel err\")\n","    plt.legend()\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"rel err\")\n","\n","    return model_encoder_decoder, val_loss_history[-1], val_rel_loss_history[-1]\n"]},{"cell_type":"code","source":["activation=\"gelu\" #\"relu\" o \"gelu\"\n","pooling=\"avg\" #\"max\" o \"avg\"\n","K_comb=[3,1,1] #o K_comb=[5,1,2]\n","L_c=2  #1,2, o 3\n","L_l=2  #1,2, o 3\n","\n","batch_size=32\n","num_epochs=200\n","lr=0.0001\n","#device=torch.device('cpu')\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model_trained,  val_loss, val_rel_loss = run_vel_training_encoder_decoder(MLP_press_ConvAutoencoder_3(activation,pooling,K_comb,L_c,L_l),num_epochs, lr, batch_size, device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"0Ijy1f4reSHp","executionInfo":{"status":"error","timestamp":1732692560398,"user_tz":-60,"elapsed":112270,"user":{"displayName":"Enzo Scurria","userId":"00963554908248762485"}},"outputId":"fbb2c1a0-6ca9-4dbb-dc20-deba3a4abfda"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["LOSS_ENC:  4.487108578284581  LOSS_MLP:  0.19591833810166767\n","Test set: Average loss: 4.9045 Average relative error: 0.2885\n","LOSS_ENC:  2.532457023859024  LOSS_MLP:  0.02111151779536158\n","Test set: Average loss: 3.5830 Average relative error: 0.2479\n","LOSS_ENC:  1.9657392005125682  LOSS_MLP:  0.0101919426524546\n","Test set: Average loss: 2.4847 Average relative error: 0.2080\n","LOSS_ENC:  1.716253769894441  LOSS_MLP:  0.00921676058593827\n","Test set: Average loss: 2.1172 Average relative error: 0.1931\n","LOSS_ENC:  1.480799009402593  LOSS_MLP:  0.010618797658632198\n","Test set: Average loss: 2.0107 Average relative error: 0.1886\n","LOSS_ENC:  1.2521847399572532  LOSS_MLP:  0.01206805849991118\n","Test set: Average loss: 1.9234 Average relative error: 0.1841\n","LOSS_ENC:  1.049667273958524  LOSS_MLP:  0.012002404449352374\n","Test set: Average loss: 1.8065 Average relative error: 0.1780\n","LOSS_ENC:  0.8741407754520575  LOSS_MLP:  0.011016145154523352\n","Test set: Average loss: 1.7061 Average relative error: 0.1729\n","LOSS_ENC:  0.7419748740891615  LOSS_MLP:  0.01049160115265598\n","Test set: Average loss: 1.6221 Average relative error: 0.1683\n","LOSS_ENC:  0.6390724132458369  LOSS_MLP:  0.010082448910300931\n","Test set: Average loss: 1.5577 Average relative error: 0.1648\n","LOSS_ENC:  0.5501284152269363  LOSS_MLP:  0.009701269872797033\n","Test set: Average loss: 1.4697 Average relative error: 0.1597\n","LOSS_ENC:  0.4719571868578593  LOSS_MLP:  0.0086517999394952\n","Test set: Average loss: 1.4171 Average relative error: 0.1566\n","LOSS_ENC:  0.43060078161458176  LOSS_MLP:  0.007947340258397162\n","Test set: Average loss: 1.3660 Average relative error: 0.1533\n","LOSS_ENC:  0.3962005830059449  LOSS_MLP:  0.007512276284008597\n","Test set: Average loss: 1.2951 Average relative error: 0.1490\n","LOSS_ENC:  0.3638337130347888  LOSS_MLP:  0.00708602760763218\n","Test set: Average loss: 1.2675 Average relative error: 0.1474\n","LOSS_ENC:  0.34416200903554756  LOSS_MLP:  0.00675793387927115\n","Test set: Average loss: 1.1999 Average relative error: 0.1432\n","LOSS_ENC:  0.3196959601094325  LOSS_MLP:  0.006416742069025834\n","Test set: Average loss: 1.1508 Average relative error: 0.1404\n","LOSS_ENC:  0.3112750326593717  LOSS_MLP:  0.006223855869999777\n","Test set: Average loss: 1.0910 Average relative error: 0.1368\n","LOSS_ENC:  0.28980408050119877  LOSS_MLP:  0.005914521054364741\n","Test set: Average loss: 1.0467 Average relative error: 0.1344\n","LOSS_ENC:  0.273178964232405  LOSS_MLP:  0.005648852587910369\n","Test set: Average loss: 1.0079 Average relative error: 0.1314\n","LOSS_ENC:  0.25729415224244195  LOSS_MLP:  0.005258940111768122\n","Test set: Average loss: 0.9931 Average relative error: 0.1308\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-8b12132c5754>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel_trained\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_rel_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_vel_training_encoder_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLP_press_ConvAutoencoder_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpooling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK_comb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-45d20e391a1b>\u001b[0m in \u001b[0;36mrun_vel_training_encoder_decoder\u001b[0;34m(model_encoder_decoder, num_epochs, lr, batch_size, device)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         train_loss, trai_loss_enc_dec, train_loss_mlp_enc = train_encoder_decoder_epoch(\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mmodel_encoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_encoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_mlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         )\n","\u001b[0;32m<ipython-input-16-45d20e391a1b>\u001b[0m in \u001b[0;36mtrain_encoder_decoder_epoch\u001b[0;34m(model, device, train_loader, optimizer, epoch, criterion)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# Update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tR7Fp6yKFBkA"},"outputs":[],"source":["#find best num_epoch and learning rate\n","\n","batch_size=32\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","val_losses=[]\n","val_rel_losses=[]\n","for lr in list(np.linspace(0.0001, 0.001, 50)):\n","  for num_epochs in list(np.linspace(50 , 200, 20).astype(int)):\n","\n","        model_trained,  val_loss, val_rel_loss = run_vel_training_encoder_decoder(num_epochs, lr, batch_size, device)\n","        val_losses.append(val_loss)\n","        val_rel_losses.append(val_rel_loss)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBBQ2fU6vsPY"},"outputs":[],"source":["print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NINq0KIOvn_I"},"outputs":[],"source":["\n","model = Simple_Model()\n","  # Valori casuali tra 0 e 1\n","z1, z2, z3= model.forward(train_params,train_vel)"]},{"cell_type":"code","source":["z1.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NjMB1zFmuSDm","executionInfo":{"status":"ok","timestamp":1732466819115,"user_tz":-60,"elapsed":2,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"6366a8d3-bcda-4c02-c001-71e83b7d8d33"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1560, 18, 8])"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1732466820965,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"},"user_tz":-60},"id":"KI43oebU3e95","outputId":"d318417b-c453-4410-e48b-f23dbd07d7d8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1560, 18, 8])"]},"metadata":{},"execution_count":80}],"source":["z2.shape"]},{"cell_type":"code","source":["z3.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PD3WsrA0vHcx","executionInfo":{"status":"ok","timestamp":1732466822151,"user_tz":-60,"elapsed":271,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"71fc5297-1b8b-463f-de2c-9dc1dee41bf2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1560, 5, 5])"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["z=model.predict(train_params)"],"metadata":{"id":"tsxauj7LvOly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["z.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlI85MUnvRAf","executionInfo":{"status":"ok","timestamp":1732465829028,"user_tz":-60,"elapsed":214,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"55d4f37e-1c91-4c0c-a58e-f5db4fe15b43"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1560, 5, 5])"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["c = torch.randn((5,3,2))\n","c.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2exQdUyryr-U","executionInfo":{"status":"ok","timestamp":1732462326076,"user_tz":-60,"elapsed":223,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"bb58ae7a-d705-45d2-ee73-61d6954b49d9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 3, 2])"]},"metadata":{},"execution_count":128}]},{"cell_type":"code","source":["c_shaped=c.view(c.shape[0], -1)"],"metadata":{"id":"M4gLIJpMzPPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c_shaped.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"csiBaQe9zZv8","executionInfo":{"status":"ok","timestamp":1732462376839,"user_tz":-60,"elapsed":5,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"bb359c84-7a14-4c52-8dd6-329649293892"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 6])"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","source":["c_shaped"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GcBNc2kKzsRD","executionInfo":{"status":"ok","timestamp":1732462432207,"user_tz":-60,"elapsed":231,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"ea09b0fa-7a03-471d-c6c6-3af1b07da260"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0247,  0.5999,  1.3813, -0.8010,  1.7893, -1.3100],\n","        [-0.0918,  1.5952, -0.1132, -1.1700,  0.2491,  2.2126],\n","        [-0.3063,  1.1698,  0.2230,  0.6958,  0.5637, -1.1418],\n","        [ 0.4004,  0.7669,  0.3747,  1.3313,  0.7806,  0.3532],\n","        [-0.2890,  0.9966,  1.9221,  0.0194, -1.7310,  1.0572]])"]},"metadata":{},"execution_count":135}]},{"cell_type":"code","source":["torch.norm(c_shaped, dim=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"crBccyUtznzZ","executionInfo":{"status":"ok","timestamp":1732462426967,"user_tz":-60,"elapsed":237,"user":{"displayName":"Manuel Curnis","userId":"08426131758052237851"}},"outputId":"94ccb136-af9e-4252-9a12-3f4b30505c4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2.7979, 2.9821, 1.9020, 1.8426, 2.9809])"]},"metadata":{},"execution_count":134}]},{"cell_type":"markdown","metadata":{"id":"sGia49f52YVb"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WxKh2cvIOjf_"},"outputs":[],"source":["def read_vtk(filename):\n","    \"\"\"Read .vtk file and return the polydata\"\"\"\n","\n","    fn_dir, fn_ext = os.path.splitext(filename)\n","\n","    if fn_ext == '.vtk':\n","        reader = vtk.vtkPolyDataReader()\n","    elif fn_ext == '.vtp':\n","        reader = vtk.vtkXMLPolyDataReader()\n","    elif fn_ext == '.stl':\n","        reader = vtk.vtkSTLReader()\n","    elif fn_ext == '.obj':\n","        reader = vtk.vtkOBJReader()\n","    elif fn_ext == '.vtu':\n","        reader = vtk.vtkXMLUnstructuredGridReader()\n","    elif fn_ext == '.pvtu':\n","        reader = vtk.vtkXMLPUnstructuredGridReader()\n","    else:\n","        raise ValueError(F\"File extension {fn_ext} not supported\")\n","\n","    reader.SetFileName(filename)\n","    reader.Update(0)\n","    mesh = reader.GetOutput()\n","\n","    return mesh\n","\n","def write_vtk(mesh, fn):\n","    \"\"\" Write a mesh (vtk polydata or unstructured grid) to disk \"\"\"\n","\n","    _, extension = os.path.splitext(fn)\n","\n","    if extension == '.vtk':\n","        writer = vtk.vtkPolyDataWriter()\n","    elif extension == '.stl':\n","        writer = vtk.vtkSTLWriter()\n","    elif extension == '.vtp':\n","        writer = vtk.vtkXMLPolyDataWriter()\n","    elif extension == '.vtu':\n","        writer = vtk.vtkXMLUnstructuredGridWriter()\n","    elif extension == '.obj':\n","        writer = vtk.vtkOBJWriter()\n","    else:\n","        raise ValueError(f\"Unrecognized extension {extension}\")\n","\n","    writer.SetInputData(mesh)\n","    writer.SetFileName(fn)\n","    writer.Update(0)\n","    writer.Write()\n","\n","    return\n","\n","def add_array(mesh, array, name):\n","    \"\"\"Add numpy array as new field to a vtk file\"\"\"\n","\n","    new_array = numpy_to_vtk(array)\n","    new_array.SetName(name)\n","    mesh.GetPointData().AddArray(new_array)\n","\n","    return mesh\n","\n","def compute_matching_idxs():\n","    \"\"\"Compute correspondences bewteen indices on the .vtu and on the .mesh file for plotting\"\"\"\n","\n","    mesh = read_vtk(os.path.join('/content/drive/MyDrive/data_ML4Science', 'geometries', 'bif_sym_alpha50_h0.10_ref.vtu'))\n","    points = vtk_to_numpy(mesh.GetPoints().GetData())\n","\n","    mesh_2 = meshio.read(os.path.join('/content/drive/MyDrive/data_ML4Science', 'geometries','bif_sym_alpha50_h0.10.mesh'))\n","    points_2 = mesh_2.points\n","\n","    dist = cdist(mesh_2.points, points, metric='euclidean')\n","\n","    idxs = np.argmin(dist, axis=0)\n","\n","    return idxs\n","\n","\n","def visualize_solution(field_array, fields=None, step_t=10):\n","    \"\"\" Export the solution corresponding to the n-th snapshot (every step_t steps) to a .vtu file.\"\"\"\n","\n","    if fields is None:\n","        fields = {'velocity': 3, 'pressure': 1}  # fields and corresponding dimensions\n","\n","    os.makedirs('solutions', exist_ok=True)\n","\n","    idxs = compute_matching_idxs()\n","\n","    mesh = read_vtk(os.path.join('/content/drive/MyDrive/data_ML4Science', 'geometries', 'bif_sym_alpha50_h0.10.vtu'))\n","\n","    fom_solution = dict()\n","    for field in fields:\n","        # print(f\"Processing field {field} - Dimension: {fields[field]}\")\n","        cur_idxs = np.hstack([idxs + k * (Nh_space[field]//fields[field]) for k in range(fields[field])])\n","        fom_solution[field] = expand(field_array, basis_space[field], basis_time[field])[cur_idxs]\n","        print(fom_solution[field].shape)\n","\n","    for cnt_t in range(0, Nh_time['velocity'], step_t):\n","        # print(f\"\\nProcessing timestep {cnt_t} of {Nh_time['velocity']}\")\n","        for field in fields:\n","            cur_fom_solution = np.reshape(fom_solution[field][:, cnt_t], (fields[field], -1)).T\n","            mesh = add_array(mesh, cur_fom_solution, field)\n","\n","        # write_vtk(mesh, os.path.join('solutions', f\"solution_{n}_{cnt_t}\" + '.vtu'))\n","        write_vtk(mesh, os.path.join('solutions', f\"solution_{cnt_t}\" + '.vtu'))\n","\n","    return\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ezn9VDSNOwKP"},"outputs":[],"source":["model_trained.eval()\n","\n","input_tensor = torch.tensor(test_params[0,:], dtype=torch.float32)  # Trasforma in tensore\n","input_tensor = input_tensor.unsqueeze(dim=0)\n","input_tensor = input_tensor.to(device)  # Se usi la GPU\n","\n","with torch.no_grad():\n","    output_visual = model_trained(input_tensor)\n","    output_visual=output_visual[0]\n","    print(output_visual.shape)\n","\n","visualize_solution(output_visual.cpu().numpy() ,fields={'velocity': 3},step_t=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWGvn5V8cY_I"},"outputs":[],"source":["!zip -r solutions.zip solutions\n","from google.colab import files\n","files.download('solutions.zip')"]}],"metadata":{"colab":{"provenance":[{"file_id":"14MiALJ4ERdh7F7aK1XRNwP3og1MPqCxx","timestamp":1731864307182}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}